{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benjamins-MacBook-Pro-2.local\n",
      "{'dropbox_data_dir': '/Users/benjaminsmith/Dropbox (University of Oregon)/UO-SAN Lab/Berkman Lab/Devaluation/analysis_files/data/'}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from socket import gethostname\n",
    "import numpy as np\n",
    "from dev_interaction_util import generate_synthetic_dev_outcomes, generate_synthetic_dev_data\n",
    "from ml_util import *\n",
    "\n",
    "print(gethostname())\n",
    "# Open the file and load the file\n",
    "with open('config.yml') as f:\n",
    "    all_yaml = yaml.load(f, Loader=SafeLoader)\n",
    "    if gethostname() in all_yaml.keys():\n",
    "        config = all_yaml[gethostname()]\n",
    "    else:\n",
    "        config = all_yaml['default']\n",
    "        \n",
    "print(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropbox_data_dir = config['dropbox_data_dir']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is a pre-registered analysis for measuring moderations of the intervention.\n",
    "\n",
    "We'll cross-validate the intervention moderations.\n",
    "\n",
    "For this analysis, we'll try to make predictions based on some synthetic data. we'll take wave 1 data and randomly mix in changes based on our predictors, then try to model how we would predict those things. Finally, we'll make the predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_by_ppt_path = dropbox_data_dir + '/data_by_ppt.csv'\n",
    "data_codebook_path = dropbox_data_dir + 'data_codebook.csv'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_ppt = pd.read_csv(data_by_ppt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_codebook = pd.read_csv(data_codebook_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find out which columns in data_by_ppt are missing from the codebook\n",
    "data_by_ppt.columns.difference(data_codebook['VarName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#copy our outcome measures, bf_1 and FFQ_1, into a new dataframe\n",
    "data_by_ppt['bf_2'] = data_by_ppt.bf_1\n",
    "#need to decide what sort of FFQ we want to use\n",
    "data_by_ppt['cancer_promoting_minus_preventing_FFQ_1'] = data_by_ppt.cancer_promoting_minus_preventing_FFQ\n",
    "data_by_ppt['cancer_promoting_minus_preventing_FFQ_2'] = data_by_ppt.cancer_promoting_minus_preventing_FFQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data  = data_by_ppt.loc[:,data_codebook.loc[data_codebook.IsSelectedPredictor,\"VarName\"]].copy()\n",
    "outcome_measures = data_by_ppt.loc[:,data_codebook.loc[data_codebook.IsSelectedOutcomeMeasure,\"VarName\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_values = pd.DataFrame(data_by_ppt.isna().sum())\n",
    "na_values.columns = ['NA_Count']\n",
    "na_values['prop_NA'] = na_values.NA_Count / data_by_ppt.shape[0]\n",
    "data_codebook = data_codebook.merge(na_values, left_on='VarName', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_codebook.to_csv(dropbox_data_dir + 'data_metadata.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to count the number of valid and missing entries in each of our data predictors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting data to numeric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_vals = pd.get_dummies(analysis_data.birthsex_factor)\n",
    "#there's only two variables here so we can convert this into a dummy variable\n",
    "analysis_data.drop(columns=['birthsex_factor'], inplace=True)\n",
    "one_hot_vals.columns = ['birthsex_factor_' + str(col) for col in one_hot_vals.columns]\n",
    "analysis_data = analysis_data.join(one_hot_vals.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BSCS</th>\n",
       "      <th>EDM</th>\n",
       "      <th>BIS_11</th>\n",
       "      <th>PCS</th>\n",
       "      <th>RS</th>\n",
       "      <th>TRSQ</th>\n",
       "      <th>ACES_neglectful_parenting</th>\n",
       "      <th>ACES_abuse</th>\n",
       "      <th>ACES_sum</th>\n",
       "      <th>ACES_divorced_separated</th>\n",
       "      <th>...</th>\n",
       "      <th>zipcode_median_income_acs</th>\n",
       "      <th>household_income_per_person</th>\n",
       "      <th>SST_prop_successful_stops</th>\n",
       "      <th>SST_GRTmean</th>\n",
       "      <th>SST_SSD</th>\n",
       "      <th>SST_PostErrorSlowW1_mean</th>\n",
       "      <th>SST_mean_ssrt_0</th>\n",
       "      <th>ROC_Crave_Regulate_Minus_Look</th>\n",
       "      <th>WTP_unhealthy_minus_healthy</th>\n",
       "      <th>birthsex_factor_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.538462</td>\n",
       "      <td>3.250</td>\n",
       "      <td>72</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.5125</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.384615</td>\n",
       "      <td>1.750</td>\n",
       "      <td>89</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.440524</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.384615</td>\n",
       "      <td>2.500</td>\n",
       "      <td>63</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>533.315052</td>\n",
       "      <td>284.375</td>\n",
       "      <td>0.058297</td>\n",
       "      <td>0.247061</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>-0.190476</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.076923</td>\n",
       "      <td>2.800</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>498.167248</td>\n",
       "      <td>103.125</td>\n",
       "      <td>0.027730</td>\n",
       "      <td>0.446583</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>0.170363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.307692</td>\n",
       "      <td>2.750</td>\n",
       "      <td>64</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>626.507764</td>\n",
       "      <td>250.000</td>\n",
       "      <td>0.105660</td>\n",
       "      <td>0.369308</td>\n",
       "      <td>-1.5500</td>\n",
       "      <td>-0.494624</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>3.461538</td>\n",
       "      <td>4.000</td>\n",
       "      <td>58</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.690347</td>\n",
       "      <td>1.768485</td>\n",
       "      <td>0.523438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.357362</td>\n",
       "      <td>-0.0125</td>\n",
       "      <td>-1.008152</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>3.692308</td>\n",
       "      <td>3.875</td>\n",
       "      <td>54</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.511475</td>\n",
       "      <td>-0.234851</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.335849</td>\n",
       "      <td>-0.1500</td>\n",
       "      <td>-1.889247</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>3.461538</td>\n",
       "      <td>3.125</td>\n",
       "      <td>69</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.335248</td>\n",
       "      <td>0.099038</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.273736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>2.846154</td>\n",
       "      <td>3.000</td>\n",
       "      <td>62</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.855379</td>\n",
       "      <td>-0.234851</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.401098</td>\n",
       "      <td>-0.9875</td>\n",
       "      <td>-0.151210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>3.230769</td>\n",
       "      <td>2.500</td>\n",
       "      <td>52</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.834004</td>\n",
       "      <td>1.768485</td>\n",
       "      <td>0.476562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.481932</td>\n",
       "      <td>-0.5500</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         BSCS    EDM  BIS_11   PCS    RS  TRSQ  ACES_neglectful_parenting  \\\n",
       "0    2.538462  3.250      72   7.0  20.0  63.0                        NaN   \n",
       "1    2.384615  1.750      89   9.0  22.0  63.0                        NaN   \n",
       "2    3.384615  2.500      63   9.0  18.0  57.0                        NaN   \n",
       "3    3.076923  2.800      75   NaN   NaN  64.0                        NaN   \n",
       "4    3.307692  2.750      64  12.0  21.0  55.0                        NaN   \n",
       "..        ...    ...     ...   ...   ...   ...                        ...   \n",
       "270  3.461538  4.000      58  18.0  17.0  54.0                        0.0   \n",
       "271  3.692308  3.875      54  17.0  13.0  55.0                        2.0   \n",
       "272  3.461538  3.125      69  11.0  13.0  53.0                        1.0   \n",
       "273  2.846154  3.000      62  15.0  22.0  84.0                        0.0   \n",
       "274  3.230769  2.500      52   9.0  15.0  59.0                        0.0   \n",
       "\n",
       "     ACES_abuse  ACES_sum  ACES_divorced_separated  ...  \\\n",
       "0           NaN       NaN                      NaN  ...   \n",
       "1           NaN       NaN                      NaN  ...   \n",
       "2           NaN       NaN                      NaN  ...   \n",
       "3           NaN       NaN                      NaN  ...   \n",
       "4           NaN       NaN                      NaN  ...   \n",
       "..          ...       ...                      ...  ...   \n",
       "270         1.0       3.0                      1.0  ...   \n",
       "271         2.0       5.0                      0.0  ...   \n",
       "272         1.0       6.0                      1.0  ...   \n",
       "273         1.0       4.0                      1.0  ...   \n",
       "274         0.0       3.0                      1.0  ...   \n",
       "\n",
       "     zipcode_median_income_acs  household_income_per_person  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          NaN                          NaN   \n",
       "4                          NaN                          NaN   \n",
       "..                         ...                          ...   \n",
       "270                  -0.690347                     1.768485   \n",
       "271                  -0.511475                    -0.234851   \n",
       "272                   1.335248                     0.099038   \n",
       "273                   0.855379                    -0.234851   \n",
       "274                   0.834004                     1.768485   \n",
       "\n",
       "     SST_prop_successful_stops  SST_GRTmean  SST_SSD  \\\n",
       "0                          NaN          NaN      NaN   \n",
       "1                          NaN          NaN      NaN   \n",
       "2                     0.500000   533.315052  284.375   \n",
       "3                     0.312500   498.167248  103.125   \n",
       "4                     0.562500   626.507764  250.000   \n",
       "..                         ...          ...      ...   \n",
       "270                   0.523438          NaN      NaN   \n",
       "271                   0.492188          NaN      NaN   \n",
       "272                   0.507812          NaN      NaN   \n",
       "273                   0.479167          NaN      NaN   \n",
       "274                   0.476562          NaN      NaN   \n",
       "\n",
       "     SST_PostErrorSlowW1_mean  SST_mean_ssrt_0  ROC_Crave_Regulate_Minus_Look  \\\n",
       "0                         NaN              NaN                        -0.5125   \n",
       "1                         NaN              NaN                            NaN   \n",
       "2                    0.058297         0.247061                        -0.8000   \n",
       "3                    0.027730         0.446583                        -0.8000   \n",
       "4                    0.105660         0.369308                        -1.5500   \n",
       "..                        ...              ...                            ...   \n",
       "270                       NaN         0.357362                        -0.0125   \n",
       "271                       NaN         0.335849                        -0.1500   \n",
       "272                       NaN         0.273736                            NaN   \n",
       "273                       NaN         0.401098                        -0.9875   \n",
       "274                       NaN         0.481932                        -0.5500   \n",
       "\n",
       "     WTP_unhealthy_minus_healthy  birthsex_factor_Male  \n",
       "0                      -0.312500                     1  \n",
       "1                       0.440524                     0  \n",
       "2                      -0.190476                     0  \n",
       "3                       0.170363                     0  \n",
       "4                      -0.494624                     0  \n",
       "..                           ...                   ...  \n",
       "270                    -1.008152                     1  \n",
       "271                    -1.889247                     1  \n",
       "272                     0.516129                     1  \n",
       "273                    -0.151210                     0  \n",
       "274                     0.343750                     1  \n",
       "\n",
       "[275 rows x 76 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data \n",
    "\n",
    "Apply missing data imputation to columns including cSES, ACES_sum, ses_aggregate, zipcode_median_income_acs, IMI, mcarthur social standing, based on demographic and self-report predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing with MICE\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn import linear_model\n",
    "\n",
    "#which columns do we want to exclude? \n",
    "# \n",
    "# Probably we can include all the columns in the codebook which aren't predictors.\n",
    "# I checked, and ACES, cSES, RTFS are the only columns over 20% missing, and I'm comfortable imputing those from other values.\n",
    "# Sitautions we might wnat to avoid imputing are where missingness is correlated with the predictor itself\n",
    "# that's the case for IPAQ probably, but we're avoiding IPAQ altogether for now.\n",
    "\n",
    "#analysis_data_imputed = analysis_data.loc[:,['ACES_sum','cSES']].copy()\n",
    "def get_data_for_imputation(analysis_data):\n",
    "    analysis_data_imputed = analysis_data.copy()\n",
    "    return(analysis_data_imputed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try a few methods of imputation and compare them.\n",
    "#I tried a few. the default is BayesianRidge, but i found this didn't pick up on the linear relationship between ACES and cSES\n",
    "#Ridge found the linear relationship, but also does some regularization which is probably useful for accuracy\n",
    "\n",
    "imputation_methods = {\n",
    "    \n",
    "    'knn_3':KNNImputer(n_neighbors=3),\n",
    "    'ridge_10':IterativeImputer(estimator=linear_model.Ridge(),n_nearest_features=10,max_iter=100),\n",
    "    'knn_4':KNNImputer(n_neighbors=4),\n",
    "    \n",
    "    'knn_6':KNNImputer(n_neighbors=6),\n",
    "    'ridge':IterativeImputer(estimator=linear_model.Ridge(),max_iter=100),\n",
    "    'ridge_5':IterativeImputer(estimator=linear_model.Ridge(),n_nearest_features=5,max_iter=100),\n",
    "    \n",
    "    'bayesianridge_3':IterativeImputer(estimator=linear_model.BayesianRidge(),max_iter=100,n_nearest_features=3)\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for imp_label in imputation_methods.keys():\n",
    "#     analysis_data_imputed = get_data_for_imputation(analysis_data)\n",
    "#     print(imp_label)\n",
    "#     imp = imputation_methods[imp_label]\n",
    "#     #this dataset is already filtered for columns so we don't need to filter those further.\n",
    "#     analysis_data_imputed = pd.DataFrame(imp.fit_transform(analysis_data_imputed), columns=analysis_data_imputed.columns)\n",
    "#     imputed_datapoint = analysis_data.isna()\n",
    "#     do_aces_cses_imputation_diagnostic(analysis_data_imputed, imputed_datapoint,imp_label)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this experiment, I'm going for Ridge regression with 10 nearest features. The values it imputes are a compromise between simply using the nearest mean, which is conservative when using these values for prediction because it doesn't introduce erroneous variance, but isn't very informative, and then using all available information, which Ridge regression with an unlimited number of features would do. It's a tough choice between this and KNN, which doesn't assume normality. Overall I'm going with KNN, because it picks up on relationships between the two variables while not generating extreme values like KNN seems to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/impute/_iterative.py:699: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "imputer = IterativeImputer(estimator=linear_model.Ridge(),n_nearest_features=10,max_iter=100,random_state=0)\n",
    "analysis_data_imputed = get_data_for_imputation(analysis_data)\n",
    "\n",
    "#this dataset is already filtered for columns so we don't need to filter those further.\n",
    "analysis_data_imputed = pd.DataFrame(imputer.fit_transform(analysis_data_imputed), columns=analysis_data_imputed.columns)\n",
    "imputed_datapoint = analysis_data.isna()\n",
    "# do_aces_cses_imputation_diagnostic(analysis_data_imputed, imputed_datapoint,'ridge_10')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add synthetic data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## synthetic condition mediator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set np random seed\n",
    "np.random.seed(3161527)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['ichi','ni','san']\n",
    "#assign each row randomly to a group\n",
    "group_assignments = np.random.choice(group_names,analysis_data_imputed.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## synthetic outcome variables\n",
    "\n",
    "Which variables will we test to predict? We need to select variables from the following groups:\n",
    "\n",
    " - self-report variables\n",
    " - demographic variables we'll test\n",
    " - summaries of neural activities from each of WTP, SST, and ROC\n",
    " - summaries of behavioral data from each of WTP, SST, and ROC\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_measures = generate_synthetic_dev_outcomes(outcome_measures)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add synthetic primary and interaction effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ni' 'san']\n",
      "[1.28335298 0.42953651]\n",
      "['san' 'san' 'ni' 'ichi' 'san' 'san' 'ichi' 'san' 'san' 'san' 'ni' 'ichi'\n",
      " 'ichi' 'ichi' 'ichi' 'san' 'san' 'san' 'ichi' 'ichi' 'san' 'san' 'ni'\n",
      " 'ni' 'ni' 'ni' 'ni' 'ni' 'ni' 'san' 'ni' 'san' 'ni' 'ichi' 'ni' 'san'\n",
      " 'ni' 'ichi' 'san' 'ni' 'ni' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ni' 'ni'\n",
      " 'san' 'ichi' 'ichi' 'ni' 'san' 'ni' 'ichi' 'ni' 'ni' 'ni' 'ichi' 'san'\n",
      " 'ni' 'ni' 'ichi' 'ni' 'ichi' 'san' 'ni' 'ni' 'ni' 'san' 'ichi' 'ni' 'san'\n",
      " 'ichi' 'san' 'ni' 'san' 'ni' 'ichi' 'ichi' 'san' 'ichi' 'san' 'san' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'san' 'ni' 'san' 'ni' 'ichi' 'san' 'san' 'san' 'ichi'\n",
      " 'ni' 'san' 'ichi' 'ichi' 'san' 'ni' 'ichi' 'san' 'ni' 'ni' 'san' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'ichi' 'ni' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ichi'\n",
      " 'ni' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ichi' 'san' 'san' 'ni' 'ichi' 'ni'\n",
      " 'ichi' 'ichi' 'san' 'ichi' 'ni' 'san' 'san' 'ni' 'ni' 'san' 'san' 'san'\n",
      " 'ichi' 'san' 'ni' 'san' 'ichi' 'ichi' 'ichi' 'ni' 'san' 'ni' 'ni' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'san' 'ni' 'san' 'ichi' 'ni' 'ni' 'ni' 'ichi' 'ni'\n",
      " 'ichi' 'san' 'san' 'san' 'san' 'ichi' 'ni' 'san' 'san' 'san' 'ichi' 'san'\n",
      " 'ni' 'ni' 'san' 'ichi' 'ichi' 'san' 'ni' 'ni' 'san' 'ni' 'san' 'san' 'ni'\n",
      " 'san' 'ni' 'ni' 'san' 'ichi' 'san' 'ichi' 'san' 'ni' 'ni' 'ni' 'ichi'\n",
      " 'ni' 'ni' 'san' 'ni' 'ni' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ni' 'ni' 'san'\n",
      " 'ichi' 'ni' 'ichi' 'ichi' 'ichi' 'ichi' 'ichi' 'ichi' 'san' 'ichi' 'san'\n",
      " 'ichi' 'ni' 'san' 'san' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ni' 'san' 'san'\n",
      " 'ichi' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ni' 'ni' 'ni' 'san' 'ichi'\n",
      " 'ichi' 'ichi' 'ni' 'ichi' 'ni' 'san' 'ichi' 'san' 'san' 'ni' 'san' 'san'\n",
      " 'san' 'san' 'ichi' 'ichi' 'ichi' 'ni' 'ni' 'ichi' 'san' 'san' 'san']\n",
      "ni\n",
      "                                    feature_name  interaction_effect\n",
      "64                                        age365            1.208606\n",
      "2                                         BIS_11           -0.990043\n",
      "17                         IMI_effort_importance           -0.958636\n",
      "15                                  BFI_openness            0.949431\n",
      "40              NCS_satisfaction_in_deliberating            0.930933\n",
      "23                         NCS_intellectual_task           -0.930569\n",
      "31                            NCS_prefer_complex           -0.888134\n",
      "65                                 education_own           -0.874354\n",
      "20                          IMI_perceived_choice            0.799869\n",
      "60                              RTFS_f1_minus_f2            0.790402\n",
      "59                  SRHI_healthy_minus_unhealthy            0.626453\n",
      "57                                    TESQ_E_sum           -0.612132\n",
      "72                               SST_mean_ssrt_0            0.606104\n",
      "32                     NCS_prefer_little_thought           -0.605050\n",
      "1                                            EDM           -0.589600\n",
      "49                                  SRHI_healthy            0.584133\n",
      "61  cancer_promoting_minus_preventing_craved_FCI            0.555034\n",
      "47                                 RTFS_factor_1            0.547233\n",
      "36                         NCS_abstract_thinking            0.546582\n",
      "67                   household_income_per_person           -0.525581\n",
      "bf_2\n",
      "cancer_promoting_minus_preventing_FFQ_w2\n",
      "FFQ_v2_Mean_Energy_w2\n",
      "san\n",
      "                        feature_name  interaction_effect\n",
      "21          IMI_perceived_competence           -1.634156\n",
      "12             BFI_conscientiousness           -1.135730\n",
      "70                           SST_SSD           -1.115029\n",
      "33       NCS_relief_not_satisfaction            0.886930\n",
      "45                    RMQ_assessment           -0.877257\n",
      "30               NCS_think_minimally            0.869242\n",
      "44                           RMQ_lie            0.866752\n",
      "40  NCS_satisfaction_in_deliberating           -0.824564\n",
      "66         zipcode_median_income_acs            0.784310\n",
      "20              IMI_perceived_choice            0.743981\n",
      "48                     RTFS_factor_2           -0.723901\n",
      "55      TESQ_E_goal_and_rule_setting            0.713090\n",
      "52    TESQ_E_controlling_temptations           -0.710659\n",
      "0                               BSCS           -0.676796\n",
      "39                 NCS_solve_puzzles           -0.675358\n",
      "11                 BFI_agreeableness            0.674328\n",
      "47                     RTFS_factor_1           -0.636170\n",
      "9            ACES_divorced_separated           -0.623386\n",
      "29             NCS_thought_appealing            0.622036\n",
      "67       household_income_per_person            0.612929\n",
      "bf_2\n",
      "cancer_promoting_minus_preventing_FFQ_w2\n",
      "FFQ_v2_Mean_Energy_w2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "synthetic_data = generate_synthetic_dev_data(analysis_data_imputed, group_assignments,outcome_measures)\n",
    "interaction_effect_df = synthetic_data['X_weights']\n",
    "outcome_measures = synthetic_data['y']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further preprocessing\n",
    "\n",
    "These steps are performed on the data regardless of whether we are using synthetic data or real data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up outcome measures and group assignment one-hot\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_measures = calculate_outcome_changes(outcome_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_assignment_onehots = pd.get_dummies(group_assignments).loc[:,['ni','san']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bf_1</th>\n",
       "      <th>bf_2</th>\n",
       "      <th>d_bf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.0</td>\n",
       "      <td>121.373628</td>\n",
       "      <td>88.373628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.8</td>\n",
       "      <td>58.983743</td>\n",
       "      <td>19.183743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.8</td>\n",
       "      <td>40.022751</td>\n",
       "      <td>-0.777249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.6</td>\n",
       "      <td>-33.046255</td>\n",
       "      <td>-80.646255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>29.9</td>\n",
       "      <td>31.240484</td>\n",
       "      <td>1.340484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>33.1</td>\n",
       "      <td>28.992403</td>\n",
       "      <td>-4.107597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>27.4</td>\n",
       "      <td>51.350718</td>\n",
       "      <td>23.950718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>42.1</td>\n",
       "      <td>64.553479</td>\n",
       "      <td>22.453479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>33.3</td>\n",
       "      <td>57.106888</td>\n",
       "      <td>23.806888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bf_1        bf_2       d_bf\n",
       "0    33.0  121.373628  88.373628\n",
       "1     NaN         NaN        NaN\n",
       "2    39.8   58.983743  19.183743\n",
       "3    40.8   40.022751  -0.777249\n",
       "4    47.6  -33.046255 -80.646255\n",
       "..    ...         ...        ...\n",
       "270  29.9   31.240484   1.340484\n",
       "271  33.1   28.992403  -4.107597\n",
       "272  27.4   51.350718  23.950718\n",
       "273  42.1   64.553479  22.453479\n",
       "274  33.3   57.106888  23.806888\n",
       "\n",
       "[275 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_measures.loc[:,['bf_1','bf_2','d_bf']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictor_data = analysis_data_imputed\n",
    "predictor_data_columns = predictor_data.columns\n",
    "predictor_data_array = np.array(predictor_data)\n",
    "predictor_data = pd.concat([predictor_data,group_assignment_onehots],axis=1)\n",
    "for group_name in group_assignment_onehots.columns:\n",
    "\n",
    "    #do a matrix multiplication of the group assignment onehot with the analysis data\n",
    "    #repeat the group assignment onehot for each column in the analysis data\n",
    "    \n",
    "    interaction_array = predictor_data_array*np.array(group_assignment_onehots[group_name],ndmin=2).T\n",
    "    interaction_df = pd.DataFrame(interaction_array, columns= [(c + '*'+group_name) for c in predictor_data_columns])\n",
    "    print(interaction_df.shape)\n",
    "    #then add the result to the analysis data\n",
    "    predictor_data = pd.concat([predictor_data,interaction_df],axis=1)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any NA values for this outcome measure in both the predictor data and the outcome data\n",
    "outcome_nas = outcome_measures['d_bf'].isna()\n",
    "\n",
    "outcome_measures_nona = outcome_measures.loc[~outcome_nas,:]\n",
    "predictor_data_nona = predictor_data.loc[~outcome_nas,:]\n",
    "group_assignment_onehots_nonan = group_assignment_onehots.loc[~outcome_nas,:]\n",
    "group_assignments_nona = group_assignments[~outcome_nas]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outcome_measures_nona' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outcome_measures_nona[\u001b[39m'\u001b[39m\u001b[39md_bf\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outcome_measures_nona' is not defined"
     ]
    }
   ],
   "source": [
    "outcome_measures_nona['d_bf']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a basic regression with interactions to ensure we can detect effects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to know: which variables are correlated with the outcome measures? A naive way is to use ridge regression to predict the outcome measures from the predictors, with every predictor crossed with a one-hot of the group assignment. \n",
    "\n",
    "To do this, unlike in R where we'd write out an equation and let the system take care of it, here, I'll build the interactions manually."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do ridge regression to hopefully identify any predictors of outcome measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "predictor_data_nona = sm.add_constant(predictor_data_nona)\n",
    "model = sm.OLS(outcome_measures_nona['d_bf'], predictor_data_nona.loc[:,['const','ni','san','BSCS','EDM','BSCS*ni','BSCS*san','EDM*ni','EDM*san']]).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "predictor_data_nona = sm.add_constant(predictor_data_nona)\n",
    "model = sm.OLS(outcome_measures_nona['d_bf'], predictor_data_nona).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a ridge regression\n",
    "primary_regression = linear_model.Ridge(fit_intercept=True, alpha=1.0)\n",
    "\n",
    "#fit the model\n",
    "model_fit = primary_regression.fit(predictor_data_nona,outcome_measures_nona['d_bf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the coefficients\n",
    "ridge_results = pd.DataFrame({\n",
    "    'predictor': predictor_data_nona.columns,\n",
    "    'coef': model_fit.coef_\n",
    "    #'std_err': np.sqrt(np.diag(model_fit.coef_cov_)),\n",
    "    #'pval': 2*(1-stats.t.cdf(np.abs(model_fit.coef_/np.sqrt(np.diag(model_fit.coef_cov_))),df=predictor_data_nona.shape[0]-predictor_data_nona.shape[1]))\n",
    "})\n",
    "\n",
    "ridge_results['coef_abs'] = np.abs(ridge_results.coef)\n",
    "ridge_results = ridge_results.sort_values('coef_abs',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running models with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_10pow_lower = 6\n",
    "alpha_10pow_upper = -1\n",
    "alpha_increments=1\n",
    "alpha_range = np.power(10,np.linspace(-alpha_10pow_lower,alpha_10pow_upper,(alpha_10pow_lower+alpha_10pow_upper)*alpha_increments+1))\n",
    "elasticnet_parameters = {\n",
    "    'alpha':alpha_range,\n",
    "    'l1_ratio': [0.1,0.3,0.5,0.9,0.99],#np.linspace(0.1,0.9,4+1),\n",
    "    'max_iter': [10000]\n",
    "    }\n",
    "print(elasticnet_parameters)\n",
    "elasticnet_model = linear_model.ElasticNet()\n",
    "\n",
    "\n",
    "\n",
    "elasticnet_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(elasticnet_model), param_grid = get_param_grid_with_preprocessing(elasticnet_parameters), cv=10,scoring='neg_mean_absolute_error')\n",
    "elasticnet_grid_search_cv.fit(predictor_data_nona,outcome_measures_nona['d_bf'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ridge_parameters = {'alpha':alpha_range}\n",
    "ridge_model = linear_model.Ridge()\n",
    "print(ridge_parameters)\n",
    "ridge_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(ridge_model), param_grid = get_param_grid_with_preprocessing(ridge_parameters), cv=10,scoring='neg_mean_absolute_error')\n",
    "ridge_grid_search_cv.fit(predictor_data_nona,outcome_measures_nona['d_bf'])\n",
    "sorted(ridge_grid_search_cv.cv_results_.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_parameters = {'alpha':alpha_range}\n",
    "lasso_model = linear_model.Lasso()\n",
    "print(lasso_parameters)\n",
    "lasso_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(lasso_model), param_grid = get_param_grid_with_preprocessing(lasso_parameters), cv=10,scoring='neg_mean_absolute_error')\n",
    "lasso_grid_search_cv.fit(predictor_data_nona,outcome_measures_nona['d_bf'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNeighborsRegressor\n",
    "knn_parameters = {'n_neighbors':np.unique(np.round(np.power(10,np.linspace(0,2,2*5+1)))).astype(int)}\n",
    "knn_model = KNeighborsRegressor()\n",
    "print(knn_parameters)\n",
    "knn_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(knn_model), param_grid = get_param_grid_with_preprocessing(knn_parameters), cv=10,scoring='neg_mean_absolute_error')\n",
    "knn_grid_search_cv.fit(predictor_data_nona,outcome_measures_nona['d_bf'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_estimator_params_from_gridsearch(param_dict):\n",
    "    dict_list = [{k.replace(pipeline_estimator_name + \"__\",\"\"):param_dict[k]} for k in param_dict.keys() if k.startswith(pipeline_estimator_name)]\n",
    "    #convert the dict_list into a dict\n",
    "    return({k:v for d in dict_list for k,v in d.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a heatmap of cv_results_ for the param alpha and l1_ratio using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#convert the cv_results_ to a dataframe\n",
    "elasticnet_cv_results_df = pd.DataFrame(elasticnet_grid_search_cv.cv_results_)\n",
    "estimator_params = elasticnet_cv_results_df.params.apply(lambda x: extract_estimator_params_from_gridsearch(x))\n",
    "elasticnet_cv_results_df['alpha'] = estimator_params.apply(lambda x: x['alpha'])\n",
    "elasticnet_cv_results_df['l1_ratio'] = estimator_params.apply(lambda x: x['l1_ratio'])\n",
    "elasticnet_cv_results_df['mean_test_score'] = elasticnet_cv_results_df.mean_test_score*-1\n",
    "\n",
    "#plot the heatmap\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(elasticnet_cv_results_df.pivot(index='alpha',columns='l1_ratio',values='mean_test_score'),annot=True,fmt='.2f')\n",
    "plt.title('ElasticNet Mean Absolute Error')\n",
    "plt.xlabel('l1_ratio')\n",
    "plt.ylabel('alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph alpha and mean_test_score\n",
    "#use the log10 of alpha but label the x-axis with alpha\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot([extract_estimator_params_from_gridsearch(p)['n_neighbors'] for p in knn_grid_search_cv.cv_results_['params']],knn_grid_search_cv.cv_results_['mean_test_score'])\n",
    "plt.ylabel('mean_test_score')\n",
    "plt.title('KNN Mean Absolute Error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#graph alpha and mean_test_score\n",
    "#use the log10 of alpha but label the x-axis with alpha\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(np.log10([extract_estimator_params_from_gridsearch(p)['alpha'] for p in lasso_grid_search_cv.cv_results_['params']]),lasso_grid_search_cv.cv_results_['mean_test_score'])\n",
    "plt.xlabel('log10(alpha)')\n",
    "plt.ylabel('mean_test_score')\n",
    "plt.title('Lasso Mean Absolute Error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph alpha and mean_test_score\n",
    "#use the log10 of alpha but label the x-axis with alpha\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(np.log10([p['alpha'] for p in ridge_grid_search_cv.cv_results_['params']]),ridge_grid_search_cv.cv_results_['mean_test_score'])\n",
    "plt.xlabel('log10(alpha)')\n",
    "plt.ylabel('mean_test_score')\n",
    "plt.title('Ridge Mean Absolute Error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph alpha and mean_test_score\n",
    "#use the log10 of alpha but label the x-axis with alpha\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(np.log10([extract_estimator_params_from_gridsearch(p)['alpha'] for p in ridge_grid_search_cv.cv_results_['params']]),ridge_grid_search_cv.cv_results_['mean_test_score'])\n",
    "plt.xlabel('log10(alpha)')\n",
    "plt.ylabel('mean_test_score')\n",
    "plt.title('Ridge Mean Absolute Error')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pick the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cv_results = [ridge_grid_search_cv, lasso_grid_search_cv, elasticnet_grid_search_cv, knn_grid_search_cv]\n",
    "\n",
    "#create a dataframe with the best parameters, best mean_test_score, and name of the model\n",
    "\n",
    "best_params_df = pd.DataFrame({\n",
    "    'model': [cv_result.estimator for cv_result in all_cv_results],\n",
    "    'model_name': [cv_result.estimator.__class__.__name__ for cv_result in all_cv_results],\n",
    "    'best_params': [extract_estimator_params_from_gridsearch(cv_result.best_params_) for cv_result in all_cv_results],\n",
    "    'best_score': [cv_result.best_score_ for cv_result in all_cv_results],\n",
    "    'best_raw_params' : [cv_result.best_params_ for cv_result in all_cv_results]\n",
    "    })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "best_params_df = best_params_df.sort_values('best_score',ascending=False).reset_index(drop=True)\n",
    "\n",
    "best_model = clone(best_params_df['model'][0])\n",
    "best_model_params = best_params_df['best_raw_params'][0]\n",
    "best_model.set_params(**best_model_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do next:\n",
    "\n",
    " - normalize the dataset, using a pipeline so that normalizing is only done on the training set, with the same transformation then done on the test set. \n",
    "   - DONE\n",
    " - after that, we'll have a reasonable pipeline for model selection, but we then need to embed that in a nested CV for estimating generalization.\n",
    " - Need to deal with the "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression seems to perform best so far, on this dataset. If the Ridge regression performs best on the actual dataset, we would potentially choose it. But we might want to rule it out from the start because it doesn't entirely eliminate variables that aren't predictive. Note that the actual, final dataset might be quite different and there's no real reason to think that it will respond in the same wya. In particular, it might choose an entirely different model. we should also ensure the grid-search has a range of params that is wider than the range we specialized on here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizing ridge regression using nested cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the sample into 10 folds using sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, KFold\n",
    "\n",
    "\n",
    "\n",
    "n_folds = 10\n",
    "#make sure there are an equal number of samples from each class in each fold\n",
    "kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# for each fold, run a grid search to find the best parameters\n",
    "# then use hte best parameters to fit the model and predict the test set\n",
    "# then calculate the mean absolute error for the test set\n",
    "# repeat for each fold\n",
    "# then average the mean absolute error across all folds\n",
    "# then pick the best fitting parameter set to generate a final model\n",
    "# that final model will be somewhat overfit, but we can perhaps use a control to make sure it's not excessively overfit\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things I need to sort out here:\n",
    "\n",
    "1. Is the pattern of GridSearchCV followed by cross_val_score enough? See https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\n",
    "2. How do we do the nested cross-validation, with the constraint that we use the same sets of groups for the inner and outer CV?\n",
    "\n",
    "It seems that whether we do GridSearchCV followed by cross_val_score, or follow the manual pattern I was planning, we need either a custom CV or a custom iterable. Perhaps we should try a custom CV class, wrapping StratifiedKFold, which stratifies on a third variable passed to the constructor.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IndependentVarStratifiedKFold function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import column_or_1d\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "class IndependentVarStratifiedKFold(StratifiedKFold):\n",
    "    def __init__(self, independent_vars, n_splits=5, shuffle=False, random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "        self.independent_vars = independent_vars\n",
    "\n",
    "    # def _iter_test_masks(self, X, y=None, groups=None):\n",
    "    #     test_folds = self._make_test_folds(X, y)\n",
    "    #     for i in range(self.n_splits):\n",
    "    #         yield test_folds == i\n",
    "\n",
    "    def _iter_test_indices(self, X, y, groups):\n",
    "        unique_independent_vars, counts = np.unique(self.independent_vars, return_counts=True)\n",
    "        for test_index in super()._iter_test_indices(X, self.independent_vars, groups):\n",
    "            test_independent_vars = self.independent_vars[test_index]\n",
    "            _, test_counts = np.unique(test_independent_vars, return_counts=True)\n",
    "            if np.all(counts >= test_counts):\n",
    "                yield test_index\n",
    "\n",
    "    def _make_test_folds(self, X, y=None):\n",
    "        rng = check_random_state(self.random_state)\n",
    "        g = np.asarray(self.independent_vars)\n",
    "        type_of_target_g = type_of_target(g)\n",
    "        allowed_target_types = (\"binary\", \"multiclass\")\n",
    "        if type_of_target_g not in allowed_target_types:\n",
    "            warnings.warn(\n",
    "                \"Supported target types are: {}. Got {!r} instead.\".format(\n",
    "                    allowed_target_types, type_of_target_g\n",
    "                )\n",
    "            )\n",
    "\n",
    "        g = column_or_1d(g)\n",
    "\n",
    "        _, g_idx, g_inv = np.unique(g, return_index=True, return_inverse=True)\n",
    "        # y_inv encodes y according to lexicographic order. We invert y_idx to\n",
    "        # map the classes so that they are encoded by order of appearance:\n",
    "        # 0 represents the first label appearing in y, 1 the second, etc.\n",
    "        _, class_perm = np.unique(g_idx, return_inverse=True)\n",
    "        g_encoded = class_perm[g_inv]\n",
    "\n",
    "        n_classes = len(g_idx)\n",
    "        g_counts = np.bincount(g_encoded)\n",
    "        min_groups = np.min(g_counts)\n",
    "        if np.all(self.n_splits > g_counts):\n",
    "            raise ValueError(\n",
    "                \"n_splits=%d cannot be greater than the\"\n",
    "                \" number of members in each class.\" % (self.n_splits)\n",
    "            )\n",
    "        if self.n_splits > min_groups:\n",
    "            warnings.warn(\n",
    "                \"The least populated class in y has only %d\"\n",
    "                \" members, which is less than n_splits=%d.\"\n",
    "                % (min_groups, self.n_splits),\n",
    "                UserWarning,\n",
    "            )\n",
    "\n",
    "        # Determine the optimal number of samples from each class in each fold,\n",
    "        # using round robin over the sorted y. (This can be done direct from\n",
    "        # counts, but that code is unreadable.)\n",
    "        g_order = np.sort(g_encoded)\n",
    "        allocation = np.asarray(\n",
    "            [\n",
    "                np.bincount(g_order[i :: self.n_splits], minlength=n_classes)\n",
    "                for i in range(self.n_splits)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # To maintain the data order dependencies as best as possible within\n",
    "        # the stratification constraint, we assign samples from each class in\n",
    "        # blocks (and then mess that up when shuffle=True).\n",
    "        test_folds = np.empty(len(g), dtype=\"i\")\n",
    "        for k in range(n_classes):\n",
    "            # since the kth column of allocation stores the number of samples\n",
    "            # of class k in each test set, this generates blocks of fold\n",
    "            # indices corresponding to the allocation for class k.\n",
    "            folds_for_class = np.arange(self.n_splits).repeat(allocation[:, k])\n",
    "            if self.shuffle:\n",
    "                rng.shuffle(folds_for_class)\n",
    "            test_folds[g_encoded == k] = folds_for_class\n",
    "        return test_folds\n",
    "\n",
    "\n",
    "# create a stratified k-fold object\n",
    "# the independent variable is the 'group' column\n",
    "# this will ensure that each fold has an equal number of samples from each group\n",
    "# this is important because we want to make sure that each fold has an equal number of samples from each class\n",
    "\n",
    "cv = IndependentVarStratifiedKFold(independent_vars=group_assignments_nona, n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "# now do cv.split and print the items in each fold\n",
    "for train_ik, test_i in cv.split(predictor_data_nona, group_assignments_nona):\n",
    "    print(\"TRAIN:\", np.array(train_ik), \"TEST:\", np.array(test_i))\n",
    "    #test to see if this works on the group assignments\n",
    "    print(\"train:\")\n",
    "    print(pd.Series(group_assignments_nona[train_ik]).value_counts())\n",
    "    print(\"test:\")\n",
    "    print(pd.Series(group_assignments_nona[test_i]).value_counts())\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# create a stratified k-fold object\n",
    "# the independent variable is the 'group' column\n",
    "# this will ensure that each fold has an equal number of samples from each group\n",
    "# this is important because we want to make sure that each fold has an equal number of samples from each class\n",
    "outer_splits = 10\n",
    "inner_splits = outer_splits - 1\n",
    "\n",
    "outer_cv = IndependentVarStratifiedKFold(independent_vars=group_assignments_nona, n_splits=outer_splits, shuffle=True, random_state=3211050)\n",
    "#hold up. how does cross_val_score manage with an outer and inner CV that are defined at the same time?\n",
    "#maybe it doesn't matter.\n",
    "\n",
    "\n",
    "# now do cv.split and print the items in each fold\n",
    "for i, (train_i, test_i) in enumerate(outer_cv.split(predictor_data_nona, outcome_measures_nona['d_bf'])):\n",
    "    print(\"outer split\" + str(i))\n",
    "    #print(str(train_ik) + \", \" + str(test_i))\n",
    "\n",
    "    #the problem with this design is we STILL, if we want to pass this to a nested CV, have to split *further* in order to hold out one component of data for validation\n",
    "    #if we use the GridSearchCV followed by cross_val_score pattern, it *might* work???\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"TRAIN:\", np.array(train_i), \"TEST:\", np.array(test_i))\n",
    "    #test to see if this works on the group assignments\n",
    "    print(\"train:\"+ str(dict(pd.Series(group_assignments_nona[train_i]).value_counts())) + \", \" + \n",
    "          \"test:\" + \n",
    "        str(dict(pd.Series(group_assignments_nona[test_i]).value_counts()))\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    train_i_X = predictor_data_nona.iloc[train_i]\n",
    "    train_i_y = outcome_measures_nona['d_bf'].iloc[train_i]\n",
    "    train_i_group_assignments = group_assignments_nona[train_i]\n",
    "\n",
    "    inner_cv = IndependentVarStratifiedKFold(independent_vars=train_i_group_assignments, n_splits=inner_splits, shuffle=True, random_state=3211050)\n",
    "    for j, (train_j, test_j) in enumerate(inner_cv.split(train_i_X, train_i_y)):\n",
    "        print(\"inner split \" + str(j))\n",
    "        #print(\"TRAIN:\", np.array(train_i[train_j]), \"TEST:\", np.array(train_i[test_j]))\n",
    "        print(\"train:\" + str(dict(pd.Series(train_i_group_assignments[train_j]).value_counts())) + \"; \"\n",
    "              \"test:\" + str(dict(pd.Series(train_i_group_assignments[test_j]).value_counts())))\n",
    "    \n",
    "    #gridsearch = GridSearchCV(estimator=Ridge(), param_grid={'alpha':[0.1,0.5,0.9]}, cv=inner_cv)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try GridSearchCV with cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# inner_cv = IndependentVarStratifiedKFold(independent_vars=group_assignments_nona, n_splits=4, shuffle=True, random_state=0)\n",
    "# outer_cv = IndependentVarStratifiedKFold(independent_vars=group_assignments_nona, n_splits=4, shuffle=True, random_state=0)\n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "\n",
    "\n",
    "gridsearch = GridSearchCV(estimator=Ridge(), param_grid={'alpha':[0.1,0.5,0.9]}, cv=inner_cv)\n",
    "nested_score = cross_val_score(gridsearch, X=predictor_data_nona, y=outcome_measures_nona['d_bf'], cv=outer_cv)\n",
    "nested_scores = nested_score.mean()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a function to do CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import clone\n",
    "from sklearn.base import clone\n",
    "\n",
    "def do_hyperparameter_selection_on_fold(X, y,cv):\n",
    "    alpha_10pow_lower = 6\n",
    "    alpha_10pow_upper = -1\n",
    "    alpha_increments=1\n",
    "    alpha_range = np.power(10,np.linspace(-alpha_10pow_lower,alpha_10pow_upper,(alpha_10pow_lower+alpha_10pow_upper)*alpha_increments+1))\n",
    "\n",
    "    ############\n",
    "    #RIDGE\n",
    "    ridge_parameters = {'alpha':alpha_range}\n",
    "    ridge_model = linear_model.Ridge()\n",
    "    print(ridge_parameters)\n",
    "    #do a gridsearch, using the same folds as the outer loop\n",
    "    ridge_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(ridge_model), param_grid = get_param_grid_with_preprocessing(ridge_parameters), cv=cv,scoring='neg_mean_absolute_error')\n",
    "    ridge_grid_search_cv.fit(X,y)\n",
    "\n",
    "    ############\n",
    "    #LASSO\n",
    "    lasso_parameters = {'alpha':alpha_range}\n",
    "    lasso_model = linear_model.Lasso()\n",
    "    print(lasso_parameters)\n",
    "    lasso_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(lasso_model), param_grid = get_param_grid_with_preprocessing(lasso_parameters), cv=cv,scoring='neg_mean_absolute_error')\n",
    "    lasso_grid_search_cv.fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "    all_cv_results = [ridge_grid_search_cv, lasso_grid_search_cv]\n",
    "\n",
    "    #create a dataframe with the best parameters, best mean_test_score, and name of the model\n",
    "\n",
    "    best_params_df = pd.DataFrame({\n",
    "        'model': [cv_result.estimator for cv_result in all_cv_results],\n",
    "        'model_name': [cv_result.estimator.__class__.__name__ for cv_result in all_cv_results],\n",
    "        'best_params': [extract_estimator_params_from_gridsearch(cv_result.best_params_) for cv_result in all_cv_results],\n",
    "        'best_score': [cv_result.best_score_ for cv_result in all_cv_results],\n",
    "        'best_raw_params' : [cv_result.best_params_ for cv_result in all_cv_results]\n",
    "        })\n",
    "    \n",
    "    best_params_df = best_params_df.sort_values('best_score',ascending=False).reset_index(drop=True)\n",
    "\n",
    "    best_model = clone(best_params_df['model'][0])\n",
    "    best_model_params = best_params_df['best_raw_params'][0]\n",
    "    best_model.set_params(**best_model_params)\n",
    "\n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_params_df':best_params_df,\n",
    "        'raw_cv_results':all_cv_results\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out CV with simple gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "outer_splits = 4\n",
    "inner_splits = outer_splits - 1\n",
    "\n",
    "outer_cv = IndependentVarStratifiedKFold(independent_vars=group_assignments_nona, n_splits=outer_splits, shuffle=True, random_state=3211050)\n",
    "#hold up. how does cross_val_score manage with an outer and inner CV that are defined at the same time?\n",
    "#maybe it doesn't matter.\n",
    "\n",
    "scores = []\n",
    "\n",
    "best_models = []\n",
    "best_params_df_list = []\n",
    "raw_cv_results_list = []\n",
    "\n",
    "# now do cv.split and print the items in each fold\n",
    "for i, (train_i, test_i) in enumerate(outer_cv.split(predictor_data_nona, outcome_measures_nona['d_bf'])):\n",
    "    print(\"outer split\" + str(i))\n",
    "\n",
    "    #test to see if this works on the group assignments\n",
    "    print(\"train:\"+ str(dict(pd.Series(group_assignments_nona[train_i]).value_counts())) + \", \" + \n",
    "          \"test:\" + \n",
    "        str(dict(pd.Series(group_assignments_nona[test_i]).value_counts()))\n",
    "    )\n",
    "\n",
    "    train_i_X = predictor_data_nona.iloc[train_i]\n",
    "    train_i_y = outcome_measures_nona['d_bf'].iloc[train_i]\n",
    "    train_i_group_assignments = group_assignments_nona[train_i]\n",
    "    print(train_i_y)\n",
    "\n",
    "    test_i_X = predictor_data_nona.iloc[test_i]\n",
    "    test_i_y = outcome_measures_nona['d_bf'].iloc[test_i]\n",
    "    print(test_i_y)\n",
    "\n",
    "    inner_cv = IndependentVarStratifiedKFold(independent_vars=train_i_group_assignments, n_splits=inner_splits, shuffle=True, random_state=3211050)\n",
    "\n",
    "    selection_info = do_hyperparameter_selection_on_fold(train_i_X, train_i_y,cv = inner_cv)\n",
    "    best_model_i = selection_info['best_model']\n",
    "    best_params_i = selection_info['best_params_df']\n",
    "    best_models.append(best_model_i)\n",
    "    best_params_df_list.append(best_params_i)\n",
    "    raw_cv_results_list.append(selection_info['raw_cv_results'])\n",
    "\n",
    "    best_model_i.fit(train_i_X, train_i_y)\n",
    "    score_r2_i = best_model_i.score(test_i_X, test_i_y)\n",
    "\n",
    "    scores.append(score_r2_i)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    #now fit the model on the training data\n",
    "    #gridsearch.fit(train_i_X, train_i_y)\n",
    "\n",
    "    #model_with_hypers = gridsearch.cv_results_.keys()\n",
    "    #print(model_with_hypers)\n",
    "\n",
    "    #score = model_with_hypers.score(test_i_X, test_i_y)\n",
    "    #scores.append(score)\n",
    "\n",
    "\n",
    "    #best_score = gridsearch.score(train_i_X.iloc[test_j], train_i_y.iloc[test_j])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n",
    "overall_score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.warn(\"Need to test how this runs with different numbers of variables because I'm getting wildly varying performance scores.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to pick the best model, including model parameters. Two ways to do it; one is to follow the way its done in CV; the otehr is to do it like this:\n",
    "\n",
    " - We use each fold to 'vote' for a model, including the particular parameters used.\n",
    " - Possibly it will be a tie in that there will be two or more models with specific parameters that are chosen as best; possibly every single fold will have selected a slightly different set of parameters. In that case, if two or more models are 'first equal', eliminate the others, then select the model that got the highest test performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we followed how it's one in CV, perhaps we just run GridSearchCV one more time, across the entire dataset, to select model parameters. hmm. That is _definitely_ going to over-fit, but I'm not sure it will be biased to choose excessive precision, because we'll still be doing train/test analysis.\n",
    "\n",
    "\n",
    "Alternatively we could just use the data that GridSearchCV has _already generated_ to select the best model, by (I assume) adding up the performance for each model over the whole GridSearch and picking the best one. That would involve extracting performance for _every single model_ across ALL GridSearches and then combining them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_list = []\n",
    "for i, raw_result_fold_i in enumerate(raw_cv_results_list):\n",
    "    for gsm in raw_cv_results_list[i]:\n",
    "        gsm_j_cv_results_df = pd.DataFrame(gsm.cv_results_)\n",
    "        gsm_j_cv_results_df['fold'] = i\n",
    "        gsm_j_cv_results_df['model_description'] = str(gsm.estimator.named_steps.values())\n",
    "        gsm_j_cv_results_df['model'] = gsm.estimator\n",
    "        \n",
    "        cv_results_list.append(gsm_j_cv_results_df)\n",
    "\n",
    "cv_results_df = pd.concat(cv_results_list)\n",
    "cv_results_df['params_str'] = cv_results_df['params'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by model_description and params across folds and get the mean and std of the mean and std test scores\n",
    "\n",
    "list_model_performance = (cv_results_df\n",
    " .groupby(['model_description','params_str'])\n",
    " .agg({'mean_test_score':['mean','std'],'std_test_score':['mean','std']})\n",
    " .sort_values(('mean_test_score','mean'),ascending=False)\n",
    ")\n",
    "\n",
    "list_model_performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's one way to do it! I am still concerned about overfitting.\n",
    "\n",
    "At least we won't be picking hyper-parameters that do excessive overfitting, but we might still be over-optimizing hyper-parameters on this specific dataset. That would lead to an inflated performance estimate, but we are not using this for peformance. It would also lead to an inflated fit of the parameters to the data, but  (1) it's not clear any of the _other_ fits would be better; (2) because the hyper-parameters are taken from the cross-validation process, \n",
    "\n",
    "Perhaps it's better to re-run the GridSearchCV one more time on the full dataset; that would be less optimization, I think."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_fits =  (cv_results_df\n",
    " .groupby(['model_description','params_str'])\n",
    " .agg({'mean_test_score':['mean','std'],'std_test_score':['mean','std']})\n",
    ").reset_index()\n",
    "\n",
    "#identify the index of best fit\n",
    "best_fit_description = overall_fits[overall_fits[('mean_test_score','mean')]==overall_fits[('mean_test_score','mean')].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit_characteristics =  cv_results_df.loc[((cv_results_df['model_description']==best_fit_description['model_description'].values[0]) & \n",
    "                   (cv_results_df['params_str']==best_fit_description['params_str'].values[0])),:].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = clone(best_fit_characteristics.model)\n",
    "best_model_params = best_fit_characteristics.params\n",
    "best_model.set_params(**best_model_params)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, estimate based on ALL the data to get a set of regressors we can report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fit = best_model.fit(predictor_data_nona, outcome_measures_nona['d_bf'])\n",
    "final_estimator = final_fit.named_steps['estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the coefficients\n",
    "final_results = pd.DataFrame({\n",
    "    'predictor': predictor_data_nona.columns,\n",
    "    'coef': final_estimator.coef_\n",
    "    #'std_err': np.sqrt(np.diag(model_fit.coef_cov_)),\n",
    "    #'pval': 2*(1-stats.t.cdf(np.abs(model_fit.coef_/np.sqrt(np.diag(model_fit.coef_cov_))),df=predictor_data_nona.shape[0]-predictor_data_nona.shape[1]))\n",
    "})\n",
    "\n",
    "final_results['coef_abs'] = np.abs(final_results.coef)\n",
    "final_results = final_results.sort_values('coef_abs',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_count = np.sum(final_estimator.coef_!=0)\n",
    "print(f\"Number of selected features: {selected_features_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataanalysis3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "014247d405695287815678bf9349a8dffb2674e9fe9a5bd4bb9820af018d638d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
