{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from socket import gethostname\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from dev_interaction_util import generate_synthetic_dev_outcomes, generate_synthetic_dev_data, set_up_interactions\n",
    "from dev_interaction_util import do_scoring_loop, get_best_model, summarize_overall_df_results, do_final_fit, present_model_results, present_results_vs_ground_truth_cors\n",
    "from dev_interaction_util import load_and_preprocess_data, impute_data\n",
    "from ml_util import *\n",
    "# Imputing with MICE\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn import linear_model\n",
    "from ml_util import get_data_for_imputation\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.base import clone\n",
    "from sklearn.inspection import permutation_importance\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benjamins-MacBook-Pro-2.local\n",
      "{'dropbox_data_dir': '/Users/benjaminsmith/Dropbox (University of Oregon)/UO-SAN Lab/Berkman Lab/Devaluation/analysis_files/data/'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(gethostname())\n",
    "# Open the file and load the file\n",
    "with open('config.yml') as f:\n",
    "    all_yaml = yaml.load(f, Loader=SafeLoader)\n",
    "    if gethostname() in all_yaml.keys():\n",
    "        config = all_yaml[gethostname()]\n",
    "    else:\n",
    "        config = all_yaml['default']\n",
    "        \n",
    "print(config)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is derived from `test_feature_selection.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropbox_data_dir = config['dropbox_data_dir']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data, outcome_measures = load_and_preprocess_data(dropbox_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/impute/_iterative.py:713: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "analysis_data_imputed = impute_data(analysis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loops through the different estimators and feature selection methods and does a grid search over all to find the best hyperparameters\n",
    "def do_hyperparameter_selection_loop(X, y,cv):\n",
    "    #alpha parameters for Ridge and Lasso\n",
    "    alpha_10pow_lower = 1\n",
    "    alpha_10pow_upper = 0\n",
    "    alpha_increments=1\n",
    "    alpha_range = np.concatenate([np.power(10,np.linspace(-alpha_10pow_lower,alpha_10pow_upper,(alpha_10pow_lower+alpha_10pow_upper)*alpha_increments+1)),\n",
    "        [0.2,0.4,0.6,0.8,1.0]])\n",
    "    \n",
    "    all_cv_results = []\n",
    "\n",
    "    pipeline_estimator_name = 'estimator'\n",
    "    feature_selection_name = 'feature_selection'\n",
    "\n",
    "\n",
    "    #define the param_grid for the estimators\n",
    "    estimators_to_run = {\n",
    "        'Ridge':{\n",
    "            'estimator':linear_model.Ridge,\n",
    "            'parameters':{'alpha':alpha_range}\n",
    "        },\n",
    "        'Lasso':{\n",
    "            'estimator':linear_model.Lasso,\n",
    "            'parameters':{'alpha':alpha_range}\n",
    "        },\n",
    "        'DecisionTreeRegressor':{\n",
    "            'estimator':DecisionTreeRegressor,\n",
    "            'parameters':{\n",
    "                'max_depth':[2, 4],\n",
    "                'min_samples_split':[20,50],\n",
    "                'min_samples_leaf':[20,50]\n",
    "            }\n",
    "        }             \n",
    "    }\n",
    "\n",
    "    k_max_val = np.min([50,X.shape[1]])\n",
    "\n",
    "    for estimator_name,estimator_dict in estimators_to_run.items():\n",
    "        #param grid for the feature seelction\n",
    "        #this is here because we need to know the estimator to pass to the feature selector\n",
    "        feature_selectors_to_run = {\n",
    "            'None':None,\n",
    "            'KBest':{\n",
    "                'selector':SelectKBest(),\n",
    "                'parameters':{\n",
    "                    'score_func' : [f_regression], \n",
    "                    'k' : [20,k_max_val]\n",
    "                    }\n",
    "            },\n",
    "            'RFE':{\n",
    "                'selector':RFE(linear_model.LinearRegression()),\n",
    "                'parameters':{\n",
    "                    'n_features_to_select' : [10,25],\n",
    "                    #'verbose':[1],\n",
    "                    'step':[5]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        for selector_name, selector_dict in feature_selectors_to_run.items():\n",
    "        #create the estimator\n",
    "            if selector_name == 'None':\n",
    "                pipeline = Pipeline([('scaler',StandardScaler()),\n",
    "                                     (pipeline_estimator_name,estimator_dict['estimator']())])\n",
    "                selector_params = {}\n",
    "            else:\n",
    "                pipeline = Pipeline([('scaler',StandardScaler()),\n",
    "                                     (feature_selection_name,selector_dict['selector']), \n",
    "                                     (pipeline_estimator_name,estimator_dict['estimator']())])\n",
    "                selector_params = selector_dict['parameters']\n",
    "\n",
    "            estimator_param_grid = {(pipeline_estimator_name + '__'+k):v for k,v in estimator_dict['parameters'].items()}\n",
    "            selector_param_grid = {(feature_selection_name + '__'+k):v for k,v in selector_params.items()}\n",
    "            #combine the two param grid dictionaries\n",
    "            full_param_grid = {**selector_param_grid, **estimator_param_grid}\n",
    "            print(pipeline)\n",
    "            print(full_param_grid)\n",
    "\n",
    "            \n",
    "        \n",
    "            gs_1 = GridSearchCV(estimator=pipeline, \n",
    "                                param_grid = full_param_grid, \n",
    "                                cv=cv,scoring='neg_mean_absolute_error',verbose=1)\n",
    "            gs_1.fit(X,y)\n",
    "            all_cv_results.append(gs_1)\n",
    "\n",
    "    #create a dataframe with the best parameters, best mean_test_score, and name of the model\n",
    "\n",
    "    best_params_df = pd.DataFrame({\n",
    "        'model': [cv_result.estimator for cv_result in all_cv_results],\n",
    "        'model_name': [cv_result.estimator.__class__.__name__ for cv_result in all_cv_results],\n",
    "        'best_params': [extract_estimator_params_from_gridsearch(cv_result.best_params_) for cv_result in all_cv_results],\n",
    "        'best_score': [cv_result.best_score_ for cv_result in all_cv_results],\n",
    "        'best_raw_params' : [cv_result.best_params_ for cv_result in all_cv_results]\n",
    "        })\n",
    "    \n",
    "    best_params_df = best_params_df.sort_values('best_score',ascending=False).reset_index(drop=True)\n",
    "\n",
    "    best_model = clone(best_params_df['model'][0])\n",
    "    best_model_params = best_params_df['best_raw_params'][0]\n",
    "    best_model.set_params(**best_model_params)\n",
    "\n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_params_df':best_params_df,\n",
    "        'raw_cv_results':all_cv_results\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving fit with manual theory-driven feature\n",
    "\n",
    "My past analysis showed that by manually removing some features before the analysis starts, we can improve performance beyond the chance performance otherwise seen.\n",
    "\n",
    "So, it might be useful to understand how much we can improve our performance by manual feature selection before the automatic feature selection applies.\n",
    "\n",
    "This was previously done in `test_limited_predictors.ipynb`. We tested as few as 2 distractor features. In that test, predictor features generally had correlations in the range of |r|=0.06 to 0.53, with most around 0.4 (we should confirm that because it seems fishy that PCS was detegted as an effect, but didn't model as a large predictor). With most `|r|=0.4`, this seems unrealistically high to expect, and we should aim to build a pipeline capable of detecting more subtle effects than that. An approximate `|r|=0.3` can be achieved by mixing in a predictor scaled to 8% of normal scale.\n",
    "\n",
    "I can imagine it is plausible to cut down to as few as two self-report, one behavioral, and one neural measure per intervention, plus sex and age. That would yield 10 different variables. At the other end, we might want 10 self-report, two behavioral, and five neural measures per intervention tested, plus 6 different demographic variables--a total of 40 variables. Let's see how these would perform, as well as mid-range of 20 predictor variables. In each case we'll restrict to three valid predictors per intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty data frame with numeric two columns, n_features and overall_score\n",
    "overall_scores = pd.DataFrame(columns=['n_features','overall_score'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_limited_predictor_analysis(total_predictor_count, outcome_measures, analysis_data_imputed):\n",
    "\n",
    "    #set np random seed\n",
    "    np.random.seed(3161527)\n",
    "\n",
    "    group_names = ['ichi','ni','san']\n",
    "    #assign each row randomly to a group\n",
    "    group_assignments = np.random.choice(group_names,analysis_data_imputed.shape[0])\n",
    "    \n",
    "\n",
    "    #synthetic outcomes\n",
    "    outcome_measures = generate_synthetic_dev_outcomes(outcome_measures)\n",
    "\n",
    "    #create a limited set of predictors\n",
    "    analysis_data_smol = analysis_data_imputed.iloc[:,0:total_predictor_count]\n",
    "\n",
    "    # add synthetic primary and interaction effects\n",
    "\n",
    "\n",
    "    #set up the interaction effects\n",
    "    #0.08 will give us correlations around 0.3 between the interaction effects and the outcome\n",
    "    custom_interaction_effects_g1 = [0]*analysis_data_smol.shape[1]\n",
    "    custom_interaction_effects_g1[0] = 0.08\n",
    "    custom_interaction_effects_g1[1] = 0.08\n",
    "    custom_interaction_effects_g1[2] = -0.08\n",
    "\n",
    "    custom_interaction_effects_g2 = [0]*analysis_data_smol.shape[1]\n",
    "    custom_interaction_effects_g2[4] = 0.08\n",
    "    custom_interaction_effects_g2[5] = 0.08\n",
    "    custom_interaction_effects_g2[6] = -0.08\n",
    "\n",
    "    custom_interaction_effects = {'ni':custom_interaction_effects_g1,'san':custom_interaction_effects_g2}\n",
    "\n",
    "\n",
    "\n",
    "    synthetic_data = generate_synthetic_dev_data(analysis_data_smol, group_assignments,outcome_measures, group_interaction_effects = custom_interaction_effects)\n",
    "    interaction_effect_df = synthetic_data['X_weights']\n",
    "    outcome_measures = synthetic_data['y']\n",
    "\n",
    "    # Set up outcome measures and group assignment one-hot\n",
    "\n",
    "    outcome_measures = calculate_outcome_changes(outcome_measures)\n",
    "    group_assignment_onehots = pd.get_dummies(group_assignments).loc[:,['ni','san']]\n",
    "\n",
    "    predictor_data = set_up_interactions(analysis_data_smol, group_assignment_onehots)\n",
    "\n",
    "\n",
    "    #remove any NA values for this outcome measure in both the predictor data and the outcome data\n",
    "    outcome_nas = outcome_measures['d_bf'].isna()\n",
    "\n",
    "    outcome_measures_nona = outcome_measures.loc[~outcome_nas,:]\n",
    "    predictor_data_nona = predictor_data.loc[~outcome_nas,:]\n",
    "    group_assignment_onehots_nonan = group_assignment_onehots.loc[~outcome_nas,:]\n",
    "    group_assignments_nona = group_assignments[~outcome_nas]\n",
    "\n",
    "    ### Try out CV with simple gridsearch\n",
    "\n",
    "    scoring_data = do_scoring_loop(X=predictor_data_nona, y= outcome_measures_nona['d_bf'], \n",
    "                    groups = group_assignments_nona, \n",
    "                    hyperparameter_selection_on_fold=do_hyperparameter_selection_loop,\n",
    "                    outer_folds=5)\n",
    "\n",
    "    scores = scoring_data['scores']\n",
    "    best_models = scoring_data['best_models']\n",
    "    best_params_df_list = scoring_data['best_params_df_list']\n",
    "    raw_cv_results_list = scoring_data['raw_cv_results_list']\n",
    "\n",
    "    print(\"scores:\")\n",
    "    print(scores)\n",
    "    overall_score = np.mean(scores)\n",
    "    print(\"overall_score:\")\n",
    "    print(overall_score)\n",
    "\n",
    "\n",
    "\n",
    "    best_model = get_best_model(summarize_overall_df_results(raw_cv_results_list))\n",
    "    final_fit = do_final_fit(X=predictor_data_nona, y= outcome_measures_nona['d_bf'], final_model=best_model)\n",
    "    final_results = present_model_results(X=predictor_data_nona, final_fit=final_fit, y=outcome_measures_nona['d_bf'])\n",
    "\n",
    "    #print rows of final_results where feature_name is the list of features to check\n",
    "    base_regressors = interaction_effect_df.predictor[interaction_effect_df.interaction_effect!=0]\n",
    "    regressors_to_check = [x+y for y in ['','*ni','*san'] for x in base_regressors]\n",
    "    final_results['planned_regression'] = final_results['predictor'].isin(regressors_to_check)\n",
    "\n",
    "    present_results_vs_ground_truth_cors(predictor_data_nona,outcome_measures_nona,group_assignments_nona,final_results,base_regressors)\n",
    "\n",
    "    return(overall_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ni' 'san']\n",
      "[1.28335298 0.42953651]\n",
      "['san' 'san' 'ni' 'ichi' 'san' 'san' 'ichi' 'san' 'san' 'san' 'ni' 'ichi'\n",
      " 'ichi' 'ichi' 'ichi' 'san' 'san' 'san' 'ichi' 'ichi' 'san' 'san' 'ni'\n",
      " 'ni' 'ni' 'ni' 'ni' 'ni' 'ni' 'san' 'ni' 'san' 'ni' 'ichi' 'ni' 'san'\n",
      " 'ni' 'ichi' 'san' 'ni' 'ni' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ni' 'ni'\n",
      " 'san' 'ichi' 'ichi' 'ni' 'san' 'ni' 'ichi' 'ni' 'ni' 'ni' 'ichi' 'san'\n",
      " 'ni' 'ni' 'ichi' 'ni' 'ichi' 'san' 'ni' 'ni' 'ni' 'san' 'ichi' 'ni' 'san'\n",
      " 'ichi' 'san' 'ni' 'san' 'ni' 'ichi' 'ichi' 'san' 'ichi' 'san' 'san' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'san' 'ni' 'san' 'ni' 'ichi' 'san' 'san' 'san' 'ichi'\n",
      " 'ni' 'san' 'ichi' 'ichi' 'san' 'ni' 'ichi' 'san' 'ni' 'ni' 'san' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'ichi' 'ni' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ichi'\n",
      " 'ni' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ichi' 'san' 'san' 'ni' 'ichi' 'ni'\n",
      " 'ichi' 'ichi' 'san' 'ichi' 'ni' 'san' 'san' 'ni' 'ni' 'san' 'san' 'san'\n",
      " 'ichi' 'san' 'ni' 'san' 'ichi' 'ichi' 'ichi' 'ni' 'san' 'ni' 'ni' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'san' 'ni' 'san' 'ichi' 'ni' 'ni' 'ni' 'ichi' 'ni'\n",
      " 'ichi' 'san' 'san' 'san' 'san' 'ichi' 'ni' 'san' 'san' 'san' 'ichi' 'san'\n",
      " 'ni' 'ni' 'san' 'ichi' 'ichi' 'san' 'ni' 'ni' 'san' 'ni' 'san' 'san' 'ni'\n",
      " 'san' 'ni' 'ni' 'san' 'ichi' 'san' 'ichi' 'san' 'ni' 'ni' 'ni' 'ichi'\n",
      " 'ni' 'ni' 'san' 'ni' 'ni' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ni' 'ni' 'san'\n",
      " 'ichi' 'ni' 'ichi' 'ichi' 'ichi' 'ichi' 'ichi' 'ichi' 'san' 'ichi' 'san'\n",
      " 'ichi' 'ni' 'san' 'san' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ni' 'san' 'san'\n",
      " 'ichi' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ni' 'ni' 'ni' 'san' 'ichi'\n",
      " 'ichi' 'ichi' 'ni' 'ichi' 'ni' 'san' 'ichi' 'san' 'san' 'ni' 'san' 'san'\n",
      " 'san' 'san' 'ichi' 'ichi' 'ichi' 'ni' 'ni' 'ichi' 'san' 'san' 'san']\n",
      "ni\n",
      "                feature_name  interaction_effect\n",
      "0                       BSCS                0.08\n",
      "1                        EDM                0.08\n",
      "2                     BIS_11               -0.08\n",
      "3                        PCS                0.00\n",
      "4                         RS                0.00\n",
      "5                       TRSQ                0.00\n",
      "6  ACES_neglectful_parenting                0.00\n",
      "7                 ACES_abuse                0.00\n",
      "8                   ACES_sum                0.00\n",
      "9    ACES_divorced_separated                0.00\n",
      "bf_2\n",
      "cancer_promoting_minus_preventing_FFQ_w2\n",
      "FFQ_v2_Mean_Energy_w2\n",
      "san\n",
      "                feature_name  interaction_effect\n",
      "4                         RS                0.08\n",
      "5                       TRSQ                0.08\n",
      "6  ACES_neglectful_parenting               -0.08\n",
      "0                       BSCS                0.00\n",
      "1                        EDM                0.00\n",
      "2                     BIS_11                0.00\n",
      "3                        PCS                0.00\n",
      "7                 ACES_abuse                0.00\n",
      "8                   ACES_sum                0.00\n",
      "9    ACES_divorced_separated                0.00\n",
      "bf_2\n",
      "cancer_promoting_minus_preventing_FFQ_w2\n",
      "FFQ_v2_Mean_Energy_w2\n",
      "(275, 10)\n",
      "(275, 10)\n",
      "outer split0\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x182d42a70>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "28 fits failed out of a total of 56.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "28 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 668, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be >=0, <= n_features = 32; got 50. Use k='all' to return all features.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [-3.43644117         nan -3.40526856         nan -3.43118658         nan\n",
      " -3.42285826         nan -3.41611806         nan -3.41034777         nan\n",
      " -3.40526856         nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Ridge())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x182d42a70>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "28 fits failed out of a total of 56.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "28 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 668, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be >=0, <= n_features = 32; got 50. Use k='all' to return all features.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [-3.33747632         nan -3.3028286          nan -3.32717733         nan\n",
      " -3.3032821          nan -3.29102116         nan -3.29704308         nan\n",
      " -3.3028286          nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Lasso())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x182d42a70>], 'feature_selection__k': [20, 50], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "32 fits failed out of a total of 64.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "32 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 668, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be >=0, <= n_features = 32; got 50. Use k='all' to return all features.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [-3.46966947         nan -3.50818632         nan -3.34313299         nan\n",
      " -3.34313299         nan -3.59563656         nan -3.58260952         nan\n",
      " -3.34313299         nan -3.34313299         nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "outer split1\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x182d42a70>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "28 fits failed out of a total of 56.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "28 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 668, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be >=0, <= n_features = 32; got 50. Use k='all' to return all features.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [-3.27063733         nan -3.24851655         nan -3.26302426         nan\n",
      " -3.25570391         nan -3.25226849         nan -3.25001778         nan\n",
      " -3.24851655         nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Ridge())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x182d42a70>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "28 fits failed out of a total of 56.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "28 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 668, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be >=0, <= n_features = 32; got 50. Use k='all' to return all features.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [-3.2231714          nan -3.18339494         nan -3.1813389          nan\n",
      " -3.15188392         nan -3.16529367         nan -3.18190015         nan\n",
      " -3.18339494         nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Lasso())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x182d42a70>], 'feature_selection__k': [20, 50], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "32 fits failed out of a total of 64.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "32 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 668, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be >=0, <= n_features = 32; got 50. Use k='all' to return all features.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [-3.17462852         nan -3.14952477         nan -3.17233552         nan\n",
      " -3.17233552         nan -3.27179223         nan -3.2323208          nan\n",
      " -3.17233552         nan -3.17233552         nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "outer split2\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x182d42a70>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "28 fits failed out of a total of 56.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "28 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 668, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be >=0, <= n_features = 32; got 50. Use k='all' to return all features.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [-3.28542052         nan -3.2583553          nan -3.28146207         nan\n",
      " -3.27454352         nan -3.26852409         nan -3.26315143         nan\n",
      " -3.2583553          nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Ridge())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x182d42a70>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "28 fits failed out of a total of 56.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "28 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 668, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be >=0, <= n_features = 32; got 50. Use k='all' to return all features.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [-3.29559946         nan -3.37651407         nan -3.32628687         nan\n",
      " -3.34654846         nan -3.36579944         nan -3.37265198         nan\n",
      " -3.37651407         nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Lasso())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x182d42a70>], 'feature_selection__k': [20, 50], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "32 fits failed out of a total of 64.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "32 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py\", line 668, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be >=0, <= n_features = 32; got 50. Use k='all' to return all features.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [-3.44093119         nan -3.42943921         nan -3.38176932         nan\n",
      " -3.38176932         nan -3.47642826         nan -3.45656816         nan\n",
      " -3.38176932         nan -3.38176932         nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#run the analysis with a limited number of predictors\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m overall_score \u001b[39m=\u001b[39m run_full_limited_predictor_analysis(\u001b[39m10\u001b[39;49m, outcome_measures, analysis_data_imputed)\n\u001b[1;32m      5\u001b[0m overall_scores \u001b[39m=\u001b[39m overall_scores\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mn_features\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m10\u001b[39m,\u001b[39m'\u001b[39m\u001b[39moverall_score\u001b[39m\u001b[39m'\u001b[39m:overall_score},ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[18], line 58\u001b[0m, in \u001b[0;36mrun_full_limited_predictor_analysis\u001b[0;34m(total_predictor_count, outcome_measures, analysis_data_imputed)\u001b[0m\n\u001b[1;32m     54\u001b[0m group_assignments_nona \u001b[39m=\u001b[39m group_assignments[\u001b[39m~\u001b[39moutcome_nas]\n\u001b[1;32m     56\u001b[0m \u001b[39m### Try out CV with simple gridsearch\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m scoring_data \u001b[39m=\u001b[39m do_scoring_loop(X\u001b[39m=\u001b[39;49mpredictor_data_nona, y\u001b[39m=\u001b[39;49m outcome_measures_nona[\u001b[39m'\u001b[39;49m\u001b[39md_bf\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     59\u001b[0m                 groups \u001b[39m=\u001b[39;49m group_assignments_nona, \n\u001b[1;32m     60\u001b[0m                 hyperparameter_selection_on_fold\u001b[39m=\u001b[39;49mdo_hyperparameter_selection_loop,\n\u001b[1;32m     61\u001b[0m                 outer_folds\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[1;32m     63\u001b[0m scores \u001b[39m=\u001b[39m scoring_data[\u001b[39m'\u001b[39m\u001b[39mscores\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     64\u001b[0m best_models \u001b[39m=\u001b[39m scoring_data[\u001b[39m'\u001b[39m\u001b[39mbest_models\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Google Drive/oregon/code/DEV_scripts/analyses/intervention_moderation/dev_interaction_util.py:198\u001b[0m, in \u001b[0;36mdo_scoring_loop\u001b[0;34m(X, y, groups, hyperparameter_selection_on_fold, outer_folds)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m#print(test_i_y)\u001b[39;00m\n\u001b[1;32m    196\u001b[0m inner_cv \u001b[39m=\u001b[39m IndependentVarStratifiedKFold(independent_vars\u001b[39m=\u001b[39mtrain_i_group_assignments, n_splits\u001b[39m=\u001b[39minner_splits, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39m3211050\u001b[39m)\n\u001b[0;32m--> 198\u001b[0m selection_info \u001b[39m=\u001b[39m hyperparameter_selection_on_fold(train_i_X, train_i_y,cv \u001b[39m=\u001b[39;49m inner_cv)\n\u001b[1;32m    199\u001b[0m best_model_i \u001b[39m=\u001b[39m selection_info[\u001b[39m'\u001b[39m\u001b[39mbest_model\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    200\u001b[0m best_params_i \u001b[39m=\u001b[39m selection_info[\u001b[39m'\u001b[39m\u001b[39mbest_params_df\u001b[39m\u001b[39m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[6], line 81\u001b[0m, in \u001b[0;36mdo_hyperparameter_selection_loop\u001b[0;34m(X, y, cv)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[39mprint\u001b[39m(full_param_grid)\n\u001b[1;32m     78\u001b[0m         gs_1 \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mpipeline, \n\u001b[1;32m     79\u001b[0m                             param_grid \u001b[39m=\u001b[39m full_param_grid, \n\u001b[1;32m     80\u001b[0m                             cv\u001b[39m=\u001b[39mcv,scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mneg_mean_absolute_error\u001b[39m\u001b[39m'\u001b[39m,verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m         gs_1\u001b[39m.\u001b[39;49mfit(X,y)\n\u001b[1;32m     82\u001b[0m         all_cv_results\u001b[39m.\u001b[39mappend(gs_1)\n\u001b[1;32m     84\u001b[0m \u001b[39m#create a dataframe with the best parameters, best mean_test_score, and name of the model\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1378\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1379\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    824\u001b[0m         clone(base_estimator),\n\u001b[1;32m    825\u001b[0m         X,\n\u001b[1;32m    826\u001b[0m         y,\n\u001b[1;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    833\u001b[0m     )\n\u001b[1;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    836\u001b[0m     )\n\u001b[1;32m    837\u001b[0m )\n\u001b[1;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:708\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    705\u001b[0m result[\u001b[39m\"\u001b[39m\u001b[39mfit_error\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    707\u001b[0m fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m--> 708\u001b[0m test_scores \u001b[39m=\u001b[39m _score(estimator, X_test, y_test, scorer, error_score)\n\u001b[1;32m    709\u001b[0m score_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time \u001b[39m-\u001b[39m fit_time\n\u001b[1;32m    710\u001b[0m \u001b[39mif\u001b[39;00m return_train_score:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:767\u001b[0m, in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[1;32m    765\u001b[0m         scores \u001b[39m=\u001b[39m scorer(estimator, X_test)\n\u001b[1;32m    766\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m         scores \u001b[39m=\u001b[39m scorer(estimator, X_test, y_test)\n\u001b[1;32m    768\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m error_score \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:219\u001b[0m, in \u001b[0;36m_BaseScorer.__call__\u001b[0;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, estimator, X, y_true, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    197\u001b[0m     \u001b[39m\"\"\"Evaluate predicted target values for X relative to y_true.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \n\u001b[1;32m    199\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39m        Score function applied to prediction of estimator on X.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_score(\n\u001b[1;32m    220\u001b[0m         partial(_cached_call, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    221\u001b[0m         estimator,\n\u001b[1;32m    222\u001b[0m         X,\n\u001b[1;32m    223\u001b[0m         y_true,\n\u001b[1;32m    224\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    225\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:261\u001b[0m, in \u001b[0;36m_PredictScorer._score\u001b[0;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_score\u001b[39m(\u001b[39mself\u001b[39m, method_caller, estimator, X, y_true, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    234\u001b[0m     \u001b[39m\"\"\"Evaluate predicted target values for X relative to y_true.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39m        Score function applied to prediction of estimator on X.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     y_pred \u001b[39m=\u001b[39m method_caller(estimator, \u001b[39m\"\u001b[39;49m\u001b[39mpredict\u001b[39;49m\u001b[39m\"\u001b[39;49m, X)\n\u001b[1;32m    262\u001b[0m     \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sign \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_score_func(\n\u001b[1;32m    264\u001b[0m             y_true, y_pred, sample_weight\u001b[39m=\u001b[39msample_weight, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kwargs\n\u001b[1;32m    265\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:71\u001b[0m, in \u001b[0;36m_cached_call\u001b[0;34m(cache, estimator, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m\"\"\"Call estimator with method and args and kwargs.\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(estimator, method)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m cache[method]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/pipeline.py:457\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    455\u001b[0m Xt \u001b[39m=\u001b[39m X\n\u001b[1;32m    456\u001b[0m \u001b[39mfor\u001b[39;00m _, name, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(with_final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 457\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(Xt)\n\u001b[1;32m    458\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpredict(Xt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpredict_params)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:972\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, X, copy\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    958\u001b[0m     \u001b[39m\"\"\"Perform standardization by centering and scaling.\u001b[39;00m\n\u001b[1;32m    959\u001b[0m \n\u001b[1;32m    960\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[39m        Transformed array.\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 972\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    974\u001b[0m     copy \u001b[39m=\u001b[39m copy \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[1;32m    975\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m    976\u001b[0m         X,\n\u001b[1;32m    977\u001b[0m         reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    981\u001b[0m         force_all_finite\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    982\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/utils/validation.py:1340\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1338\u001b[0m     fitted \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39m__sklearn_is_fitted__()\n\u001b[1;32m   1339\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1340\u001b[0m     fitted \u001b[39m=\u001b[39m [\n\u001b[1;32m   1341\u001b[0m         v \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m \u001b[39mvars\u001b[39m(estimator) \u001b[39mif\u001b[39;00m v\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m v\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1342\u001b[0m     ]\n\u001b[1;32m   1344\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fitted:\n\u001b[1;32m   1345\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(msg \u001b[39m%\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(estimator)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m})\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/utils/validation.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1338\u001b[0m     fitted \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39m__sklearn_is_fitted__()\n\u001b[1;32m   1339\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1340\u001b[0m     fitted \u001b[39m=\u001b[39m [\n\u001b[0;32m-> 1341\u001b[0m         v \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m \u001b[39mvars\u001b[39m(estimator) \u001b[39mif\u001b[39;00m v\u001b[39m.\u001b[39;49mendswith(\u001b[39m\"\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m v\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1342\u001b[0m     ]\n\u001b[1;32m   1344\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fitted:\n\u001b[1;32m   1345\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(msg \u001b[39m%\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(estimator)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#run the analysis with a limited number of predictors\n",
    "\n",
    "overall_score = run_full_limited_predictor_analysis(10, outcome_measures, analysis_data_imputed)\n",
    "\n",
    "overall_scores = overall_scores.append({'n_features':10,'overall_score':overall_score},ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the analysis with a limited number of predictors\n",
    "predictors = 20\n",
    "\n",
    "overall_score = run_full_limited_predictor_analysis(predictors)\n",
    "\n",
    "overall_scores = overall_scores.append({'n_features':predictors,'overall_score':overall_score},ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the analysis with a limited number of predictors\n",
    "predictors = 40\n",
    "\n",
    "overall_score = run_full_limited_predictor_analysis(predictors)\n",
    "\n",
    "overall_scores = overall_scores.append({'n_features':predictors,'overall_score':overall_score},ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion so far\n",
    "\n",
    "The feature selection applied here hasn't helped very much. That surprises me because in test_limited_predictors, I got clear evidence that cutting down irrelevant predictors improved model performance.\n",
    "\n",
    "One reason might be that we've actually cut down on useful predictors--unlike in `test_limited_predictors.ipynb`, we can't cheat by removing predictors we know to be irrelevant. That means we're left with less information in the model itself.\n",
    "\n",
    "We've only really tried SelectKBest(); there might be other feature selection mechanisms that could do the job. But I don't know yet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataanalysis3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "014247d405695287815678bf9349a8dffb2674e9fe9a5bd4bb9820af018d638d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
