{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from socket import gethostname\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from dev_interaction_util import generate_synthetic_dev_outcomes, generate_synthetic_dev_data, set_up_interactions\n",
    "from dev_interaction_util import do_scoring_loop, get_best_model, summarize_overall_df_results, do_final_fit, present_model_results, present_results_vs_ground_truth_cors\n",
    "from dev_interaction_util import load_and_preprocess_data, impute_data, run_full_limited_predictor_analysis\n",
    "from ml_util import *\n",
    "# Imputing with MICE\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn import linear_model\n",
    "from ml_util import get_data_for_imputation\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.base import clone\n",
    "from sklearn.inspection import permutation_importance\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benjamins-MacBook-Pro-2.local\n",
      "{'dropbox_data_dir': '/Users/benjaminsmith/Dropbox (University of Oregon)/UO-SAN Lab/Berkman Lab/Devaluation/analysis_files/data/'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(gethostname())\n",
    "# Open the file and load the file\n",
    "with open('config.yml') as f:\n",
    "    all_yaml = yaml.load(f, Loader=SafeLoader)\n",
    "    if gethostname() in all_yaml.keys():\n",
    "        config = all_yaml[gethostname()]\n",
    "    else:\n",
    "        config = all_yaml['default']\n",
    "        \n",
    "print(config)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is derived from `test_feature_selection.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropbox_data_dir = config['dropbox_data_dir']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data, outcome_measures = load_and_preprocess_data(dropbox_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      33.0\n",
       "1       NaN\n",
       "2      39.8\n",
       "3      40.8\n",
       "4      47.6\n",
       "       ... \n",
       "270    29.9\n",
       "271    33.1\n",
       "272    27.4\n",
       "273    42.1\n",
       "274    33.3\n",
       "Name: bf_1, Length: 275, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_measures['bf_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -0.113462\n",
       "1     -0.682692\n",
       "2     -0.698077\n",
       "3      0.321154\n",
       "4     -0.603846\n",
       "         ...   \n",
       "270   -0.400000\n",
       "271   -0.619231\n",
       "272    0.157692\n",
       "273   -0.709615\n",
       "274   -1.011538\n",
       "Name: cancer_promoting_minus_preventing_FFQ, Length: 275, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_measures['cancer_promoting_minus_preventing_FFQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/impute/_iterative.py:713: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "analysis_data_imputed = impute_data(analysis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def do_hyperparameter_selection_loop_r2(X,y,cv):\n",
    "    return(do_hyperparameter_selection_loop_w_metric(X,y,cv,'r2'))\n",
    "\n",
    "#loops through the different estimators and feature selection methods and does a grid search over all to find the best hyperparameters\n",
    "def do_hyperparameter_selection_loop(X, y,cv):\n",
    "    return(do_hyperparameter_selection_loop_w_metric(X,y,cv,'neg_mean_absolute_error'))\n",
    "\n",
    "#loops through the different estimators and feature selection methods and does a grid search over all to find the best hyperparameters\n",
    "def do_hyperparameter_selection_loop_w_metric(X, y,cv,metric):\n",
    "    #alpha parameters for Ridge and Lasso\n",
    "    alpha_10pow_lower = 1\n",
    "    alpha_10pow_upper = 0\n",
    "    alpha_increments=1\n",
    "    alpha_range = np.concatenate([np.power(10,np.linspace(-alpha_10pow_lower,alpha_10pow_upper,(alpha_10pow_lower+alpha_10pow_upper)*alpha_increments+1)),\n",
    "        [0.2,0.3,0.4,0.6,0.8,1.0]])\n",
    "    \n",
    "    all_cv_results = []\n",
    "\n",
    "    pipeline_estimator_name = 'estimator'\n",
    "    feature_selection_name = 'feature_selection'\n",
    "\n",
    "\n",
    "    #define the param_grid for the estimators\n",
    "    estimators_to_run = {\n",
    "        'Ridge':{\n",
    "            'estimator':linear_model.Ridge,\n",
    "            'parameters':{'alpha':alpha_range}\n",
    "        },\n",
    "        'Lasso':{\n",
    "            'estimator':linear_model.Lasso,\n",
    "            'parameters':{'alpha':alpha_range}\n",
    "        },\n",
    "        'DecisionTreeRegressor':{\n",
    "            'estimator':DecisionTreeRegressor,\n",
    "            'parameters':{\n",
    "                'max_depth':[2, 4],\n",
    "                'min_samples_split':[20,50],\n",
    "                'min_samples_leaf':[20,50]\n",
    "            }\n",
    "        }             \n",
    "    }\n",
    "\n",
    "    k_max_val = np.min([50,X.shape[1]])\n",
    "\n",
    "    for estimator_name,estimator_dict in estimators_to_run.items():\n",
    "        #param grid for the feature seelction\n",
    "        #this is here because we need to know the estimator to pass to the feature selector\n",
    "        feature_selectors_to_run = {\n",
    "            'None':None,\n",
    "            'KBest':{\n",
    "                'selector':SelectKBest(),\n",
    "                'parameters':{\n",
    "                    'score_func' : [f_regression], \n",
    "                    'k' : [10,25,k_max_val]\n",
    "                    }\n",
    "            },\n",
    "            'RFE':{\n",
    "                'selector':RFE(linear_model.LinearRegression()),\n",
    "                'parameters':{\n",
    "                    'n_features_to_select' : [10,25],\n",
    "                    #'verbose':[1],\n",
    "                    'step':[5]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        for selector_name, selector_dict in feature_selectors_to_run.items():\n",
    "        #create the estimator\n",
    "            if selector_name == 'None':\n",
    "                pipeline = Pipeline([('scaler',StandardScaler()),\n",
    "                                     (pipeline_estimator_name,estimator_dict['estimator']())])\n",
    "                selector_params = {}\n",
    "            else:\n",
    "                pipeline = Pipeline([('scaler',StandardScaler()),\n",
    "                                     (feature_selection_name,selector_dict['selector']), \n",
    "                                     (pipeline_estimator_name,estimator_dict['estimator']())])\n",
    "                selector_params = selector_dict['parameters']\n",
    "\n",
    "            estimator_param_grid = {(pipeline_estimator_name + '__'+k):v for k,v in estimator_dict['parameters'].items()}\n",
    "            selector_param_grid = {(feature_selection_name + '__'+k):v for k,v in selector_params.items()}\n",
    "            #combine the two param grid dictionaries\n",
    "            full_param_grid = {**selector_param_grid, **estimator_param_grid}\n",
    "            print(pipeline)\n",
    "            print(full_param_grid)\n",
    "\n",
    "            \n",
    "        \n",
    "            gs_1 = GridSearchCV(estimator=pipeline, \n",
    "                                param_grid = full_param_grid, \n",
    "                                cv=cv,scoring=metric,verbose=1)\n",
    "            gs_1.fit(X,y)\n",
    "            all_cv_results.append(gs_1)\n",
    "\n",
    "    #create a dataframe with the best parameters, best mean_test_score, and name of the model\n",
    "\n",
    "    best_params_df = pd.DataFrame({\n",
    "        'model': [cv_result.estimator for cv_result in all_cv_results],\n",
    "        'model_name': [cv_result.estimator.__class__.__name__ for cv_result in all_cv_results],\n",
    "        'best_params': [extract_estimator_params_from_gridsearch(cv_result.best_params_) for cv_result in all_cv_results],\n",
    "        'best_score': [cv_result.best_score_ for cv_result in all_cv_results],\n",
    "        'best_raw_params' : [cv_result.best_params_ for cv_result in all_cv_results]\n",
    "        })\n",
    "    \n",
    "    best_params_df = best_params_df.sort_values('best_score',ascending=False).reset_index(drop=True)\n",
    "\n",
    "    best_model = clone(best_params_df['model'][0])\n",
    "    best_model_params = best_params_df['best_raw_params'][0]\n",
    "    best_model.set_params(**best_model_params)\n",
    "\n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_params_df':best_params_df,\n",
    "        'raw_cv_results':all_cv_results\n",
    "    }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving fit with manual theory-driven feature\n",
    "\n",
    "My past analysis showed that by manually removing some features before the analysis starts, we can improve performance beyond the chance performance otherwise seen.\n",
    "\n",
    "So, it might be useful to understand how much we can improve our performance by manual feature selection before the automatic feature selection applies.\n",
    "\n",
    "This was previously done in `test_limited_predictors.ipynb`. We tested as few as 2 distractor features. In that test, predictor features generally had correlations in the range of |r|=0.06 to 0.53, with most around 0.4 (we should confirm that because it seems fishy that PCS was detegted as an effect, but didn't model as a large predictor). With most `|r|=0.4`, this seems unrealistically high to expect, and we should aim to build a pipeline capable of detecting more subtle effects than that. An approximate `|r|=0.3` can be achieved by mixing in a predictor scaled to 8% of normal scale.\n",
    "\n",
    "I can imagine it is plausible to cut down to as few as two self-report, one behavioral, and one neural measure per intervention, plus sex and age. That would yield 10 different variables. At the other end, we might want 10 self-report, two behavioral, and five neural measures per intervention tested, plus 6 different demographic variables--a total of 40 variables. Let's see how these would perform, as well as mid-range of 20 predictor variables. In each case we'll restrict to three valid predictors per intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_2_group_predictor_analysis(total_predictor_count, outcome_measures, analysis_data_imputed, effect_size, hyperparameter_optimizer,\n",
    "                                        custom_interaction_effects=None\n",
    "                                        ):\n",
    "\n",
    "\n",
    "\n",
    "    ### Try out CV with simple gridsearch\n",
    "\n",
    "    scoring_data = do_scoring_loop(X=predictor_data_nona, y= outcome_measures_nona['d_bf'], \n",
    "                    groups = group_assignments_nona, \n",
    "                    hyperparameter_selection_on_fold=hyperparameter_optimizer,\n",
    "                    outer_folds=5)\n",
    "\n",
    "    scores = scoring_data['scores']\n",
    "    best_models = scoring_data['best_models']\n",
    "    best_params_df_list = scoring_data['best_params_df_list']\n",
    "    raw_cv_results_list = scoring_data['raw_cv_results_list']\n",
    "\n",
    "    print(\"scores:\")\n",
    "    print(scores)\n",
    "    overall_score = np.mean(scores)\n",
    "    print(\"overall_score:\")\n",
    "    print(overall_score)\n",
    "\n",
    "\n",
    "\n",
    "    best_model = get_best_model(summarize_overall_df_results(raw_cv_results_list))\n",
    "    final_fit = do_final_fit(X=predictor_data_nona, y= outcome_measures_nona['d_bf'], final_model=best_model)\n",
    "    final_results = present_model_results(X=predictor_data_nona, final_fit=final_fit, y=outcome_measures_nona['d_bf'])\n",
    "\n",
    "    #print rows of final_results where feature_name is the list of features to check\n",
    "    base_regressors = interaction_effect_df.predictor[interaction_effect_df.interaction_effect!=0]\n",
    "    regressors_to_check = [x+y for y in ['','*nisan'] for x in base_regressors]\n",
    "    final_results['planned_regression'] = final_results['predictor'].isin(regressors_to_check)\n",
    "\n",
    "    present_results_vs_ground_truth_cors(predictor_data_nona,outcome_measures_nona,group_assignments_nona,final_results,base_regressors)\n",
    "\n",
    "    return(overall_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def run_2_group_predictor_analysis(total_predictor_count, outcome_measures, analysis_data_imputed, effect_size, hyperparameter_optimizer,\n",
    "#                                         custom_interaction_effects=None\n",
    "#                                         ):\n",
    "\n",
    "#set np random seed\n",
    "np.random.seed(3161527)\n",
    "\n",
    "group_names = ['ichi','ni','san']\n",
    "#assign each row randomly to a group\n",
    "group_assignments_3 = np.random.choice(group_names,analysis_data_imputed.shape[0])\n",
    "\n",
    "#two-group analysis, comparing ichi vs ni and san\n",
    "group_assignments_2 = group_assignments_3.copy()\n",
    "group_assignments_2[group_assignments_2=='ni'] = 'nisan'\n",
    "group_assignments_2[group_assignments_2=='san'] = 'nisan'\n",
    "\n",
    "\n",
    "#synthetic outcomes\n",
    "outcome_measures = generate_synthetic_dev_outcomes(outcome_measures)\n",
    "\n",
    "#create a limited set of predictors\n",
    "# analysis_data_smol = analysis_data_imputed.iloc[:,0:total_predictor_count]\n",
    "\n",
    "# # add synthetic primary and interaction effects\n",
    "\n",
    "# if custom_interaction_effects is None:\n",
    "#     #set up the interaction effects\n",
    "#     #0.08 will give us correlations around 0.3 between the interaction effects and the outcome\n",
    "#     custom_interaction_effects_g1 = [0]*analysis_data_smol.shape[1]\n",
    "#     custom_interaction_effects_g1[0] = effect_size\n",
    "#     custom_interaction_effects_g1[1] = effect_size\n",
    "#     custom_interaction_effects_g1[2] = -effect_size\n",
    "\n",
    "\n",
    "#     custom_interaction_effects = {'nisan':custom_interaction_effects_g1}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# synthetic_data = generate_synthetic_dev_data(analysis_data_smol, group_assignments_2,outcome_measures, group_interaction_effects = custom_interaction_effects)\n",
    "# interaction_effect_df = synthetic_data['X_weights']\n",
    "outcome_measures = outcome_measures\n",
    "\n",
    "# Set up outcome measures and group assignment one-hot\n",
    "\n",
    "outcome_measures = calculate_outcome_changes(outcome_measures)\n",
    "\n",
    "\n",
    "# group_assignment_onehots = pd.get_dummies(group_assignments_2).loc[:,['nisan']]\n",
    "\n",
    "# predictor_data = set_up_interactions(analysis_data_smol, group_assignment_onehots)\n",
    "\n",
    "\n",
    "#remove any NA values for this outcome measure in both the predictor data and the outcome data\n",
    "outcome_nas = outcome_measures['d_bf'].isna()\n",
    "\n",
    "outcome_measures_nona = outcome_measures.loc[~outcome_nas,:]\n",
    "# predictor_data_nona = predictor_data.loc[~outcome_nas,:]\n",
    "# group_assignment_onehots_nonan = group_assignment_onehots.loc[~outcome_nas,:]\n",
    "# group_assignments_nona = group_assignments_2[~outcome_nas]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_measures['d_bf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_features</th>\n",
       "      <th>effect_size</th>\n",
       "      <th>overall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.078301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.121002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.004276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.054448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.101453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.090946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.045115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.033768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.035614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.119037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.054560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.116193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.085246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.014657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.039457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.011581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.104402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.045829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.008903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.092410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_features  effect_size  overall_score\n",
       "0         15.0         0.08      -0.078301\n",
       "1         15.0         0.10      -0.121002\n",
       "2         15.0         0.12       0.004276\n",
       "3         15.0         0.14       0.054448\n",
       "4         15.0         0.16       0.101453\n",
       "5         20.0         0.08      -0.090946\n",
       "6         20.0         0.10      -0.045115\n",
       "7         20.0         0.12      -0.033768\n",
       "8         20.0         0.14       0.035614\n",
       "9         20.0         0.16       0.119037\n",
       "10        25.0         0.08      -0.054560\n",
       "11        25.0         0.10      -0.116193\n",
       "12        25.0         0.12      -0.085246\n",
       "13        25.0         0.14      -0.014657\n",
       "14        25.0         0.16       0.039457\n",
       "15        30.0         0.08       0.011581\n",
       "16        30.0         0.10      -0.104402\n",
       "17        30.0         0.12      -0.045829\n",
       "18        30.0         0.14      -0.008903\n",
       "19        30.0         0.16       0.092410"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataanalysis3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "014247d405695287815678bf9349a8dffb2674e9fe9a5bd4bb9820af018d638d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
