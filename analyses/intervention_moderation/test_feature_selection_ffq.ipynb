{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from socket import gethostname\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from dev_interaction_util import generate_synthetic_dev_outcomes, generate_synthetic_dev_data, set_up_interactions\n",
    "from dev_interaction_util import do_scoring_loop, get_best_model, summarize_overall_df_results, do_final_fit, present_model_results, present_results_vs_ground_truth_cors\n",
    "from ml_util import *\n",
    "# Imputing with MICE\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn import linear_model\n",
    "from ml_util import get_data_for_imputation\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.base import clone\n",
    "from sklearn.inspection import permutation_importance\n",
    "#import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benjamins-MacBook-Pro-2.local\n",
      "{'dropbox_data_dir': '/Users/benjaminsmith/Dropbox (University of Oregon)/UO-SAN Lab/Berkman Lab/Devaluation/analysis_files/data/'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(gethostname())\n",
    "# Open the file and load the file\n",
    "with open('config.yml') as f:\n",
    "    all_yaml = yaml.load(f, Loader=SafeLoader)\n",
    "    if gethostname() in all_yaml.keys():\n",
    "        config = all_yaml[gethostname()]\n",
    "    else:\n",
    "        config = all_yaml['default']\n",
    "        \n",
    "print(config)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is derived from pre_registered_preview.ipynb.\n",
    "\n",
    "The aim is to look at how the model pipeline does with different sets of ground truths. If we plug in five actual effects, or ten, or twenty, how many are actually identified and how many irrelevant effects are identified?\n",
    "\n",
    "This can't be too black and white, because of course in real life, teh features are correlated iwth one another. But at least, the features we select to be correlated should _actually be_ the most correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropbox_data_dir = config['dropbox_data_dir']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is a pre-registered analysis for measuring moderations of the intervention.\n",
    "\n",
    "We'll cross-validate the intervention moderations.\n",
    "\n",
    "For this analysis, we'll try to make predictions based on some synthetic data. we'll take wave 1 data and randomly mix in changes based on our predictors, then try to model how we would predict those things. Finally, we'll make the predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_ppt_path = dropbox_data_dir + '/data_by_ppt.csv'\n",
    "data_codebook_path = dropbox_data_dir + 'data_codebook.csv'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_ppt = pd.read_csv(data_by_ppt_path)\n",
    "data_codebook = pd.read_csv(data_codebook_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out which columns in data_by_ppt are missing from the codebook\n",
    "data_by_ppt.columns.difference(data_codebook['VarName'])\n",
    "\n",
    "\n",
    "#copy our outcome measures, bf_1 and FFQ_1, into a new dataframe\n",
    "data_by_ppt['bf_2'] = data_by_ppt.bf_1\n",
    "#need to decide what sort of FFQ we want to use\n",
    "data_by_ppt['cancer_promoting_minus_preventing_FFQ_1'] = data_by_ppt.cancer_promoting_minus_preventing_FFQ\n",
    "data_by_ppt['cancer_promoting_minus_preventing_FFQ_2'] = data_by_ppt.cancer_promoting_minus_preventing_FFQ\n",
    "\n",
    "# do a report on missing data\n",
    "analysis_data  = data_by_ppt.loc[:,data_codebook.loc[data_codebook.IsSelectedPredictor,\"VarName\"]].copy()\n",
    "outcome_measures = data_by_ppt.loc[:,data_codebook.loc[data_codebook.IsSelectedOutcomeMeasure,\"VarName\"]].copy()\n",
    "\n",
    "na_values = pd.DataFrame(data_by_ppt.isna().sum())\n",
    "na_values.columns = ['NA_Count']\n",
    "na_values['prop_NA'] = na_values.NA_Count / data_by_ppt.shape[0]\n",
    "data_codebook = data_codebook.merge(na_values, left_on='VarName', right_index=True)\n",
    "\n",
    "data_codebook.to_csv(dropbox_data_dir + 'data_metadata.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to count the number of valid and missing entries in each of our data predictors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting data to numeric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_vals = pd.get_dummies(analysis_data.birthsex_factor)\n",
    "#there's only two variables here so we can convert this into a dummy variable\n",
    "analysis_data.drop(columns=['birthsex_factor'], inplace=True)\n",
    "one_hot_vals.columns = ['birthsex_factor_' + str(col) for col in one_hot_vals.columns]\n",
    "analysis_data = analysis_data.join(one_hot_vals.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BSCS</th>\n",
       "      <th>EDM</th>\n",
       "      <th>BIS_11</th>\n",
       "      <th>PCS</th>\n",
       "      <th>RS</th>\n",
       "      <th>TRSQ</th>\n",
       "      <th>ACES_neglectful_parenting</th>\n",
       "      <th>ACES_abuse</th>\n",
       "      <th>ACES_sum</th>\n",
       "      <th>ACES_divorced_separated</th>\n",
       "      <th>...</th>\n",
       "      <th>zipcode_median_income_acs</th>\n",
       "      <th>household_income_per_person</th>\n",
       "      <th>SST_prop_successful_stops</th>\n",
       "      <th>SST_GRTmean</th>\n",
       "      <th>SST_SSD</th>\n",
       "      <th>SST_PostErrorSlowW1_mean</th>\n",
       "      <th>SST_mean_ssrt_0</th>\n",
       "      <th>ROC_Crave_Regulate_Minus_Look</th>\n",
       "      <th>WTP_unhealthy_minus_healthy</th>\n",
       "      <th>birthsex_factor_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.538462</td>\n",
       "      <td>3.250</td>\n",
       "      <td>72</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.5125</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.384615</td>\n",
       "      <td>1.750</td>\n",
       "      <td>89</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.440524</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.384615</td>\n",
       "      <td>2.500</td>\n",
       "      <td>63</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>533.315052</td>\n",
       "      <td>284.375</td>\n",
       "      <td>0.058297</td>\n",
       "      <td>0.247061</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>-0.190476</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.076923</td>\n",
       "      <td>2.800</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>498.167248</td>\n",
       "      <td>103.125</td>\n",
       "      <td>0.027730</td>\n",
       "      <td>0.446583</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>0.170363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.307692</td>\n",
       "      <td>2.750</td>\n",
       "      <td>64</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>626.507764</td>\n",
       "      <td>250.000</td>\n",
       "      <td>0.105660</td>\n",
       "      <td>0.369308</td>\n",
       "      <td>-1.5500</td>\n",
       "      <td>-0.494624</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>3.461538</td>\n",
       "      <td>4.000</td>\n",
       "      <td>58</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.690347</td>\n",
       "      <td>1.768485</td>\n",
       "      <td>0.523438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.357362</td>\n",
       "      <td>-0.0125</td>\n",
       "      <td>-1.008152</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>3.692308</td>\n",
       "      <td>3.875</td>\n",
       "      <td>54</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.511475</td>\n",
       "      <td>-0.234851</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.335849</td>\n",
       "      <td>-0.1500</td>\n",
       "      <td>-1.889247</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>3.461538</td>\n",
       "      <td>3.125</td>\n",
       "      <td>69</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.335248</td>\n",
       "      <td>0.099038</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.273736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>2.846154</td>\n",
       "      <td>3.000</td>\n",
       "      <td>62</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.855379</td>\n",
       "      <td>-0.234851</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.401098</td>\n",
       "      <td>-0.9875</td>\n",
       "      <td>-0.151210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>3.230769</td>\n",
       "      <td>2.500</td>\n",
       "      <td>52</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.834004</td>\n",
       "      <td>1.768485</td>\n",
       "      <td>0.476562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.481932</td>\n",
       "      <td>-0.5500</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275 rows Ã— 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         BSCS    EDM  BIS_11   PCS    RS  TRSQ  ACES_neglectful_parenting  \\\n",
       "0    2.538462  3.250      72   7.0  20.0  63.0                        NaN   \n",
       "1    2.384615  1.750      89   9.0  22.0  63.0                        NaN   \n",
       "2    3.384615  2.500      63   9.0  18.0  57.0                        NaN   \n",
       "3    3.076923  2.800      75   NaN   NaN  64.0                        NaN   \n",
       "4    3.307692  2.750      64  12.0  21.0  55.0                        NaN   \n",
       "..        ...    ...     ...   ...   ...   ...                        ...   \n",
       "270  3.461538  4.000      58  18.0  17.0  54.0                        0.0   \n",
       "271  3.692308  3.875      54  17.0  13.0  55.0                        2.0   \n",
       "272  3.461538  3.125      69  11.0  13.0  53.0                        1.0   \n",
       "273  2.846154  3.000      62  15.0  22.0  84.0                        0.0   \n",
       "274  3.230769  2.500      52   9.0  15.0  59.0                        0.0   \n",
       "\n",
       "     ACES_abuse  ACES_sum  ACES_divorced_separated  ...  \\\n",
       "0           NaN       NaN                      NaN  ...   \n",
       "1           NaN       NaN                      NaN  ...   \n",
       "2           NaN       NaN                      NaN  ...   \n",
       "3           NaN       NaN                      NaN  ...   \n",
       "4           NaN       NaN                      NaN  ...   \n",
       "..          ...       ...                      ...  ...   \n",
       "270         1.0       3.0                      1.0  ...   \n",
       "271         2.0       5.0                      0.0  ...   \n",
       "272         1.0       6.0                      1.0  ...   \n",
       "273         1.0       4.0                      1.0  ...   \n",
       "274         0.0       3.0                      1.0  ...   \n",
       "\n",
       "     zipcode_median_income_acs  household_income_per_person  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          NaN                          NaN   \n",
       "4                          NaN                          NaN   \n",
       "..                         ...                          ...   \n",
       "270                  -0.690347                     1.768485   \n",
       "271                  -0.511475                    -0.234851   \n",
       "272                   1.335248                     0.099038   \n",
       "273                   0.855379                    -0.234851   \n",
       "274                   0.834004                     1.768485   \n",
       "\n",
       "     SST_prop_successful_stops  SST_GRTmean  SST_SSD  \\\n",
       "0                          NaN          NaN      NaN   \n",
       "1                          NaN          NaN      NaN   \n",
       "2                     0.500000   533.315052  284.375   \n",
       "3                     0.312500   498.167248  103.125   \n",
       "4                     0.562500   626.507764  250.000   \n",
       "..                         ...          ...      ...   \n",
       "270                   0.523438          NaN      NaN   \n",
       "271                   0.492188          NaN      NaN   \n",
       "272                   0.507812          NaN      NaN   \n",
       "273                   0.479167          NaN      NaN   \n",
       "274                   0.476562          NaN      NaN   \n",
       "\n",
       "     SST_PostErrorSlowW1_mean  SST_mean_ssrt_0  ROC_Crave_Regulate_Minus_Look  \\\n",
       "0                         NaN              NaN                        -0.5125   \n",
       "1                         NaN              NaN                            NaN   \n",
       "2                    0.058297         0.247061                        -0.8000   \n",
       "3                    0.027730         0.446583                        -0.8000   \n",
       "4                    0.105660         0.369308                        -1.5500   \n",
       "..                        ...              ...                            ...   \n",
       "270                       NaN         0.357362                        -0.0125   \n",
       "271                       NaN         0.335849                        -0.1500   \n",
       "272                       NaN         0.273736                            NaN   \n",
       "273                       NaN         0.401098                        -0.9875   \n",
       "274                       NaN         0.481932                        -0.5500   \n",
       "\n",
       "     WTP_unhealthy_minus_healthy  birthsex_factor_Male  \n",
       "0                      -0.312500                     1  \n",
       "1                       0.440524                     0  \n",
       "2                      -0.190476                     0  \n",
       "3                       0.170363                     0  \n",
       "4                      -0.494624                     0  \n",
       "..                           ...                   ...  \n",
       "270                    -1.008152                     1  \n",
       "271                    -1.889247                     1  \n",
       "272                     0.516129                     1  \n",
       "273                    -0.151210                     0  \n",
       "274                     0.343750                     1  \n",
       "\n",
       "[275 rows x 76 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data \n",
    "\n",
    "Apply missing data imputation to columns including cSES, ACES_sum, ses_aggregate, zipcode_median_income_acs, IMI, mcarthur social standing, based on demographic and self-report predictors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this experiment, I'm going for Ridge regression with 10 nearest features. The values it imputes are a compromise between simply using the nearest mean, which is conservative when using these values for prediction because it doesn't introduce erroneous variance, but isn't very informative, and then using all available information, which Ridge regression with an unlimited number of features would do. It's a tough choice between this and KNN, which doesn't assume normality. Overall I'm going with KNN, because it picks up on relationships between the two variables while not generating extreme values like KNN seems to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis/lib/python3.11/site-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "imputer = IterativeImputer(estimator=linear_model.Ridge(),n_nearest_features=10,max_iter=100,random_state=0)\n",
    "analysis_data_imputed = get_data_for_imputation(analysis_data)\n",
    "\n",
    "#this dataset is already filtered for columns so we don't need to filter those further.\n",
    "analysis_data_imputed = pd.DataFrame(imputer.fit_transform(analysis_data_imputed), columns=analysis_data_imputed.columns)\n",
    "imputed_datapoint = analysis_data.isna()\n",
    "# do_aces_cses_imputation_diagnostic(analysis_data_imputed, imputed_datapoint,'ridge_10')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With feature selection, KBest vs. none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loops through the different estimators and feature selection methods and does a grid search over all to find the best hyperparameters\n",
    "def do_hyperparameter_selection_loop(X, y,cv):\n",
    "    #alpha parameters for Ridge and Lasso\n",
    "    alpha_10pow_lower = 1\n",
    "    alpha_10pow_upper = 0\n",
    "    alpha_increments=1\n",
    "    alpha_range = np.concatenate([np.power(10,np.linspace(-alpha_10pow_lower,alpha_10pow_upper,(alpha_10pow_lower+alpha_10pow_upper)*alpha_increments+1)),\n",
    "        [0.2,0.4,0.6,0.8,1.0]])\n",
    "    \n",
    "    all_cv_results = []\n",
    "\n",
    "    pipeline_estimator_name = 'estimator'\n",
    "    feature_selection_name = 'feature_selection'\n",
    "\n",
    "\n",
    "    #define the param_grid for the estimators\n",
    "    estimators_to_run = {\n",
    "        'Ridge':{\n",
    "            'estimator':linear_model.Ridge,\n",
    "            'parameters':{'alpha':alpha_range}\n",
    "        },\n",
    "        'Lasso':{\n",
    "            'estimator':linear_model.Lasso,\n",
    "            'parameters':{'alpha':alpha_range}\n",
    "        },\n",
    "        # 'DecisionTreeRegressor':{\n",
    "        #     'estimator':DecisionTreeRegressor,\n",
    "        #     'parameters':{\n",
    "        #         'max_depth':[2, 3,5,10],\n",
    "        #         'min_samples_split':[5,20,50],\n",
    "        #         'min_samples_leaf':[5,20,50]\n",
    "        #     }\n",
    "        # }             \n",
    "    }\n",
    "\n",
    "    for estimator_name,estimator_dict in estimators_to_run.items():\n",
    "        #param grid for the feature seelction\n",
    "        #this is here because we need to know the estimator to pass to the feature selector\n",
    "        feature_selectors_to_run = {\n",
    "            'None':None,\n",
    "            'KBest':{\n",
    "                'selector':SelectKBest(),\n",
    "                'parameters':{\n",
    "                    'score_func' : [f_regression], \n",
    "                    'k' : [20,50]\n",
    "                    }\n",
    "            }#,\n",
    "            # 'RFE':{\n",
    "            #     'selector':RFE(linear_model.LinearRegression()),\n",
    "            #     'parameters':{\n",
    "            #         'n_features_to_select' : [10,25],\n",
    "            #         #'verbose':[1],\n",
    "            #         'step':[5]\n",
    "            #     }\n",
    "            # }\n",
    "        }\n",
    "        for selector_name, selector_dict in feature_selectors_to_run.items():\n",
    "        #create the estimator\n",
    "            if selector_name == 'None':\n",
    "                pipeline = Pipeline([('scaler',StandardScaler()),\n",
    "                                     (pipeline_estimator_name,estimator_dict['estimator']())])\n",
    "                selector_params = {}\n",
    "            else:\n",
    "                pipeline = Pipeline([('scaler',StandardScaler()),\n",
    "                                     (feature_selection_name,selector_dict['selector']), \n",
    "                                     (pipeline_estimator_name,estimator_dict['estimator']())])\n",
    "                selector_params = selector_dict['parameters']\n",
    "\n",
    "            estimator_param_grid = {(pipeline_estimator_name + '__'+k):v for k,v in estimator_dict['parameters'].items()}\n",
    "            selector_param_grid = {(feature_selection_name + '__'+k):v for k,v in selector_params.items()}\n",
    "            #combine the two param grid dictionaries\n",
    "            full_param_grid = {**selector_param_grid, **estimator_param_grid}\n",
    "            print(pipeline)\n",
    "            print(full_param_grid)\n",
    "\n",
    "            \n",
    "        \n",
    "            gs_1 = GridSearchCV(estimator=pipeline, \n",
    "                                param_grid = full_param_grid, \n",
    "                                cv=cv,scoring='neg_mean_absolute_error',verbose=1)\n",
    "            gs_1.fit(X,y)\n",
    "            all_cv_results.append(gs_1)\n",
    "\n",
    "    #create a dataframe with the best parameters, best mean_test_score, and name of the model\n",
    "\n",
    "    best_params_df = pd.DataFrame({\n",
    "        'model': [cv_result.estimator for cv_result in all_cv_results],\n",
    "        'model_name': [cv_result.estimator.__class__.__name__ for cv_result in all_cv_results],\n",
    "        'best_params': [extract_estimator_params_from_gridsearch(cv_result.best_params_) for cv_result in all_cv_results],\n",
    "        'best_score': [cv_result.best_score_ for cv_result in all_cv_results],\n",
    "        'best_raw_params' : [cv_result.best_params_ for cv_result in all_cv_results]\n",
    "        })\n",
    "    \n",
    "    best_params_df = best_params_df.sort_values('best_score',ascending=False).reset_index(drop=True)\n",
    "\n",
    "    best_model = clone(best_params_df['model'][0])\n",
    "    best_model_params = best_params_df['best_raw_params'][0]\n",
    "    best_model.set_params(**best_model_params)\n",
    "\n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_params_df':best_params_df,\n",
    "        'raw_cv_results':all_cv_results\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def present_model_results(X,y, final_fit):\n",
    "    final_estimator = final_fit.named_steps['estimator']\n",
    "    \n",
    "    if hasattr(final_estimator,'coef_'):\n",
    "        coef = final_estimator.coef_\n",
    "    else:\n",
    "        coef = None\n",
    "\n",
    "    #now check to see if there was a feature selection step,\n",
    "    #if so, get the feature names from the feature selection step\n",
    "    if 'feature_selection' in final_fit.named_steps:\n",
    "        feature_bool = final_fit.named_steps['feature_selection'].get_support(indices=True)\n",
    "    else:\n",
    "        feature_bool = [True]*len(X.columns)\n",
    "    \n",
    "    feature_names = X.columns[feature_bool]\n",
    "\n",
    "    #now do a permutation test to do feature importance\n",
    "    #view the coefficients\n",
    "    print(\"doing permutation test on importance; this may take time.\")\n",
    "    permutation_res= [im for im in permutation_importance(final_fit, X, y, n_repeats=10).importances_mean]\n",
    "    # print(len(feature_names))\n",
    "    # print(len(permutation_res))\n",
    "    # print(len(coef))\n",
    "    \n",
    "    \n",
    "    final_results = pd.DataFrame({\n",
    "        'predictor': feature_names,\n",
    "        'coef': coef,\n",
    "        'feature_importance':pd.Series(permutation_res)[feature_bool]\n",
    "        #'std_err': np.sqrt(np.diag(model_fit.coef_cov_)),\n",
    "        #'pval': 2*(1-stats.t.cdf(np.abs(model_fit.coef_/np.sqrt(np.diag(model_fit.coef_cov_))),df=predictor_data_nona.shape[0]-predictor_data_nona.shape[1]))\n",
    "    })\n",
    "\n",
    "    final_results['fa_abs'] = np.abs(final_results.feature_importance)\n",
    "    final_results = final_results.sort_values('fa_abs',ascending=False)\n",
    "\n",
    "    if coef is not None:\n",
    "        selected_features_count = np.sum(final_estimator.coef_!=0)\n",
    "        print(f\"Number of selected features: {selected_features_count}\")\n",
    "\n",
    "    display(HTML(final_results[0:20].to_html()))\n",
    "    return(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ni' 'san']\n",
      "[1.28335298 0.42953651]\n",
      "['san' 'san' 'ni' 'ichi' 'san' 'san' 'ichi' 'san' 'san' 'san' 'ni' 'ichi'\n",
      " 'ichi' 'ichi' 'ichi' 'san' 'san' 'san' 'ichi' 'ichi' 'san' 'san' 'ni'\n",
      " 'ni' 'ni' 'ni' 'ni' 'ni' 'ni' 'san' 'ni' 'san' 'ni' 'ichi' 'ni' 'san'\n",
      " 'ni' 'ichi' 'san' 'ni' 'ni' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ni' 'ni'\n",
      " 'san' 'ichi' 'ichi' 'ni' 'san' 'ni' 'ichi' 'ni' 'ni' 'ni' 'ichi' 'san'\n",
      " 'ni' 'ni' 'ichi' 'ni' 'ichi' 'san' 'ni' 'ni' 'ni' 'san' 'ichi' 'ni' 'san'\n",
      " 'ichi' 'san' 'ni' 'san' 'ni' 'ichi' 'ichi' 'san' 'ichi' 'san' 'san' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'san' 'ni' 'san' 'ni' 'ichi' 'san' 'san' 'san' 'ichi'\n",
      " 'ni' 'san' 'ichi' 'ichi' 'san' 'ni' 'ichi' 'san' 'ni' 'ni' 'san' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'ichi' 'ni' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ichi'\n",
      " 'ni' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ichi' 'san' 'san' 'ni' 'ichi' 'ni'\n",
      " 'ichi' 'ichi' 'san' 'ichi' 'ni' 'san' 'san' 'ni' 'ni' 'san' 'san' 'san'\n",
      " 'ichi' 'san' 'ni' 'san' 'ichi' 'ichi' 'ichi' 'ni' 'san' 'ni' 'ni' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'san' 'ni' 'san' 'ichi' 'ni' 'ni' 'ni' 'ichi' 'ni'\n",
      " 'ichi' 'san' 'san' 'san' 'san' 'ichi' 'ni' 'san' 'san' 'san' 'ichi' 'san'\n",
      " 'ni' 'ni' 'san' 'ichi' 'ichi' 'san' 'ni' 'ni' 'san' 'ni' 'san' 'san' 'ni'\n",
      " 'san' 'ni' 'ni' 'san' 'ichi' 'san' 'ichi' 'san' 'ni' 'ni' 'ni' 'ichi'\n",
      " 'ni' 'ni' 'san' 'ni' 'ni' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ni' 'ni' 'san'\n",
      " 'ichi' 'ni' 'ichi' 'ichi' 'ichi' 'ichi' 'ichi' 'ichi' 'san' 'ichi' 'san'\n",
      " 'ichi' 'ni' 'san' 'san' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ni' 'san' 'san'\n",
      " 'ichi' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ni' 'ni' 'ni' 'san' 'ichi'\n",
      " 'ichi' 'ichi' 'ni' 'ichi' 'ni' 'san' 'ichi' 'san' 'san' 'ni' 'san' 'san'\n",
      " 'san' 'san' 'ichi' 'ichi' 'ichi' 'ni' 'ni' 'ichi' 'san' 'san' 'san']\n",
      "['ichi' 'ni' 'san']\n",
      "ichi\n",
      "no interaction effects for group: ichi. No effects will be included for this group.\n",
      "ni\n",
      "                   feature_name  interaction_effect\n",
      "0                          BSCS                0.15\n",
      "3                           PCS               -0.15\n",
      "1                           EDM                0.15\n",
      "2                        BIS_11               -0.15\n",
      "74  WTP_unhealthy_minus_healthy                0.00\n",
      "bf_2\n",
      "cancer_promoting_minus_preventing_FFQ_w2\n",
      "FFQ_v2_Mean_Energy_w2\n",
      "san\n",
      "                feature_name  interaction_effect\n",
      "4                         RS                0.15\n",
      "5                       TRSQ                0.15\n",
      "6  ACES_neglectful_parenting               -0.15\n",
      "7                 ACES_abuse               -0.15\n",
      "0                       BSCS                0.00\n",
      "bf_2\n",
      "cancer_promoting_minus_preventing_FFQ_w2\n",
      "FFQ_v2_Mean_Energy_w2\n",
      "(275, 76)\n",
      "(275, 76)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#set np random seed\n",
    "np.random.seed(3161527)\n",
    "\n",
    "group_names = ['ichi','ni','san']\n",
    "#assign each row randomly to a group\n",
    "group_assignments = np.random.choice(group_names,analysis_data_imputed.shape[0])\n",
    "\n",
    "#synthetic outcomes\n",
    "outcome_measures = generate_synthetic_dev_outcomes(outcome_measures)\n",
    "\n",
    "# add synthetic primary and interaction effects\n",
    "\n",
    "\n",
    "#set up the interaction effects\n",
    "custom_interaction_effects_g1 = [0]*analysis_data_imputed.shape[1]\n",
    "custom_interaction_effects_g1[0] = 0.15\n",
    "custom_interaction_effects_g1[1] = 0.15\n",
    "custom_interaction_effects_g1[2] = -0.15\n",
    "custom_interaction_effects_g1[3] = -0.15\n",
    "\n",
    "custom_interaction_effects_g2 = [0]*analysis_data_imputed.shape[1]\n",
    "custom_interaction_effects_g2[4] = 0.15\n",
    "custom_interaction_effects_g2[5] = 0.15\n",
    "custom_interaction_effects_g2[6] = -0.15\n",
    "custom_interaction_effects_g2[7] = -0.15\n",
    "\n",
    "custom_interaction_effects = {'ni':custom_interaction_effects_g1,'san':custom_interaction_effects_g2}\n",
    "\n",
    "\n",
    "\n",
    "synthetic_data = generate_synthetic_dev_data(analysis_data_imputed, group_assignments,outcome_measures, group_interaction_effects = custom_interaction_effects)\n",
    "interaction_effect_df = synthetic_data['X_weights']\n",
    "outcome_measures = synthetic_data['y']\n",
    "\n",
    "# Set up outcome measures and group assignment one-hot\n",
    "\n",
    "outcome_measures = calculate_outcome_changes(outcome_measures)\n",
    "group_assignment_onehots = pd.get_dummies(group_assignments).loc[:,['ni','san']]\n",
    "\n",
    "predictor_data = set_up_interactions(analysis_data_imputed, group_assignment_onehots)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer split0\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "outer split1\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "outer split2\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "outer split3\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "outer split4\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "scores:\n",
      "[0.6143505740796231, 0.7714256549559589, 0.6310741367326764, 0.5753540448131884, 0.6213734391441958]\n",
      "overall_score:\n",
      "0.6427155699451286\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean_test_score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">std_test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_description</th>\n",
       "      <th>params_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.1}</th>\n",
       "      <td>-0.289938</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>0.021273</td>\n",
       "      <td>0.012721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.290794</td>\n",
       "      <td>0.005968</td>\n",
       "      <td>0.022867</td>\n",
       "      <td>0.004430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.291894</td>\n",
       "      <td>0.006714</td>\n",
       "      <td>0.023126</td>\n",
       "      <td>0.004239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.293261</td>\n",
       "      <td>0.007252</td>\n",
       "      <td>0.023502</td>\n",
       "      <td>0.003641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.294541</td>\n",
       "      <td>0.003920</td>\n",
       "      <td>0.021524</td>\n",
       "      <td>0.009170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.294999</td>\n",
       "      <td>0.008027</td>\n",
       "      <td>0.024075</td>\n",
       "      <td>0.002824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.295234</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>0.020316</td>\n",
       "      <td>0.011688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.295299</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.011895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.297494</td>\n",
       "      <td>0.009146</td>\n",
       "      <td>0.024906</td>\n",
       "      <td>0.001832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.297517</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.021779</td>\n",
       "      <td>0.010156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.299374</td>\n",
       "      <td>0.009869</td>\n",
       "      <td>0.025489</td>\n",
       "      <td>0.001573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.301676</td>\n",
       "      <td>0.004708</td>\n",
       "      <td>0.022387</td>\n",
       "      <td>0.010546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.307907</td>\n",
       "      <td>0.004984</td>\n",
       "      <td>0.023463</td>\n",
       "      <td>0.010707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.319563</td>\n",
       "      <td>0.006303</td>\n",
       "      <td>0.026823</td>\n",
       "      <td>0.011431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.330986</td>\n",
       "      <td>0.007142</td>\n",
       "      <td>0.030889</td>\n",
       "      <td>0.012083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.332344</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>0.022035</td>\n",
       "      <td>0.013580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.2}</th>\n",
       "      <td>-0.332344</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>0.022035</td>\n",
       "      <td>0.013580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.332367</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>0.022066</td>\n",
       "      <td>0.013625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">dict_values([StandardScaler(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 1.0}</th>\n",
       "      <td>-0.338501</td>\n",
       "      <td>0.029098</td>\n",
       "      <td>0.029414</td>\n",
       "      <td>0.010589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8}</th>\n",
       "      <td>-0.347916</td>\n",
       "      <td>0.033965</td>\n",
       "      <td>0.030728</td>\n",
       "      <td>0.011530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6}</th>\n",
       "      <td>-0.360835</td>\n",
       "      <td>0.037915</td>\n",
       "      <td>0.032118</td>\n",
       "      <td>0.012033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4}</th>\n",
       "      <td>-0.379385</td>\n",
       "      <td>0.042756</td>\n",
       "      <td>0.033621</td>\n",
       "      <td>0.013626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.2}</th>\n",
       "      <td>-0.411027</td>\n",
       "      <td>0.047215</td>\n",
       "      <td>0.035238</td>\n",
       "      <td>0.017128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1}</th>\n",
       "      <td>-0.439350</td>\n",
       "      <td>0.048514</td>\n",
       "      <td>0.036170</td>\n",
       "      <td>0.019874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.451078</td>\n",
       "      <td>0.005362</td>\n",
       "      <td>0.027566</td>\n",
       "      <td>0.012011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.4}</th>\n",
       "      <td>-0.451078</td>\n",
       "      <td>0.005362</td>\n",
       "      <td>0.027566</td>\n",
       "      <td>0.012011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.451078</td>\n",
       "      <td>0.005362</td>\n",
       "      <td>0.027566</td>\n",
       "      <td>0.012011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.6}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.010321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.010321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 1.0}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.009731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.010321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.009731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.009731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.010321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.010321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.010321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing permutation test on importance; this may take time.\n",
      "Number of selected features: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictor</th>\n",
       "      <th>coef</th>\n",
       "      <th>feature_importance</th>\n",
       "      <th>fa_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>BSCS*ni</td>\n",
       "      <td>0.360661</td>\n",
       "      <td>0.857528</td>\n",
       "      <td>0.857528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>BFI_conscientiousness*ni</td>\n",
       "      <td>0.026170</td>\n",
       "      <td>0.016946</td>\n",
       "      <td>0.016946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>IMI_perceived_choice*ni</td>\n",
       "      <td>0.017550</td>\n",
       "      <td>0.010778</td>\n",
       "      <td>0.010778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>RTFS_factor_2*ni</td>\n",
       "      <td>0.012885</td>\n",
       "      <td>0.007558</td>\n",
       "      <td>0.007558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>TRSQ*san</td>\n",
       "      <td>0.009197</td>\n",
       "      <td>0.005296</td>\n",
       "      <td>0.005296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BSCS</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>PCS*san</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>SST_SSD*ni</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>SST_PostErrorSlowW1_mean*ni</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>SST_mean_ssrt_0*ni</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>ROC_Crave_Regulate_Minus_Look*ni</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>WTP_unhealthy_minus_healthy*ni</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>birthsex_factor_Male*ni</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>BSCS*san</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>EDM*san</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>BIS_11*san</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>RS*san</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>SST_prop_successful_stops*ni</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>ACES_neglectful_parenting*san</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>ACES_abuse*san</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/Google Drive/oregon/code/DEV_scripts/analyses/intervention_moderation/dev_interaction_util.py:585: FutureWarning: merging between different levels is deprecated and will be removed in a future version. (2 levels on the left, 1 on the right)\n",
      "  results_vs_cors = final_results_wide.merge(group_correlations, left_index=True, right_index=True, how='outer')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(coef, base)</th>\n",
       "      <th>(coef, ni)</th>\n",
       "      <th>(coef, san)</th>\n",
       "      <th>(feature_importance, base)</th>\n",
       "      <th>(feature_importance, ni)</th>\n",
       "      <th>(feature_importance, san)</th>\n",
       "      <th>ichi_cor</th>\n",
       "      <th>ni_cor</th>\n",
       "      <th>san_cor</th>\n",
       "      <th>abs_effect_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BSCS</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BFI_conscientiousness</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMI_perceived_choice</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RTFS_factor_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRSQ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'results_vs_cors':                                (coef, base)  (coef, ni)  (coef, san)  \\\n",
       " BSCS                                   -0.0    0.360661     0.000000   \n",
       " BFI_conscientiousness                   0.0    0.026170     0.000000   \n",
       " IMI_perceived_choice                   -0.0    0.017550     0.000000   \n",
       " RTFS_factor_2                           0.0    0.012885     0.000000   \n",
       " TRSQ                                    0.0    0.000000     0.009197   \n",
       " ...                                     ...         ...          ...   \n",
       " NCS_prefer_little_thought               0.0    0.000000     0.000000   \n",
       " NCS_prefer_complex                      0.0    0.000000     0.000000   \n",
       " NCS_new_solutions_to_problems           0.0    0.000000     0.000000   \n",
       " NCS_like_responsibility                 0.0    0.000000     0.000000   \n",
       " zipcode_median_income_acs               0.0    0.000000     0.000000   \n",
       " \n",
       "                                (feature_importance, base)  \\\n",
       " BSCS                                                  0.0   \n",
       " BFI_conscientiousness                                 0.0   \n",
       " IMI_perceived_choice                                  0.0   \n",
       " RTFS_factor_2                                         0.0   \n",
       " TRSQ                                                  0.0   \n",
       " ...                                                   ...   \n",
       " NCS_prefer_little_thought                             0.0   \n",
       " NCS_prefer_complex                                    0.0   \n",
       " NCS_new_solutions_to_problems                         0.0   \n",
       " NCS_like_responsibility                               0.0   \n",
       " zipcode_median_income_acs                             0.0   \n",
       " \n",
       "                                (feature_importance, ni)  \\\n",
       " BSCS                                           0.857528   \n",
       " BFI_conscientiousness                          0.016946   \n",
       " IMI_perceived_choice                           0.010778   \n",
       " RTFS_factor_2                                  0.007558   \n",
       " TRSQ                                           0.000000   \n",
       " ...                                                 ...   \n",
       " NCS_prefer_little_thought                      0.000000   \n",
       " NCS_prefer_complex                             0.000000   \n",
       " NCS_new_solutions_to_problems                  0.000000   \n",
       " NCS_like_responsibility                        0.000000   \n",
       " zipcode_median_income_acs                      0.000000   \n",
       " \n",
       "                                (feature_importance, san)  ichi_cor    ni_cor  \\\n",
       " BSCS                                            0.000000 -0.137476  0.448089   \n",
       " BFI_conscientiousness                           0.000000       NaN       NaN   \n",
       " IMI_perceived_choice                            0.000000       NaN       NaN   \n",
       " RTFS_factor_2                                   0.000000       NaN       NaN   \n",
       " TRSQ                                            0.005296  0.090684 -0.239024   \n",
       " ...                                                  ...       ...       ...   \n",
       " NCS_prefer_little_thought                       0.000000       NaN       NaN   \n",
       " NCS_prefer_complex                              0.000000       NaN       NaN   \n",
       " NCS_new_solutions_to_problems                   0.000000       NaN       NaN   \n",
       " NCS_like_responsibility                         0.000000       NaN       NaN   \n",
       " zipcode_median_income_acs                       0.000000       NaN       NaN   \n",
       " \n",
       "                                 san_cor  abs_effect_sum  \n",
       " BSCS                          -0.079543        0.857528  \n",
       " BFI_conscientiousness               NaN        0.016946  \n",
       " IMI_perceived_choice                NaN        0.010778  \n",
       " RTFS_factor_2                       NaN        0.007558  \n",
       " TRSQ                           0.427083        0.005296  \n",
       " ...                                 ...             ...  \n",
       " NCS_prefer_little_thought           NaN        0.000000  \n",
       " NCS_prefer_complex                  NaN        0.000000  \n",
       " NCS_new_solutions_to_problems       NaN        0.000000  \n",
       " NCS_like_responsibility             NaN        0.000000  \n",
       " zipcode_median_income_acs           NaN        0.000000  \n",
       " \n",
       " [78 rows x 10 columns],\n",
       " 'group_correlations':                            ichi_cor    ni_cor   san_cor\n",
       " BSCS                      -0.137476  0.448089 -0.079543\n",
       " ACES_abuse                 0.146592 -0.124513 -0.414056\n",
       " RS                         0.038831 -0.184627  0.394202\n",
       " TRSQ                       0.090684 -0.239024  0.427083\n",
       " ACES_neglectful_parenting -0.046428 -0.045721 -0.476010\n",
       " EDM                        0.053087  0.281601 -0.129810\n",
       " BIS_11                     0.046739 -0.527679  0.019656\n",
       " PCS                       -0.058229 -0.065661 -0.061743,\n",
       " 'final_results_wide':                             coef           feature_importance          \n",
       " group                       base   ni  san               base   ni  san\n",
       " measure                                                                \n",
       " ACES_abuse                  -0.0  0.0 -0.0                0.0  0.0  0.0\n",
       " ACES_divorced_separated     -0.0  0.0  0.0                0.0  0.0  0.0\n",
       " ACES_household_dysfunction  -0.0  0.0  0.0                0.0  0.0  0.0\n",
       " ACES_neglectful_parenting   -0.0  0.0 -0.0                0.0  0.0  0.0\n",
       " ACES_sum                    -0.0  0.0 -0.0                0.0  0.0  0.0\n",
       " ...                          ...  ...  ...                ...  ...  ...\n",
       " education_own                0.0  0.0 -0.0                0.0  0.0  0.0\n",
       " household_income_per_person  0.0 -0.0  0.0                0.0  0.0  0.0\n",
       " ni                           0.0  NaN  NaN                0.0  NaN  NaN\n",
       " san                          0.0  NaN  NaN                0.0  NaN  NaN\n",
       " zipcode_median_income_acs    0.0  0.0  0.0                0.0  0.0  0.0\n",
       " \n",
       " [78 rows x 6 columns]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove any NA values for this outcome measure in both the predictor data and the outcome data\n",
    "outcome_nas = outcome_measures['d_cancer_promoting_minus_preventing_FFQ'].isna()\n",
    "\n",
    "outcome_measures_nona = outcome_measures.loc[~outcome_nas,:]\n",
    "predictor_data_nona = predictor_data.loc[~outcome_nas,:]\n",
    "group_assignment_onehots_nonan = group_assignment_onehots.loc[~outcome_nas,:]\n",
    "group_assignments_nona = group_assignments[~outcome_nas]\n",
    "\n",
    "### Try out CV with simple gridsearch\n",
    "\n",
    "scoring_data = do_scoring_loop(X=predictor_data_nona, y= outcome_measures_nona['d_cancer_promoting_minus_preventing_FFQ'], \n",
    "                groups = group_assignments_nona, \n",
    "                hyperparameter_selection_on_fold=do_hyperparameter_selection_loop,\n",
    "                outer_folds=5)\n",
    "\n",
    "scores = scoring_data['scores']\n",
    "best_models = scoring_data['best_models']\n",
    "best_params_df_list = scoring_data['best_params_df_list']\n",
    "raw_cv_results_list = scoring_data['raw_cv_results_list']\n",
    "\n",
    "print(\"scores:\")\n",
    "print(scores)\n",
    "overall_score = np.mean(scores)\n",
    "print(\"overall_score:\")\n",
    "print(overall_score)\n",
    "\n",
    "\n",
    "\n",
    "best_model = get_best_model(summarize_overall_df_results(raw_cv_results_list))\n",
    "final_fit = do_final_fit(X=predictor_data_nona, y= outcome_measures_nona['d_cancer_promoting_minus_preventing_FFQ'], final_model=best_model)\n",
    "final_results = present_model_results(X=predictor_data_nona, final_fit=final_fit, y=outcome_measures_nona['d_cancer_promoting_minus_preventing_FFQ'])\n",
    "\n",
    "#print rows of final_results where feature_name is the list of features to check\n",
    "base_regressors = interaction_effect_df.predictor[interaction_effect_df.interaction_effect!=0]\n",
    "regressors_to_check = [x+y for y in ['','*ni','*san'] for x in base_regressors]\n",
    "final_results['planned_regression'] = final_results['predictor'].isin(regressors_to_check)\n",
    "\n",
    "present_results_vs_ground_truth_cors(predictor_data_nona,outcome_measures_nona,group_assignments_nona,final_results,base_regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         BSCS\n",
       "7                   ACES_abuse\n",
       "4                           RS\n",
       "5                         TRSQ\n",
       "6    ACES_neglectful_parenting\n",
       "1                          EDM\n",
       "2                       BIS_11\n",
       "3                          PCS\n",
       "Name: predictor, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           ichi_cor\n",
      "BSCS                       0.032330\n",
      "ACES_abuse                 0.083333\n",
      "RS                        -0.187175\n",
      "TRSQ                      -0.020164\n",
      "ACES_neglectful_parenting  0.105595\n",
      "EDM                       -0.112815\n",
      "BIS_11                    -0.152706\n",
      "PCS                       -0.055152\n",
      "                             ni_cor\n",
      "BSCS                       0.464079\n",
      "ACES_abuse                 0.045392\n",
      "RS                        -0.321486\n",
      "TRSQ                      -0.221584\n",
      "ACES_neglectful_parenting -0.016256\n",
      "EDM                        0.142513\n",
      "BIS_11                    -0.586145\n",
      "PCS                       -0.296484\n",
      "                            san_cor\n",
      "BSCS                      -0.173121\n",
      "ACES_abuse                -0.563012\n",
      "RS                         0.392869\n",
      "TRSQ                       0.553858\n",
      "ACES_neglectful_parenting -0.502054\n",
      "EDM                       -0.184659\n",
      "BIS_11                     0.033254\n",
      "PCS                       -0.051662\n"
     ]
    }
   ],
   "source": [
    "#import sns\n",
    "\n",
    "#present_results_vs_ground_truth_cors(predictor_data_nona,outcome_measures_nona,group_assignments_nona,final_results,base_regressors)\n",
    "\n",
    "\n",
    "for group_name in ['ichi','ni','san']:\n",
    "\n",
    "    #print(group_name)\n",
    "    group_data = predictor_data_nona.loc[group_assignments_nona==group_name,:]\n",
    "    group_outcomes = outcome_measures_nona.loc[group_assignments_nona==group_name,'d_cancer_promoting_minus_preventing_FFQ']\n",
    "\n",
    "    \n",
    "    \n",
    "    #get the two-way correlation between data and the outcome column\n",
    "    #these are what was actually modeled into the data.\n",
    "    group_correlations = pd.DataFrame({group_name + '_cor':group_data[base_regressors].corrwith(group_outcomes)})\n",
    "    print(group_correlations)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground Truth:\n",
    "```\n",
    "0                              BSCS                0.15\n",
    "3                               PCS               -0.15\n",
    "1                               EDM                0.15\n",
    "2                            BIS_11               -0.15\n",
    "cancer_promoting_minus_preventing_FFQ_w2\n",
    "FFQ_v2_Mean_Energy_w2\n",
    "san\n",
    "                       feature_name  interaction_effect\n",
    "4                                RS                0.15\n",
    "5                              TRSQ                0.15\n",
    "6         ACES_neglectful_parenting               -0.15\n",
    "7                        ACES_abuse               -0.15\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KBest vs RFE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a simple feature selection pipelien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_estimator_name = 'estimator'\n",
    "feature_selection_name = 'feature_selection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_pipeline = Pipeline(steps=[('scaler', StandardScaler()),\n",
    "                ('feature_selection', RFE(estimator=linear_model.LinearRegression())),\n",
    "                ('estimator', Ridge())])\n",
    "\n",
    "estimator_params = {'alpha':[0.1,0.5,0.9]}\n",
    "feature_selection_params = {'n_features_to_select':[10,25]}\n",
    "\n",
    "\n",
    "estimator_param_grid = {(pipeline_estimator_name + '__'+k):v for k,v in estimator_params.items()}\n",
    "selector_param_grid = {(feature_selection_name + '__'+k):v for k,v in feature_selection_params.items()}\n",
    "#combine the two param grid dictionaries\n",
    "full_param_grid = {**selector_param_grid, **estimator_param_grid}\n",
    "#full_param_grid = {**estimator_param_grid}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_selection__n_features_to_select': [10, 25],\n",
       " 'estimator__alpha': [0.1, 0.5, 0.9]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 230)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_data_nona.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 230 features.\n",
      "Fitting estimator with 226 features.\n",
      "Fitting estimator with 222 features.\n",
      "Fitting estimator with 218 features.\n",
      "Fitting estimator with 214 features.\n",
      "Fitting estimator with 210 features.\n",
      "Fitting estimator with 206 features.\n",
      "Fitting estimator with 202 features.\n",
      "Fitting estimator with 198 features.\n",
      "Fitting estimator with 194 features.\n",
      "Fitting estimator with 190 features.\n",
      "Fitting estimator with 186 features.\n",
      "Fitting estimator with 182 features.\n",
      "Fitting estimator with 178 features.\n",
      "Fitting estimator with 174 features.\n",
      "Fitting estimator with 170 features.\n",
      "Fitting estimator with 166 features.\n",
      "Fitting estimator with 162 features.\n",
      "Fitting estimator with 158 features.\n",
      "Fitting estimator with 154 features.\n",
      "Fitting estimator with 150 features.\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 14 features.\n"
     ]
    }
   ],
   "source": [
    "#let's do StandardScaler, RFE, and Ridge outside of the GridSearchCV\n",
    "#I want to see how it works from one to the next\n",
    "#no pipeline, just separate steps\n",
    "#I want to see how it works from one to the next\n",
    "#no pipeline, just separate steps\n",
    "scaler = StandardScaler()\n",
    "scaled_predictor_data = scaler.fit_transform(predictor_data_nona)\n",
    "rfe = RFE(estimator=linear_model.LinearRegression(),verbose=1,n_features_to_select=10,step=4)\n",
    "rfe.fit(scaled_predictor_data,outcome_measures_nona['d_cancer_promoting_minus_preventing_FFQ'])\n",
    "rfe_results = pd.DataFrame({'predictor':predictor_data_nona.columns,'rfe_support':rfe.support_,'rfe_ranking':rfe.ranking_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictor</th>\n",
       "      <th>rfe_support</th>\n",
       "      <th>rfe_ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BSCS</td>\n",
       "      <td>False</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EDM</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BIS_11</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PCS</td>\n",
       "      <td>False</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RS</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>SST_PostErrorSlowW1_mean*san</td>\n",
       "      <td>False</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>SST_mean_ssrt_0*san</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>ROC_Crave_Regulate_Minus_Look*san</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>WTP_unhealthy_minus_healthy*san</td>\n",
       "      <td>False</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>birthsex_factor_Male*san</td>\n",
       "      <td>False</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             predictor  rfe_support  rfe_ranking\n",
       "0                                 BSCS        False           48\n",
       "1                                  EDM        False           16\n",
       "2                               BIS_11        False           12\n",
       "3                                  PCS        False           23\n",
       "4                                   RS        False           29\n",
       "..                                 ...          ...          ...\n",
       "225       SST_PostErrorSlowW1_mean*san        False           55\n",
       "226                SST_mean_ssrt_0*san        False           10\n",
       "227  ROC_Crave_Regulate_Minus_Look*san        False           18\n",
       "228    WTP_unhealthy_minus_healthy*san        False           47\n",
       "229           birthsex_factor_Male*san        False           41\n",
       "\n",
       "[230 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model with RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loops through the different estimators and feature selection methods and does a grid search over all to find the best hyperparameters\n",
    "def do_hyperparameter_selection_loop(X, y,cv):\n",
    "    #alpha parameters for Ridge and Lasso\n",
    "    alpha_10pow_lower = 1\n",
    "    alpha_10pow_upper = 0\n",
    "    alpha_increments=1\n",
    "    alpha_range = np.concatenate([np.power(10,np.linspace(-alpha_10pow_lower,alpha_10pow_upper,(alpha_10pow_lower+alpha_10pow_upper)*alpha_increments+1)),\n",
    "        [0.2,0.4,0.6,0.8,1.0]])\n",
    "    \n",
    "    all_cv_results = []\n",
    "\n",
    "    pipeline_estimator_name = 'estimator'\n",
    "    feature_selection_name = 'feature_selection'\n",
    "\n",
    "\n",
    "    #define the param_grid for the estimators\n",
    "    estimators_to_run = {\n",
    "        'Ridge':{\n",
    "            'estimator':linear_model.Ridge,\n",
    "            'parameters':{'alpha':alpha_range}\n",
    "        },\n",
    "        'Lasso':{\n",
    "            'estimator':linear_model.Lasso,\n",
    "            'parameters':{'alpha':alpha_range}\n",
    "        },\n",
    "        'DecisionTreeRegressor':{\n",
    "            'estimator':DecisionTreeRegressor,\n",
    "            'parameters':{\n",
    "                'max_depth':[2, 4],\n",
    "                'min_samples_split':[20,50],\n",
    "                'min_samples_leaf':[20,50]\n",
    "            }\n",
    "        }             \n",
    "    }\n",
    "\n",
    "    for estimator_name,estimator_dict in estimators_to_run.items():\n",
    "        #param grid for the feature seelction\n",
    "        #this is here because we need to know the estimator to pass to the feature selector\n",
    "        feature_selectors_to_run = {\n",
    "            'None':None,\n",
    "            'KBest':{\n",
    "                'selector':SelectKBest(),\n",
    "                'parameters':{\n",
    "                    'score_func' : [f_regression], \n",
    "                    'k' : [20,50]\n",
    "                    }\n",
    "            },\n",
    "            'RFE':{\n",
    "                'selector':RFE(linear_model.LinearRegression()),\n",
    "                'parameters':{\n",
    "                    'n_features_to_select' : [10,25],\n",
    "                    #'verbose':[1],\n",
    "                    'step':[5]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        for selector_name, selector_dict in feature_selectors_to_run.items():\n",
    "        #create the estimator\n",
    "            if selector_name == 'None':\n",
    "                pipeline = Pipeline([('scaler',StandardScaler()),\n",
    "                                     (pipeline_estimator_name,estimator_dict['estimator']())])\n",
    "                selector_params = {}\n",
    "            else:\n",
    "                pipeline = Pipeline([('scaler',StandardScaler()),\n",
    "                                     (feature_selection_name,selector_dict['selector']), \n",
    "                                     (pipeline_estimator_name,estimator_dict['estimator']())])\n",
    "                selector_params = selector_dict['parameters']\n",
    "\n",
    "            estimator_param_grid = {(pipeline_estimator_name + '__'+k):v for k,v in estimator_dict['parameters'].items()}\n",
    "            selector_param_grid = {(feature_selection_name + '__'+k):v for k,v in selector_params.items()}\n",
    "            #combine the two param grid dictionaries\n",
    "            full_param_grid = {**selector_param_grid, **estimator_param_grid}\n",
    "            print(pipeline)\n",
    "            print(full_param_grid)\n",
    "\n",
    "            \n",
    "        \n",
    "            gs_1 = GridSearchCV(estimator=pipeline, \n",
    "                                param_grid = full_param_grid, \n",
    "                                cv=cv,scoring='neg_mean_absolute_error',verbose=1)\n",
    "            gs_1.fit(X,y)\n",
    "            all_cv_results.append(gs_1)\n",
    "\n",
    "    #create a dataframe with the best parameters, best mean_test_score, and name of the model\n",
    "\n",
    "    best_params_df = pd.DataFrame({\n",
    "        'model': [cv_result.estimator for cv_result in all_cv_results],\n",
    "        'model_name': [cv_result.estimator.__class__.__name__ for cv_result in all_cv_results],\n",
    "        'best_params': [extract_estimator_params_from_gridsearch(cv_result.best_params_) for cv_result in all_cv_results],\n",
    "        'best_score': [cv_result.best_score_ for cv_result in all_cv_results],\n",
    "        'best_raw_params' : [cv_result.best_params_ for cv_result in all_cv_results]\n",
    "        })\n",
    "    \n",
    "    best_params_df = best_params_df.sort_values('best_score',ascending=False).reset_index(drop=True)\n",
    "\n",
    "    best_model = clone(best_params_df['model'][0])\n",
    "    best_model_params = best_params_df['best_raw_params'][0]\n",
    "    best_model.set_params(**best_model_params)\n",
    "\n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_params_df':best_params_df,\n",
    "        'raw_cv_results':all_cv_results\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ni' 'san']\n",
      "[1.28335298 0.42953651]\n",
      "['san' 'san' 'ni' 'ichi' 'san' 'san' 'ichi' 'san' 'san' 'san' 'ni' 'ichi'\n",
      " 'ichi' 'ichi' 'ichi' 'san' 'san' 'san' 'ichi' 'ichi' 'san' 'san' 'ni'\n",
      " 'ni' 'ni' 'ni' 'ni' 'ni' 'ni' 'san' 'ni' 'san' 'ni' 'ichi' 'ni' 'san'\n",
      " 'ni' 'ichi' 'san' 'ni' 'ni' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ni' 'ni'\n",
      " 'san' 'ichi' 'ichi' 'ni' 'san' 'ni' 'ichi' 'ni' 'ni' 'ni' 'ichi' 'san'\n",
      " 'ni' 'ni' 'ichi' 'ni' 'ichi' 'san' 'ni' 'ni' 'ni' 'san' 'ichi' 'ni' 'san'\n",
      " 'ichi' 'san' 'ni' 'san' 'ni' 'ichi' 'ichi' 'san' 'ichi' 'san' 'san' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'san' 'ni' 'san' 'ni' 'ichi' 'san' 'san' 'san' 'ichi'\n",
      " 'ni' 'san' 'ichi' 'ichi' 'san' 'ni' 'ichi' 'san' 'ni' 'ni' 'san' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'ichi' 'ni' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ichi'\n",
      " 'ni' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ichi' 'san' 'san' 'ni' 'ichi' 'ni'\n",
      " 'ichi' 'ichi' 'san' 'ichi' 'ni' 'san' 'san' 'ni' 'ni' 'san' 'san' 'san'\n",
      " 'ichi' 'san' 'ni' 'san' 'ichi' 'ichi' 'ichi' 'ni' 'san' 'ni' 'ni' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'san' 'ni' 'san' 'ichi' 'ni' 'ni' 'ni' 'ichi' 'ni'\n",
      " 'ichi' 'san' 'san' 'san' 'san' 'ichi' 'ni' 'san' 'san' 'san' 'ichi' 'san'\n",
      " 'ni' 'ni' 'san' 'ichi' 'ichi' 'san' 'ni' 'ni' 'san' 'ni' 'san' 'san' 'ni'\n",
      " 'san' 'ni' 'ni' 'san' 'ichi' 'san' 'ichi' 'san' 'ni' 'ni' 'ni' 'ichi'\n",
      " 'ni' 'ni' 'san' 'ni' 'ni' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ni' 'ni' 'san'\n",
      " 'ichi' 'ni' 'ichi' 'ichi' 'ichi' 'ichi' 'ichi' 'ichi' 'san' 'ichi' 'san'\n",
      " 'ichi' 'ni' 'san' 'san' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ni' 'san' 'san'\n",
      " 'ichi' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ni' 'ni' 'ni' 'san' 'ichi'\n",
      " 'ichi' 'ichi' 'ni' 'ichi' 'ni' 'san' 'ichi' 'san' 'san' 'ni' 'san' 'san'\n",
      " 'san' 'san' 'ichi' 'ichi' 'ichi' 'ni' 'ni' 'ichi' 'san' 'san' 'san']\n",
      "['ichi' 'ni' 'san']\n",
      "ichi\n",
      "no interaction effects for group: ichi. No effects will be included for this group.\n",
      "ni\n",
      "                   feature_name  interaction_effect\n",
      "0                          BSCS                0.15\n",
      "3                           PCS               -0.15\n",
      "1                           EDM                0.15\n",
      "2                        BIS_11               -0.15\n",
      "74  WTP_unhealthy_minus_healthy                0.00\n",
      "bf_2\n",
      "cancer_promoting_minus_preventing_FFQ_w2\n",
      "FFQ_v2_Mean_Energy_w2\n",
      "san\n",
      "                feature_name  interaction_effect\n",
      "4                         RS                0.15\n",
      "5                       TRSQ                0.15\n",
      "6  ACES_neglectful_parenting               -0.15\n",
      "7                 ACES_abuse               -0.15\n",
      "0                       BSCS                0.00\n",
      "bf_2\n",
      "cancer_promoting_minus_preventing_FFQ_w2\n",
      "FFQ_v2_Mean_Energy_w2\n",
      "(275, 76)\n",
      "(275, 76)\n",
      "outer split0\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Ridge())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Lasso())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "outer split1\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Ridge())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Lasso())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "outer split2\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Ridge())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Lasso())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "outer split3\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Ridge())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Lasso())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "outer split4\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Ridge())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Lasso())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "scores:\n",
      "[0.30980553714127046, 0.8228896608430611, 0.6924333506110059, 0.7741724426356583, 0.7013864341623488]\n",
      "overall_score:\n",
      "0.6601374850786689\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean_test_score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">std_test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_description</th>\n",
       "      <th>params_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.242582</td>\n",
       "      <td>0.011935</td>\n",
       "      <td>0.015178</td>\n",
       "      <td>0.011331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.242612</td>\n",
       "      <td>0.011707</td>\n",
       "      <td>0.015433</td>\n",
       "      <td>0.010934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.242612</td>\n",
       "      <td>0.012019</td>\n",
       "      <td>0.015016</td>\n",
       "      <td>0.011438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.242683</td>\n",
       "      <td>0.011450</td>\n",
       "      <td>0.015649</td>\n",
       "      <td>0.010525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.242750</td>\n",
       "      <td>0.011186</td>\n",
       "      <td>0.015851</td>\n",
       "      <td>0.010169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.242822</td>\n",
       "      <td>0.010281</td>\n",
       "      <td>0.016033</td>\n",
       "      <td>0.009289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">dict_values([StandardScaler(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-0.244519</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>0.027355</td>\n",
       "      <td>0.011346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-0.244999</td>\n",
       "      <td>0.009106</td>\n",
       "      <td>0.022486</td>\n",
       "      <td>0.008245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-0.244999</td>\n",
       "      <td>0.009106</td>\n",
       "      <td>0.022486</td>\n",
       "      <td>0.008245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-0.245135</td>\n",
       "      <td>0.007674</td>\n",
       "      <td>0.029332</td>\n",
       "      <td>0.012889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.254590</td>\n",
       "      <td>0.011775</td>\n",
       "      <td>0.021963</td>\n",
       "      <td>0.006884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">dict_values([StandardScaler(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-0.255357</td>\n",
       "      <td>0.009243</td>\n",
       "      <td>0.020866</td>\n",
       "      <td>0.005133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-0.255357</td>\n",
       "      <td>0.009243</td>\n",
       "      <td>0.020866</td>\n",
       "      <td>0.005133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-0.255357</td>\n",
       "      <td>0.009243</td>\n",
       "      <td>0.020866</td>\n",
       "      <td>0.005133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.255646</td>\n",
       "      <td>0.012527</td>\n",
       "      <td>0.021824</td>\n",
       "      <td>0.007307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-0.256425</td>\n",
       "      <td>0.009751</td>\n",
       "      <td>0.022413</td>\n",
       "      <td>0.006419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.256914</td>\n",
       "      <td>0.012422</td>\n",
       "      <td>0.021539</td>\n",
       "      <td>0.007417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.257849</td>\n",
       "      <td>0.016027</td>\n",
       "      <td>0.023884</td>\n",
       "      <td>0.008038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.258352</td>\n",
       "      <td>0.011138</td>\n",
       "      <td>0.018696</td>\n",
       "      <td>0.009280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.258352</td>\n",
       "      <td>0.011138</td>\n",
       "      <td>0.018696</td>\n",
       "      <td>0.009280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.258352</td>\n",
       "      <td>0.011138</td>\n",
       "      <td>0.018696</td>\n",
       "      <td>0.009280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.258352</td>\n",
       "      <td>0.011138</td>\n",
       "      <td>0.018696</td>\n",
       "      <td>0.009280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.258616</td>\n",
       "      <td>0.012240</td>\n",
       "      <td>0.021106</td>\n",
       "      <td>0.007612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.258661</td>\n",
       "      <td>0.014233</td>\n",
       "      <td>0.023696</td>\n",
       "      <td>0.007287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.261116</td>\n",
       "      <td>0.012153</td>\n",
       "      <td>0.020479</td>\n",
       "      <td>0.007786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.261134</td>\n",
       "      <td>0.016149</td>\n",
       "      <td>0.018140</td>\n",
       "      <td>0.010146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.261239</td>\n",
       "      <td>0.015651</td>\n",
       "      <td>0.019012</td>\n",
       "      <td>0.009938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.261436</td>\n",
       "      <td>0.014048</td>\n",
       "      <td>0.024191</td>\n",
       "      <td>0.004762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.261436</td>\n",
       "      <td>0.014048</td>\n",
       "      <td>0.024191</td>\n",
       "      <td>0.004762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.262192</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.009516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.262192</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.009516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.262192</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.009516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.262192</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.009516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.262944</td>\n",
       "      <td>0.012102</td>\n",
       "      <td>0.019896</td>\n",
       "      <td>0.007774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.264003</td>\n",
       "      <td>0.015807</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.264003</td>\n",
       "      <td>0.015807</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.281254</td>\n",
       "      <td>0.008730</td>\n",
       "      <td>0.022105</td>\n",
       "      <td>0.005503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.281254</td>\n",
       "      <td>0.008730</td>\n",
       "      <td>0.022105</td>\n",
       "      <td>0.005503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.281254</td>\n",
       "      <td>0.008730</td>\n",
       "      <td>0.022105</td>\n",
       "      <td>0.005503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.281254</td>\n",
       "      <td>0.008730</td>\n",
       "      <td>0.022105</td>\n",
       "      <td>0.005503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.287340</td>\n",
       "      <td>0.009130</td>\n",
       "      <td>0.020441</td>\n",
       "      <td>0.005599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.287340</td>\n",
       "      <td>0.009130</td>\n",
       "      <td>0.020441</td>\n",
       "      <td>0.005599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.287423</td>\n",
       "      <td>0.009045</td>\n",
       "      <td>0.020541</td>\n",
       "      <td>0.005753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.287423</td>\n",
       "      <td>0.009045</td>\n",
       "      <td>0.020541</td>\n",
       "      <td>0.005753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.287423</td>\n",
       "      <td>0.009045</td>\n",
       "      <td>0.020541</td>\n",
       "      <td>0.005753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.288246</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.021956</td>\n",
       "      <td>0.006772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.288246</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.021956</td>\n",
       "      <td>0.006772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.288329</td>\n",
       "      <td>0.008802</td>\n",
       "      <td>0.022055</td>\n",
       "      <td>0.006873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.1}</th>\n",
       "      <td>-0.289938</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>0.021273</td>\n",
       "      <td>0.012721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.290794</td>\n",
       "      <td>0.005968</td>\n",
       "      <td>0.022867</td>\n",
       "      <td>0.004430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.291894</td>\n",
       "      <td>0.006714</td>\n",
       "      <td>0.023126</td>\n",
       "      <td>0.004239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.293261</td>\n",
       "      <td>0.007252</td>\n",
       "      <td>0.023502</td>\n",
       "      <td>0.003641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.293429</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>0.023930</td>\n",
       "      <td>0.005311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.293429</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>0.023930</td>\n",
       "      <td>0.005311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.293429</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>0.023930</td>\n",
       "      <td>0.005311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.293429</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>0.023930</td>\n",
       "      <td>0.005311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.294541</td>\n",
       "      <td>0.003920</td>\n",
       "      <td>0.021524</td>\n",
       "      <td>0.009170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.294999</td>\n",
       "      <td>0.008027</td>\n",
       "      <td>0.024075</td>\n",
       "      <td>0.002824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.295234</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>0.020316</td>\n",
       "      <td>0.011688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.295299</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.011895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.295906</td>\n",
       "      <td>0.004420</td>\n",
       "      <td>0.020502</td>\n",
       "      <td>0.012719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.296761</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.020528</td>\n",
       "      <td>0.011767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.297494</td>\n",
       "      <td>0.009146</td>\n",
       "      <td>0.024906</td>\n",
       "      <td>0.001832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.297517</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.021779</td>\n",
       "      <td>0.010156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.299374</td>\n",
       "      <td>0.009869</td>\n",
       "      <td>0.025489</td>\n",
       "      <td>0.001573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.301676</td>\n",
       "      <td>0.004708</td>\n",
       "      <td>0.022387</td>\n",
       "      <td>0.010546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.307907</td>\n",
       "      <td>0.004984</td>\n",
       "      <td>0.023463</td>\n",
       "      <td>0.010707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.319563</td>\n",
       "      <td>0.006303</td>\n",
       "      <td>0.026823</td>\n",
       "      <td>0.011431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.330986</td>\n",
       "      <td>0.007142</td>\n",
       "      <td>0.030889</td>\n",
       "      <td>0.012083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.332344</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>0.022035</td>\n",
       "      <td>0.013580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.2}</th>\n",
       "      <td>-0.332344</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>0.022035</td>\n",
       "      <td>0.013580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.332367</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>0.022066</td>\n",
       "      <td>0.013625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.337554</td>\n",
       "      <td>0.003931</td>\n",
       "      <td>0.019283</td>\n",
       "      <td>0.012368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.338178</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>0.019516</td>\n",
       "      <td>0.011388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">dict_values([StandardScaler(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 1.0}</th>\n",
       "      <td>-0.338501</td>\n",
       "      <td>0.029098</td>\n",
       "      <td>0.029414</td>\n",
       "      <td>0.010589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8}</th>\n",
       "      <td>-0.347916</td>\n",
       "      <td>0.033965</td>\n",
       "      <td>0.030728</td>\n",
       "      <td>0.011530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6}</th>\n",
       "      <td>-0.360835</td>\n",
       "      <td>0.037915</td>\n",
       "      <td>0.032118</td>\n",
       "      <td>0.012033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4}</th>\n",
       "      <td>-0.379385</td>\n",
       "      <td>0.042756</td>\n",
       "      <td>0.033621</td>\n",
       "      <td>0.013626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.2}</th>\n",
       "      <td>-0.411027</td>\n",
       "      <td>0.047215</td>\n",
       "      <td>0.035238</td>\n",
       "      <td>0.017128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1}</th>\n",
       "      <td>-0.439350</td>\n",
       "      <td>0.048514</td>\n",
       "      <td>0.036170</td>\n",
       "      <td>0.019874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.451078</td>\n",
       "      <td>0.005362</td>\n",
       "      <td>0.027566</td>\n",
       "      <td>0.012011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.451078</td>\n",
       "      <td>0.005362</td>\n",
       "      <td>0.027566</td>\n",
       "      <td>0.012011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.4}</th>\n",
       "      <td>-0.451078</td>\n",
       "      <td>0.005362</td>\n",
       "      <td>0.027566</td>\n",
       "      <td>0.012011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.454651</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>0.025386</td>\n",
       "      <td>0.012674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.454698</td>\n",
       "      <td>0.004588</td>\n",
       "      <td>0.025313</td>\n",
       "      <td>0.012682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.6}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.010321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.010321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 1.0}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.009731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.009731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.010321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.010321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.010321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.010321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.009731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.009731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.009731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.010321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.010321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.010321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.530314</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.010321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing permutation test on importance; this may take time.\n",
      "Number of selected features: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictor</th>\n",
       "      <th>coef</th>\n",
       "      <th>feature_importance</th>\n",
       "      <th>fa_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>BSCS*ni</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>1.969307</td>\n",
       "      <td>1.969307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>TRSQ*san</td>\n",
       "      <td>0.459476</td>\n",
       "      <td>1.060477</td>\n",
       "      <td>1.060477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>SRHI_sum*san</td>\n",
       "      <td>-0.205621</td>\n",
       "      <td>0.216856</td>\n",
       "      <td>0.216856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>ACES_abuse*san</td>\n",
       "      <td>-0.090189</td>\n",
       "      <td>0.046144</td>\n",
       "      <td>0.046144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>ACES_neglectful_parenting*san</td>\n",
       "      <td>-0.053777</td>\n",
       "      <td>0.018320</td>\n",
       "      <td>0.018320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>ACES_sum*san</td>\n",
       "      <td>-0.056054</td>\n",
       "      <td>0.017809</td>\n",
       "      <td>0.017809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>SRHI_healthy*san</td>\n",
       "      <td>0.046549</td>\n",
       "      <td>0.010450</td>\n",
       "      <td>0.010450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>ACES_household_dysfunction*san</td>\n",
       "      <td>0.037219</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>0.007228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>ACES_divorced_separated*san</td>\n",
       "      <td>-0.004476</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>SRHI_healthy_minus_unhealthy*san</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/Google Drive/oregon/code/DEV_scripts/analyses/intervention_moderation/dev_interaction_util.py:585: FutureWarning: merging between different levels is deprecated and will be removed in a future version. (2 levels on the left, 1 on the right)\n",
      "  results_vs_cors = final_results_wide.merge(group_correlations, left_index=True, right_index=True, how='outer')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(coef, ni)</th>\n",
       "      <th>(coef, san)</th>\n",
       "      <th>(feature_importance, ni)</th>\n",
       "      <th>(feature_importance, san)</th>\n",
       "      <th>ichi_cor</th>\n",
       "      <th>ni_cor</th>\n",
       "      <th>san_cor</th>\n",
       "      <th>abs_effect_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BSCS</th>\n",
       "      <td>0.607</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.969</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>1.969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRSQ</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.060</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.427</td>\n",
       "      <td>1.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRHI_sum</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACES_abuse</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACES_neglectful_parenting</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACES_sum</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRHI_healthy</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACES_household_dysfunction</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'results_vs_cors':                               (coef, ni)  (coef, san)  \\\n",
       " BSCS                            0.606557          NaN   \n",
       " TRSQ                                 NaN     0.459476   \n",
       " SRHI_sum                             NaN    -0.205621   \n",
       " ACES_abuse                           NaN    -0.090189   \n",
       " ACES_neglectful_parenting            NaN    -0.053777   \n",
       " ACES_sum                             NaN    -0.056054   \n",
       " SRHI_healthy                         NaN     0.046549   \n",
       " ACES_household_dysfunction           NaN     0.037219   \n",
       " ACES_divorced_separated              NaN    -0.004476   \n",
       " SRHI_healthy_minus_unhealthy         NaN     0.000224   \n",
       " BIS_11                               NaN          NaN   \n",
       " EDM                                  NaN          NaN   \n",
       " PCS                                  NaN          NaN   \n",
       " RS                                   NaN          NaN   \n",
       " \n",
       "                               (feature_importance, ni)  \\\n",
       " BSCS                                          1.969307   \n",
       " TRSQ                                               NaN   \n",
       " SRHI_sum                                           NaN   \n",
       " ACES_abuse                                         NaN   \n",
       " ACES_neglectful_parenting                          NaN   \n",
       " ACES_sum                                           NaN   \n",
       " SRHI_healthy                                       NaN   \n",
       " ACES_household_dysfunction                         NaN   \n",
       " ACES_divorced_separated                            NaN   \n",
       " SRHI_healthy_minus_unhealthy                       NaN   \n",
       " BIS_11                                             NaN   \n",
       " EDM                                                NaN   \n",
       " PCS                                                NaN   \n",
       " RS                                                 NaN   \n",
       " \n",
       "                               (feature_importance, san)  ichi_cor    ni_cor  \\\n",
       " BSCS                                                NaN -0.137476  0.448089   \n",
       " TRSQ                                           1.060477  0.090684 -0.239024   \n",
       " SRHI_sum                                       0.216856       NaN       NaN   \n",
       " ACES_abuse                                     0.046144  0.146592 -0.124513   \n",
       " ACES_neglectful_parenting                      0.018320 -0.046428 -0.045721   \n",
       " ACES_sum                                       0.017809       NaN       NaN   \n",
       " SRHI_healthy                                   0.010450       NaN       NaN   \n",
       " ACES_household_dysfunction                     0.007228       NaN       NaN   \n",
       " ACES_divorced_separated                        0.000138       NaN       NaN   \n",
       " SRHI_healthy_minus_unhealthy                   0.000003       NaN       NaN   \n",
       " BIS_11                                              NaN  0.046739 -0.527679   \n",
       " EDM                                                 NaN  0.053087  0.281601   \n",
       " PCS                                                 NaN -0.058229 -0.065661   \n",
       " RS                                                  NaN  0.038831 -0.184627   \n",
       " \n",
       "                                san_cor  abs_effect_sum  \n",
       " BSCS                         -0.079543        1.969307  \n",
       " TRSQ                          0.427083        1.060477  \n",
       " SRHI_sum                           NaN        0.216856  \n",
       " ACES_abuse                   -0.414056        0.046144  \n",
       " ACES_neglectful_parenting    -0.476010        0.018320  \n",
       " ACES_sum                           NaN        0.017809  \n",
       " SRHI_healthy                       NaN        0.010450  \n",
       " ACES_household_dysfunction         NaN        0.007228  \n",
       " ACES_divorced_separated            NaN        0.000138  \n",
       " SRHI_healthy_minus_unhealthy       NaN        0.000003  \n",
       " BIS_11                        0.019656        0.000000  \n",
       " EDM                          -0.129810        0.000000  \n",
       " PCS                          -0.061743        0.000000  \n",
       " RS                            0.394202        0.000000  ,\n",
       " 'group_correlations':                            ichi_cor    ni_cor   san_cor\n",
       " BSCS                      -0.137476  0.448089 -0.079543\n",
       " ACES_abuse                 0.146592 -0.124513 -0.414056\n",
       " RS                         0.038831 -0.184627  0.394202\n",
       " TRSQ                       0.090684 -0.239024  0.427083\n",
       " ACES_neglectful_parenting -0.046428 -0.045721 -0.476010\n",
       " EDM                        0.053087  0.281601 -0.129810\n",
       " BIS_11                     0.046739 -0.527679  0.019656\n",
       " PCS                       -0.058229 -0.065661 -0.061743,\n",
       " 'final_results_wide':                                   coef           feature_importance          \n",
       " group                               ni       san                 ni       san\n",
       " measure                                                                      \n",
       " ACES_abuse                         NaN -0.090189                NaN  0.046144\n",
       " ACES_divorced_separated            NaN -0.004476                NaN  0.000138\n",
       " ACES_household_dysfunction         NaN  0.037219                NaN  0.007228\n",
       " ACES_neglectful_parenting          NaN -0.053777                NaN  0.018320\n",
       " ACES_sum                           NaN -0.056054                NaN  0.017809\n",
       " BSCS                          0.606557       NaN           1.969307       NaN\n",
       " SRHI_healthy                       NaN  0.046549                NaN  0.010450\n",
       " SRHI_healthy_minus_unhealthy       NaN  0.000224                NaN  0.000003\n",
       " SRHI_sum                           NaN -0.205621                NaN  0.216856\n",
       " TRSQ                               NaN  0.459476                NaN  1.060477}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#set np random seed\n",
    "np.random.seed(3161527)\n",
    "\n",
    "group_names = ['ichi','ni','san']\n",
    "#assign each row randomly to a group\n",
    "group_assignments = np.random.choice(group_names,analysis_data_imputed.shape[0])\n",
    "\n",
    "#synthetic outcomes\n",
    "outcome_measures = generate_synthetic_dev_outcomes(outcome_measures)\n",
    "\n",
    "# add synthetic primary and interaction effects\n",
    "\n",
    "\n",
    "#set up the interaction effects\n",
    "custom_interaction_effects_g1 = [0]*analysis_data_imputed.shape[1]\n",
    "custom_interaction_effects_g1[0] = 0.15\n",
    "custom_interaction_effects_g1[1] = 0.15\n",
    "custom_interaction_effects_g1[2] = -0.15\n",
    "custom_interaction_effects_g1[3] = -0.15\n",
    "\n",
    "custom_interaction_effects_g2 = [0]*analysis_data_imputed.shape[1]\n",
    "custom_interaction_effects_g2[4] = 0.15\n",
    "custom_interaction_effects_g2[5] = 0.15\n",
    "custom_interaction_effects_g2[6] = -0.15\n",
    "custom_interaction_effects_g2[7] = -0.15\n",
    "\n",
    "custom_interaction_effects = {'ni':custom_interaction_effects_g1,'san':custom_interaction_effects_g2}\n",
    "\n",
    "\n",
    "\n",
    "synthetic_data = generate_synthetic_dev_data(analysis_data_imputed, group_assignments,outcome_measures, group_interaction_effects = custom_interaction_effects)\n",
    "interaction_effect_df = synthetic_data['X_weights']\n",
    "outcome_measures = synthetic_data['y']\n",
    "\n",
    "# Set up outcome measures and group assignment one-hot\n",
    "\n",
    "outcome_measures = calculate_outcome_changes(outcome_measures)\n",
    "group_assignment_onehots = pd.get_dummies(group_assignments).loc[:,['ni','san']]\n",
    "\n",
    "predictor_data = set_up_interactions(analysis_data_imputed, group_assignment_onehots)\n",
    "\n",
    "\n",
    "#remove any NA values for this outcome measure in both the predictor data and the outcome data\n",
    "outcome_nas = outcome_measures['d_cancer_promoting_minus_preventing_FFQ'].isna()\n",
    "\n",
    "outcome_measures_nona = outcome_measures.loc[~outcome_nas,:]\n",
    "predictor_data_nona = predictor_data.loc[~outcome_nas,:]\n",
    "group_assignment_onehots_nonan = group_assignment_onehots.loc[~outcome_nas,:]\n",
    "group_assignments_nona = group_assignments[~outcome_nas]\n",
    "\n",
    "### Try out CV with simple gridsearch\n",
    "\n",
    "scoring_data = do_scoring_loop(X=predictor_data_nona, y= outcome_measures_nona['d_cancer_promoting_minus_preventing_FFQ'], \n",
    "                groups = group_assignments_nona, \n",
    "                hyperparameter_selection_on_fold=do_hyperparameter_selection_loop,\n",
    "                outer_folds=5)\n",
    "\n",
    "scores = scoring_data['scores']\n",
    "best_models = scoring_data['best_models']\n",
    "best_params_df_list = scoring_data['best_params_df_list']\n",
    "raw_cv_results_list = scoring_data['raw_cv_results_list']\n",
    "\n",
    "print(\"scores:\")\n",
    "print(scores)\n",
    "overall_score = np.mean(scores)\n",
    "print(\"overall_score:\")\n",
    "print(overall_score)\n",
    "\n",
    "\n",
    "\n",
    "best_model = get_best_model(summarize_overall_df_results(raw_cv_results_list))\n",
    "final_fit = do_final_fit(X=predictor_data_nona, y= outcome_measures_nona['d_cancer_promoting_minus_preventing_FFQ'], final_model=best_model)\n",
    "final_results = present_model_results(X=predictor_data_nona, final_fit=final_fit, y=outcome_measures_nona['d_cancer_promoting_minus_preventing_FFQ'])\n",
    "\n",
    "#print rows of final_results where feature_name is the list of features to check\n",
    "base_regressors = interaction_effect_df.predictor[interaction_effect_df.interaction_effect!=0]\n",
    "regressors_to_check = [x+y for y in ['','*ni','*san'] for x in base_regressors]\n",
    "final_results['planned_regression'] = final_results['predictor'].isin(regressors_to_check)\n",
    "\n",
    "present_results_vs_ground_truth_cors(predictor_data_nona,outcome_measures_nona,group_assignments_nona,final_results,base_regressors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that actually worked. It seems to be left with an $R^2$ of 0.07, which is reasonably good performance, I think.\n",
    "\n",
    "I now want to re-run with a more ecologically valid design to see if that works.\n",
    "\n",
    "We probably realistically will only pick up on around 3 features per item (given the size of the dataset), and we would expect their correlations to be not much higher than r=0.3. So let's design a simulated dataset that looks like that, and re-run."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat with 4 features with r around 0.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loops through the different estimators and feature selection methods and does a grid search over all to find the best hyperparameters\n",
    "def do_hyperparameter_selection_loop_simple(X, y,cv):\n",
    "    #alpha parameters for Ridge and Lasso\n",
    "    alpha_10pow_lower = 1\n",
    "    alpha_10pow_upper = 0\n",
    "    alpha_increments=1\n",
    "    alpha_range = np.concatenate([np.power(10,np.linspace(-alpha_10pow_lower,alpha_10pow_upper,(alpha_10pow_lower+alpha_10pow_upper)*alpha_increments+1)),\n",
    "        [0.2,0.4,0.6,0.8,1.0]])\n",
    "    \n",
    "    all_cv_results = []\n",
    "\n",
    "    pipeline_estimator_name = 'estimator'\n",
    "    feature_selection_name = 'feature_selection'\n",
    "\n",
    "\n",
    "    #define the param_grid for the estimators\n",
    "    estimators_to_run = {\n",
    "        'Ridge':{\n",
    "            'estimator':linear_model.Ridge,\n",
    "            'parameters':{'alpha':alpha_range}\n",
    "        },\n",
    "        # 'Lasso':{\n",
    "        #     'estimator':linear_model.Lasso,\n",
    "        #     'parameters':{'alpha':alpha_range}\n",
    "        # },\n",
    "        'DecisionTreeRegressor':{\n",
    "            'estimator':DecisionTreeRegressor,\n",
    "            'parameters':{\n",
    "                'max_depth':[2, 4],\n",
    "                'min_samples_split':[20,50],\n",
    "                'min_samples_leaf':[20,50]\n",
    "            }\n",
    "        }             \n",
    "    }\n",
    "\n",
    "    for estimator_name,estimator_dict in estimators_to_run.items():\n",
    "        #param grid for the feature seelction\n",
    "        #this is here because we need to know the estimator to pass to the feature selector\n",
    "        feature_selectors_to_run = {\n",
    "            # 'None':None,\n",
    "            'KBest':{\n",
    "                'selector':SelectKBest(),\n",
    "                'parameters':{\n",
    "                    'score_func' : [f_regression], \n",
    "                    'k' : [20,50]\n",
    "                    }\n",
    "            }#,\n",
    "            # 'RFE':{\n",
    "            #     'selector':RFE(linear_model.LinearRegression()),\n",
    "            #     'parameters':{\n",
    "            #         'n_features_to_select' : [10,25],\n",
    "            #         #'verbose':[1],\n",
    "            #         'step':[5]\n",
    "            #     }\n",
    "            # }\n",
    "        }\n",
    "        for selector_name, selector_dict in feature_selectors_to_run.items():\n",
    "        #create the estimator\n",
    "            if selector_name == 'None':\n",
    "                pipeline = Pipeline([('scaler',StandardScaler()),\n",
    "                                     (pipeline_estimator_name,estimator_dict['estimator']())])\n",
    "                selector_params = {}\n",
    "            else:\n",
    "                pipeline = Pipeline([('scaler',StandardScaler()),\n",
    "                                     (feature_selection_name,selector_dict['selector']), \n",
    "                                     (pipeline_estimator_name,estimator_dict['estimator']())])\n",
    "                selector_params = selector_dict['parameters']\n",
    "\n",
    "            estimator_param_grid = {(pipeline_estimator_name + '__'+k):v for k,v in estimator_dict['parameters'].items()}\n",
    "            selector_param_grid = {(feature_selection_name + '__'+k):v for k,v in selector_params.items()}\n",
    "            #combine the two param grid dictionaries\n",
    "            full_param_grid = {**selector_param_grid, **estimator_param_grid}\n",
    "            print(pipeline)\n",
    "            print(full_param_grid)\n",
    "\n",
    "            \n",
    "        \n",
    "            gs_1 = GridSearchCV(estimator=pipeline, \n",
    "                                param_grid = full_param_grid, \n",
    "                                cv=cv,scoring='neg_mean_absolute_error',verbose=1)\n",
    "            gs_1.fit(X,y)\n",
    "            all_cv_results.append(gs_1)\n",
    "\n",
    "    #create a dataframe with the best parameters, best mean_test_score, and name of the model\n",
    "\n",
    "    best_params_df = pd.DataFrame({\n",
    "        'model': [cv_result.estimator for cv_result in all_cv_results],\n",
    "        'model_name': [cv_result.estimator.__class__.__name__ for cv_result in all_cv_results],\n",
    "        'best_params': [extract_estimator_params_from_gridsearch(cv_result.best_params_) for cv_result in all_cv_results],\n",
    "        'best_score': [cv_result.best_score_ for cv_result in all_cv_results],\n",
    "        'best_raw_params' : [cv_result.best_params_ for cv_result in all_cv_results]\n",
    "        })\n",
    "    \n",
    "    best_params_df = best_params_df.sort_values('best_score',ascending=False).reset_index(drop=True)\n",
    "\n",
    "    best_model = clone(best_params_df['model'][0])\n",
    "    best_model_params = best_params_df['best_raw_params'][0]\n",
    "    best_model.set_params(**best_model_params)\n",
    "\n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_params_df':best_params_df,\n",
    "        'raw_cv_results':all_cv_results\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ni' 'san']\n",
      "[1.28335298 0.42953651]\n",
      "['san' 'san' 'ni' 'ichi' 'san' 'san' 'ichi' 'san' 'san' 'san' 'ni' 'ichi'\n",
      " 'ichi' 'ichi' 'ichi' 'san' 'san' 'san' 'ichi' 'ichi' 'san' 'san' 'ni'\n",
      " 'ni' 'ni' 'ni' 'ni' 'ni' 'ni' 'san' 'ni' 'san' 'ni' 'ichi' 'ni' 'san'\n",
      " 'ni' 'ichi' 'san' 'ni' 'ni' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ni' 'ni'\n",
      " 'san' 'ichi' 'ichi' 'ni' 'san' 'ni' 'ichi' 'ni' 'ni' 'ni' 'ichi' 'san'\n",
      " 'ni' 'ni' 'ichi' 'ni' 'ichi' 'san' 'ni' 'ni' 'ni' 'san' 'ichi' 'ni' 'san'\n",
      " 'ichi' 'san' 'ni' 'san' 'ni' 'ichi' 'ichi' 'san' 'ichi' 'san' 'san' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'san' 'ni' 'san' 'ni' 'ichi' 'san' 'san' 'san' 'ichi'\n",
      " 'ni' 'san' 'ichi' 'ichi' 'san' 'ni' 'ichi' 'san' 'ni' 'ni' 'san' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'ichi' 'ni' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ichi'\n",
      " 'ni' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ichi' 'san' 'san' 'ni' 'ichi' 'ni'\n",
      " 'ichi' 'ichi' 'san' 'ichi' 'ni' 'san' 'san' 'ni' 'ni' 'san' 'san' 'san'\n",
      " 'ichi' 'san' 'ni' 'san' 'ichi' 'ichi' 'ichi' 'ni' 'san' 'ni' 'ni' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'san' 'ni' 'san' 'ichi' 'ni' 'ni' 'ni' 'ichi' 'ni'\n",
      " 'ichi' 'san' 'san' 'san' 'san' 'ichi' 'ni' 'san' 'san' 'san' 'ichi' 'san'\n",
      " 'ni' 'ni' 'san' 'ichi' 'ichi' 'san' 'ni' 'ni' 'san' 'ni' 'san' 'san' 'ni'\n",
      " 'san' 'ni' 'ni' 'san' 'ichi' 'san' 'ichi' 'san' 'ni' 'ni' 'ni' 'ichi'\n",
      " 'ni' 'ni' 'san' 'ni' 'ni' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ni' 'ni' 'san'\n",
      " 'ichi' 'ni' 'ichi' 'ichi' 'ichi' 'ichi' 'ichi' 'ichi' 'san' 'ichi' 'san'\n",
      " 'ichi' 'ni' 'san' 'san' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ni' 'san' 'san'\n",
      " 'ichi' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ni' 'ni' 'ni' 'san' 'ichi'\n",
      " 'ichi' 'ichi' 'ni' 'ichi' 'ni' 'san' 'ichi' 'san' 'san' 'ni' 'san' 'san'\n",
      " 'san' 'san' 'ichi' 'ichi' 'ichi' 'ni' 'ni' 'ichi' 'san' 'san' 'san']\n",
      "['ichi' 'ni' 'san']\n",
      "ichi\n",
      "no interaction effects for group: ichi. No effects will be included for this group.\n",
      "ni\n",
      "                   feature_name  interaction_effect\n",
      "0                          BSCS                0.08\n",
      "2                        BIS_11               -0.08\n",
      "1                           EDM                0.08\n",
      "74  WTP_unhealthy_minus_healthy                0.00\n",
      "bf_2\n",
      "cancer_promoting_minus_preventing_FFQ_w2\n",
      "FFQ_v2_Mean_Energy_w2\n",
      "san\n",
      "                feature_name  interaction_effect\n",
      "4                         RS                0.08\n",
      "5                       TRSQ                0.08\n",
      "6  ACES_neglectful_parenting               -0.08\n",
      "0                       BSCS                0.00\n",
      "bf_2\n",
      "cancer_promoting_minus_preventing_FFQ_w2\n",
      "FFQ_v2_Mean_Energy_w2\n",
      "(275, 76)\n",
      "(275, 76)\n",
      "outer split0\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Ridge())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Lasso())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "outer split1\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Ridge())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Lasso())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "outer split2\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Ridge())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Lasso())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "outer split3\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Ridge())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Lasso())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "outer split4\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Ridge())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Ridge())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Ridge())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('estimator', Lasso())])\n",
      "{'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()), ('estimator', Lasso())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', Lasso())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__alpha': array([0.1, 1. , 0.2, 0.4, 0.6, 0.8, 1. ])}\n",
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest()),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__score_func': [<function f_regression at 0x1816d7920>], 'feature_selection__k': [20, 50], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('feature_selection', RFE(estimator=LinearRegression())),\n",
      "                ('estimator', DecisionTreeRegressor())])\n",
      "{'feature_selection__n_features_to_select': [10, 25], 'feature_selection__step': [5], 'estimator__max_depth': [2, 4], 'estimator__min_samples_split': [20, 50], 'estimator__min_samples_leaf': [20, 50]}\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "scores:\n",
      "[0.7627218640787623, 0.7826353643654684, 0.8032456225910188, 0.8309837058677088, 0.8250458722553565]\n",
      "overall_score:\n",
      "0.8009264858316628\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean_test_score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">std_test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_description</th>\n",
       "      <th>params_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">dict_values([StandardScaler(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-0.208750</td>\n",
       "      <td>0.011840</td>\n",
       "      <td>0.015733</td>\n",
       "      <td>0.006939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-0.210212</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>0.015904</td>\n",
       "      <td>0.003648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-0.210212</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>0.015904</td>\n",
       "      <td>0.003648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-0.210888</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>0.015489</td>\n",
       "      <td>0.007159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-0.211880</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.016322</td>\n",
       "      <td>0.007236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-0.211880</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.016322</td>\n",
       "      <td>0.007236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-0.213641</td>\n",
       "      <td>0.006726</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.007967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-0.216589</td>\n",
       "      <td>0.007338</td>\n",
       "      <td>0.018234</td>\n",
       "      <td>0.006911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.222224</td>\n",
       "      <td>0.025497</td>\n",
       "      <td>0.037027</td>\n",
       "      <td>0.055896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.222224</td>\n",
       "      <td>0.025497</td>\n",
       "      <td>0.037027</td>\n",
       "      <td>0.055896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.222224</td>\n",
       "      <td>0.025497</td>\n",
       "      <td>0.037027</td>\n",
       "      <td>0.055896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.222224</td>\n",
       "      <td>0.025497</td>\n",
       "      <td>0.037027</td>\n",
       "      <td>0.055896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.223999</td>\n",
       "      <td>0.024345</td>\n",
       "      <td>0.038809</td>\n",
       "      <td>0.056282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.223999</td>\n",
       "      <td>0.024345</td>\n",
       "      <td>0.038809</td>\n",
       "      <td>0.056282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.224058</td>\n",
       "      <td>0.025547</td>\n",
       "      <td>0.036874</td>\n",
       "      <td>0.055467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.224058</td>\n",
       "      <td>0.025547</td>\n",
       "      <td>0.036874</td>\n",
       "      <td>0.055467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.224058</td>\n",
       "      <td>0.025547</td>\n",
       "      <td>0.036874</td>\n",
       "      <td>0.055467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.224058</td>\n",
       "      <td>0.025547</td>\n",
       "      <td>0.036874</td>\n",
       "      <td>0.055467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.224136</td>\n",
       "      <td>0.021208</td>\n",
       "      <td>0.038013</td>\n",
       "      <td>0.052424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.224475</td>\n",
       "      <td>0.022520</td>\n",
       "      <td>0.038197</td>\n",
       "      <td>0.055369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.224540</td>\n",
       "      <td>0.025566</td>\n",
       "      <td>0.037589</td>\n",
       "      <td>0.056354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.224540</td>\n",
       "      <td>0.025566</td>\n",
       "      <td>0.037589</td>\n",
       "      <td>0.056354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.224873</td>\n",
       "      <td>0.022528</td>\n",
       "      <td>0.038423</td>\n",
       "      <td>0.055103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.225231</td>\n",
       "      <td>0.024466</td>\n",
       "      <td>0.039003</td>\n",
       "      <td>0.055484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.225381</td>\n",
       "      <td>0.022584</td>\n",
       "      <td>0.038740</td>\n",
       "      <td>0.054866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.225554</td>\n",
       "      <td>0.025115</td>\n",
       "      <td>0.039272</td>\n",
       "      <td>0.055411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.225631</td>\n",
       "      <td>0.024261</td>\n",
       "      <td>0.039132</td>\n",
       "      <td>0.055424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.226131</td>\n",
       "      <td>0.022728</td>\n",
       "      <td>0.039224</td>\n",
       "      <td>0.054640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.226289</td>\n",
       "      <td>0.024614</td>\n",
       "      <td>0.038733</td>\n",
       "      <td>0.055770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.226610</td>\n",
       "      <td>0.022822</td>\n",
       "      <td>0.039527</td>\n",
       "      <td>0.054487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"16\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.240266</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>0.013792</td>\n",
       "      <td>0.005417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.240266</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>0.013792</td>\n",
       "      <td>0.005417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.241079</td>\n",
       "      <td>0.005224</td>\n",
       "      <td>0.013916</td>\n",
       "      <td>0.005162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.241425</td>\n",
       "      <td>0.002983</td>\n",
       "      <td>0.016723</td>\n",
       "      <td>0.003546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.241425</td>\n",
       "      <td>0.002983</td>\n",
       "      <td>0.016723</td>\n",
       "      <td>0.003546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.242325</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.004847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.242325</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.004847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.242448</td>\n",
       "      <td>0.003897</td>\n",
       "      <td>0.016965</td>\n",
       "      <td>0.003230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.242836</td>\n",
       "      <td>0.006256</td>\n",
       "      <td>0.011914</td>\n",
       "      <td>0.005224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.242855</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.016149</td>\n",
       "      <td>0.007061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.242855</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.016149</td>\n",
       "      <td>0.007061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.242855</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.016149</td>\n",
       "      <td>0.007061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.243183</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.016317</td>\n",
       "      <td>0.003914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.243887</td>\n",
       "      <td>0.007201</td>\n",
       "      <td>0.015268</td>\n",
       "      <td>0.006338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 4, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.243887</td>\n",
       "      <td>0.007201</td>\n",
       "      <td>0.015268</td>\n",
       "      <td>0.006338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.244613</td>\n",
       "      <td>0.007123</td>\n",
       "      <td>0.014147</td>\n",
       "      <td>0.007854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.245820</td>\n",
       "      <td>0.023602</td>\n",
       "      <td>0.036240</td>\n",
       "      <td>0.046262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.246279</td>\n",
       "      <td>0.003286</td>\n",
       "      <td>0.017623</td>\n",
       "      <td>0.002236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.247031</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.017952</td>\n",
       "      <td>0.002706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.247736</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>0.036247</td>\n",
       "      <td>0.048735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.248055</td>\n",
       "      <td>0.003739</td>\n",
       "      <td>0.018324</td>\n",
       "      <td>0.003165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.249353</td>\n",
       "      <td>0.004051</td>\n",
       "      <td>0.018837</td>\n",
       "      <td>0.003882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.250018</td>\n",
       "      <td>0.025701</td>\n",
       "      <td>0.036265</td>\n",
       "      <td>0.048361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), SelectKBest(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.251294</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.019579</td>\n",
       "      <td>0.004972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.1}</th>\n",
       "      <td>-0.252142</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>0.015357</td>\n",
       "      <td>0.009082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), SelectKBest(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.252788</td>\n",
       "      <td>0.005083</td>\n",
       "      <td>0.020180</td>\n",
       "      <td>0.005842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.252896</td>\n",
       "      <td>0.025964</td>\n",
       "      <td>0.036302</td>\n",
       "      <td>0.047969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.253855</td>\n",
       "      <td>0.004669</td>\n",
       "      <td>0.014405</td>\n",
       "      <td>0.009202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.253924</td>\n",
       "      <td>0.004374</td>\n",
       "      <td>0.014342</td>\n",
       "      <td>0.008523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.256642</td>\n",
       "      <td>0.026280</td>\n",
       "      <td>0.036415</td>\n",
       "      <td>0.047525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.259076</td>\n",
       "      <td>0.026371</td>\n",
       "      <td>0.036584</td>\n",
       "      <td>0.047340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.261115</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>0.014903</td>\n",
       "      <td>0.001523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.263935</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>0.015013</td>\n",
       "      <td>0.001170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.266201</td>\n",
       "      <td>0.021257</td>\n",
       "      <td>0.035694</td>\n",
       "      <td>0.050401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.266376</td>\n",
       "      <td>0.021382</td>\n",
       "      <td>0.035840</td>\n",
       "      <td>0.050317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.267681</td>\n",
       "      <td>0.006811</td>\n",
       "      <td>0.015077</td>\n",
       "      <td>0.000859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.273048</td>\n",
       "      <td>0.007235</td>\n",
       "      <td>0.015213</td>\n",
       "      <td>0.001182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.282788</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>0.015848</td>\n",
       "      <td>0.002263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.292567</td>\n",
       "      <td>0.008225</td>\n",
       "      <td>0.016965</td>\n",
       "      <td>0.003501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.295881</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.019702</td>\n",
       "      <td>0.009513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.2}</th>\n",
       "      <td>-0.295881</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.019702</td>\n",
       "      <td>0.009513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.295881</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.019702</td>\n",
       "      <td>0.009513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.306743</td>\n",
       "      <td>0.018068</td>\n",
       "      <td>0.034664</td>\n",
       "      <td>0.041471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.2, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.306797</td>\n",
       "      <td>0.017972</td>\n",
       "      <td>0.034761</td>\n",
       "      <td>0.041489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">dict_values([StandardScaler(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 1.0}</th>\n",
       "      <td>-0.328533</td>\n",
       "      <td>0.030837</td>\n",
       "      <td>0.029331</td>\n",
       "      <td>0.008913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8}</th>\n",
       "      <td>-0.338707</td>\n",
       "      <td>0.035591</td>\n",
       "      <td>0.030429</td>\n",
       "      <td>0.009426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6}</th>\n",
       "      <td>-0.351987</td>\n",
       "      <td>0.038923</td>\n",
       "      <td>0.031592</td>\n",
       "      <td>0.009770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4}</th>\n",
       "      <td>-0.371425</td>\n",
       "      <td>0.042667</td>\n",
       "      <td>0.033448</td>\n",
       "      <td>0.011317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.2}</th>\n",
       "      <td>-0.404473</td>\n",
       "      <td>0.046296</td>\n",
       "      <td>0.035404</td>\n",
       "      <td>0.016143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.426568</td>\n",
       "      <td>0.004414</td>\n",
       "      <td>0.024873</td>\n",
       "      <td>0.012562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.426568</td>\n",
       "      <td>0.004414</td>\n",
       "      <td>0.024873</td>\n",
       "      <td>0.012562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.4}</th>\n",
       "      <td>-0.426568</td>\n",
       "      <td>0.004414</td>\n",
       "      <td>0.024873</td>\n",
       "      <td>0.012562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.431118</td>\n",
       "      <td>0.005355</td>\n",
       "      <td>0.030038</td>\n",
       "      <td>0.015980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.431238</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.029981</td>\n",
       "      <td>0.015895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 0.1}</th>\n",
       "      <td>-0.433954</td>\n",
       "      <td>0.047430</td>\n",
       "      <td>0.037053</td>\n",
       "      <td>0.019438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.503526</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.503526</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.503526</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.503526</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 1.0}</th>\n",
       "      <td>-0.503526</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__k': 20, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.503526</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.8}</th>\n",
       "      <td>-0.503526</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.503526</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.6}</th>\n",
       "      <td>-0.503526</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.503526</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), SelectKBest(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.6, 'feature_selection__k': 50, 'feature_selection__score_func': &lt;function f_regression at 0x1816d7920&gt;}</th>\n",
       "      <td>-0.503526</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">dict_values([StandardScaler(), RFE(estimator=LinearRegression()), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.503526</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__n_features_to_select': 25, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.503526</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 1.0, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.503526</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8, 'feature_selection__n_features_to_select': 10, 'feature_selection__step': 5}</th>\n",
       "      <td>-0.503526</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing permutation test on importance; this may take time.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictor</th>\n",
       "      <th>coef</th>\n",
       "      <th>feature_importance</th>\n",
       "      <th>fa_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>IMI_perceived_choice*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>1.818891</td>\n",
       "      <td>1.818891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>TRSQ*san</td>\n",
       "      <td>None</td>\n",
       "      <td>0.166071</td>\n",
       "      <td>0.166071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BSCS</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>PCS*san</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>SST_prop_successful_stops*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>SST_GRTmean*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>SST_SSD*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>SST_PostErrorSlowW1_mean*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>SST_mean_ssrt_0*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>ROC_Crave_Regulate_Minus_Look*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>WTP_unhealthy_minus_healthy*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>birthsex_factor_Male*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>BSCS*san</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>EDM*san</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>BIS_11*san</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>RS*san</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>zipcode_median_income_acs*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>ACES_neglectful_parenting*san</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>ACES_abuse*san</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>ACES_sum*san</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/Google Drive/oregon/code/DEV_scripts/analyses/intervention_moderation/dev_interaction_util.py:593: FutureWarning: merging between different levels is deprecated and will be removed in a future version. (2 levels on the left, 1 on the right)\n",
      "  results_vs_cors = final_results_wide.merge(group_correlations, left_index=True, right_index=True, how='outer')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(feature_importance, base)</th>\n",
       "      <th>(feature_importance, ni)</th>\n",
       "      <th>(feature_importance, san)</th>\n",
       "      <th>ichi_cor</th>\n",
       "      <th>ni_cor</th>\n",
       "      <th>san_cor</th>\n",
       "      <th>abs_effect_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACES_abuse</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RTFS_factor_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SST_SSD</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SST_PostErrorSlowW1_mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SST_GRTmean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRHI_unhealthy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRHI_sum</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRHI_healthy_minus_unhealthy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRHI_healthy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RTFS_factor_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SST_prop_successful_stops</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RTFS_f1_minus_f2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RS</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC_Crave_Regulate_Minus_Look</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMQ_locomotion</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMQ_lie</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMQ_assessment</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAN_temporal_orientation</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SST_mean_ssrt_0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TESQ_E_avoidance_of_temptations</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACES_divorced_separated</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birthsex_factor_Male</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>san</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ni</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>household_income_per_person</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_own</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancer_promoting_minus_preventing_liked_FCI</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancer_promoting_minus_preventing_craved_FCI</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cSES</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age365</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TESQ_E_controlling_temptations</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WTP_unhealthy_minus_healthy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRSQ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TESQ_E_suppression</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TESQ_E_sum</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TESQ_E_goal_deliberation</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TESQ_E_goal_and_rule_setting</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TESQ_E_distraction</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAN_mental_forecasting</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAN_cognitive_strategies</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCS</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIS_11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMI_perceived_competence</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMI_perceived_choice</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.819</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMI_interest_enjoyment</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMI_effort_importance</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDM</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEMO_mcarthur_social_standing</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BSCS</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BFI_openness</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_total</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BFI_neuroticism</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BFI_extraversion</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BFI_conscientiousness</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BFI_agreeableness</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACES_sum</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACES_neglectful_parenting</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'results_vs_cors':                                (feature_importance, base)  \\\n",
       " ACES_abuse                                            0.0   \n",
       " RTFS_factor_2                                         0.0   \n",
       " SST_SSD                                               0.0   \n",
       " SST_PostErrorSlowW1_mean                              0.0   \n",
       " SST_GRTmean                                           0.0   \n",
       " ...                                                   ...   \n",
       " NCS_new_solutions_to_problems                         0.0   \n",
       " NCS_like_responsibility                               0.0   \n",
       " NCS_intellectual_task                                 0.0   \n",
       " NCS_get_job_done                                      0.0   \n",
       " zipcode_median_income_acs                             0.0   \n",
       " \n",
       "                                (feature_importance, ni)  \\\n",
       " ACES_abuse                                          0.0   \n",
       " RTFS_factor_2                                       0.0   \n",
       " SST_SSD                                             0.0   \n",
       " SST_PostErrorSlowW1_mean                            0.0   \n",
       " SST_GRTmean                                         0.0   \n",
       " ...                                                 ...   \n",
       " NCS_new_solutions_to_problems                       0.0   \n",
       " NCS_like_responsibility                             0.0   \n",
       " NCS_intellectual_task                               0.0   \n",
       " NCS_get_job_done                                    0.0   \n",
       " zipcode_median_income_acs                           0.0   \n",
       " \n",
       "                                (feature_importance, san)  ichi_cor  ni_cor  \\\n",
       " ACES_abuse                                           0.0       NaN     NaN   \n",
       " RTFS_factor_2                                        0.0       NaN     NaN   \n",
       " SST_SSD                                              0.0       NaN     NaN   \n",
       " SST_PostErrorSlowW1_mean                             0.0       NaN     NaN   \n",
       " SST_GRTmean                                          0.0       NaN     NaN   \n",
       " ...                                                  ...       ...     ...   \n",
       " NCS_new_solutions_to_problems                        0.0       NaN     NaN   \n",
       " NCS_like_responsibility                              0.0       NaN     NaN   \n",
       " NCS_intellectual_task                                0.0       NaN     NaN   \n",
       " NCS_get_job_done                                     0.0       NaN     NaN   \n",
       " zipcode_median_income_acs                            0.0       NaN     NaN   \n",
       " \n",
       "                                san_cor  abs_effect_sum  \n",
       " ACES_abuse                         NaN             0.0  \n",
       " RTFS_factor_2                      NaN             0.0  \n",
       " SST_SSD                            NaN             0.0  \n",
       " SST_PostErrorSlowW1_mean           NaN             0.0  \n",
       " SST_GRTmean                        NaN             0.0  \n",
       " ...                                ...             ...  \n",
       " NCS_new_solutions_to_problems      NaN             0.0  \n",
       " NCS_like_responsibility            NaN             0.0  \n",
       " NCS_intellectual_task              NaN             0.0  \n",
       " NCS_get_job_done                   NaN             0.0  \n",
       " zipcode_median_income_acs          NaN             0.0  \n",
       " \n",
       " [78 rows x 7 columns],\n",
       " 'group_correlations':                            ichi_cor    ni_cor   san_cor\n",
       " BSCS                      -0.137476  0.349681  0.007552\n",
       " ACES_neglectful_parenting -0.046428 -0.017313 -0.217997\n",
       " RS                         0.038831 -0.154165  0.259644\n",
       " TRSQ                       0.090684 -0.222242  0.264390\n",
       " EDM                        0.053087  0.233229 -0.056365\n",
       " BIS_11                     0.046739 -0.383142 -0.043368,\n",
       " 'final_results_wide':                             feature_importance          \n",
       " group                                     base   ni  san\n",
       " measure                                                 \n",
       " ACES_abuse                                 0.0  0.0  0.0\n",
       " ACES_divorced_separated                    0.0  0.0  0.0\n",
       " ACES_household_dysfunction                 0.0  0.0  0.0\n",
       " ACES_neglectful_parenting                  0.0  0.0  0.0\n",
       " ACES_sum                                   0.0  0.0  0.0\n",
       " ...                                        ...  ...  ...\n",
       " education_own                              0.0  0.0  0.0\n",
       " household_income_per_person                0.0  0.0  0.0\n",
       " ni                                         0.0  NaN  NaN\n",
       " san                                        0.0  NaN  NaN\n",
       " zipcode_median_income_acs                  0.0  0.0  0.0\n",
       " \n",
       " [78 rows x 3 columns]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#set np random seed\n",
    "np.random.seed(3161527)\n",
    "\n",
    "group_names = ['ichi','ni','san']\n",
    "#assign each row randomly to a group\n",
    "group_assignments = np.random.choice(group_names,analysis_data_imputed.shape[0])\n",
    "\n",
    "#synthetic outcomes\n",
    "outcome_measures = generate_synthetic_dev_outcomes(outcome_measures)\n",
    "\n",
    "# add synthetic primary and interaction effects\n",
    "\n",
    "\n",
    "#set up the interaction effects\n",
    "#0.08 will give us correlations around 0.3 between the interaction effects and the outcome\n",
    "custom_interaction_effects_g1 = [0]*analysis_data_imputed.shape[1]\n",
    "custom_interaction_effects_g1[0] = 0.08\n",
    "custom_interaction_effects_g1[1] = 0.08\n",
    "custom_interaction_effects_g1[2] = -0.08\n",
    "\n",
    "custom_interaction_effects_g2 = [0]*analysis_data_imputed.shape[1]\n",
    "custom_interaction_effects_g2[4] = 0.08\n",
    "custom_interaction_effects_g2[5] = 0.08\n",
    "custom_interaction_effects_g2[6] = -0.08\n",
    "\n",
    "custom_interaction_effects = {'ni':custom_interaction_effects_g1,'san':custom_interaction_effects_g2}\n",
    "\n",
    "\n",
    "\n",
    "synthetic_data = generate_synthetic_dev_data(analysis_data_imputed, group_assignments,outcome_measures, group_interaction_effects = custom_interaction_effects)\n",
    "interaction_effect_df = synthetic_data['X_weights']\n",
    "outcome_measures = synthetic_data['y']\n",
    "\n",
    "# Set up outcome measures and group assignment one-hot\n",
    "\n",
    "outcome_measures = calculate_outcome_changes(outcome_measures)\n",
    "group_assignment_onehots = pd.get_dummies(group_assignments).loc[:,['ni','san']]\n",
    "\n",
    "predictor_data = set_up_interactions(analysis_data_imputed, group_assignment_onehots)\n",
    "\n",
    "\n",
    "#remove any NA values for this outcome measure in both the predictor data and the outcome data\n",
    "outcome_nas = outcome_measures['d_cancer_promoting_minus_preventing_FFQ'].isna()\n",
    "\n",
    "outcome_measures_nona = outcome_measures.loc[~outcome_nas,:]\n",
    "predictor_data_nona = predictor_data.loc[~outcome_nas,:]\n",
    "group_assignment_onehots_nonan = group_assignment_onehots.loc[~outcome_nas,:]\n",
    "group_assignments_nona = group_assignments[~outcome_nas]\n",
    "\n",
    "### Try out CV with simple gridsearch\n",
    "\n",
    "scoring_data = do_scoring_loop(X=predictor_data_nona, y= outcome_measures_nona['d_cancer_promoting_minus_preventing_FFQ'], \n",
    "                groups = group_assignments_nona, \n",
    "                hyperparameter_selection_on_fold=do_hyperparameter_selection_loop,\n",
    "                outer_folds=5)\n",
    "\n",
    "scores = scoring_data['scores']\n",
    "best_models = scoring_data['best_models']\n",
    "best_params_df_list = scoring_data['best_params_df_list']\n",
    "raw_cv_results_list = scoring_data['raw_cv_results_list']\n",
    "\n",
    "print(\"scores:\")\n",
    "print(scores)\n",
    "overall_score = np.mean(scores)\n",
    "print(\"overall_score:\")\n",
    "print(overall_score)\n",
    "\n",
    "\n",
    "\n",
    "best_model = get_best_model(summarize_overall_df_results(raw_cv_results_list))\n",
    "final_fit = do_final_fit(X=predictor_data_nona, y= outcome_measures_nona['d_cancer_promoting_minus_preventing_FFQ'], final_model=best_model)\n",
    "final_results = present_model_results(X=predictor_data_nona, final_fit=final_fit, y=outcome_measures_nona['d_cancer_promoting_minus_preventing_FFQ'])\n",
    "\n",
    "#print rows of final_results where feature_name is the list of features to check\n",
    "base_regressors = interaction_effect_df.predictor[interaction_effect_df.interaction_effect!=0]\n",
    "regressors_to_check = [x+y for y in ['','*ni','*san'] for x in base_regressors]\n",
    "final_results['planned_regression'] = final_results['predictor'].isin(regressors_to_check)\n",
    "\n",
    "present_results_vs_ground_truth_cors(predictor_data_nona,outcome_measures_nona,group_assignments_nona,final_results,base_regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:\n",
      "[0.7627218640787623, 0.7826353643654684, 0.8032456225910188, 0.8309837058677088, 0.8250458722553565]\n",
      "overall_score:\n",
      "0.8009264858316628\n"
     ]
    }
   ],
   "source": [
    "print(\"scores:\")\n",
    "print(scores)\n",
    "overall_score = np.mean(scores)\n",
    "print(\"overall_score:\")\n",
    "print(overall_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "ni\n",
    "                        feature_name  interaction_effect\n",
    "0                               BSCS                 0.1\n",
    "2                             BIS_11                -0.1\n",
    "1                                EDM                 0.1\n",
    "san\n",
    "                       feature_name  interaction_effect\n",
    "4                                RS                 0.1\n",
    "5                              TRSQ                 0.1\n",
    "6         ACES_neglectful_parenting                -0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 21\u001b[0m\n\u001b[1;32m     14\u001b[0m df_melted \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmelt(graph_data, id_vars\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39moutcome\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgroup\u001b[39m\u001b[39m'\u001b[39m], var_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[39m# Create a FacetGrid with scatter plots\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m#wrap the facetgrid so that the rows and columns are equal, as much as is possible\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m#allow the axes for each facet to vary freely to best display data in each facet\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m#be sure to allow enough room for the title\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m g \u001b[39m=\u001b[39m sns\u001b[39m.\u001b[39mFacetGrid(df_melted, col\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m'\u001b[39m, hue\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgroup\u001b[39m\u001b[39m'\u001b[39m, col_wrap\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, height\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, aspect\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, sharex\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, sharey\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,margin_titles\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m \u001b[39m#do the scatterplot; include a trendline for each group\u001b[39;00m\n\u001b[1;32m     23\u001b[0m g\u001b[39m.\u001b[39mmap(sns\u001b[39m.\u001b[39mregplot,  \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39moutcome\u001b[39m\u001b[39m'\u001b[39m, ci\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, scatter_kws\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m0.5\u001b[39m}, line_kws\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m0.5\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Great--now print a scatterplot of the series group_outcomes and each column of the dataframe group_data\n",
    "# # do this separately for each group\n",
    "# for group_name in ['ichi','ni','san']:\n",
    "\n",
    "#     #print(group_name)\n",
    "#     group_data = predictor_data_nona.loc[group_assignments_nona==group_name,base_regressors]\n",
    "#     group_outcomes = outcome_measures_nona.loc[group_assignments_nona==group_name,'d_cancer_promoting_minus_preventing_FFQ']\n",
    "\n",
    "\n",
    "graph_data = predictor_data_nona.loc[:,base_regressors].copy()\n",
    "graph_data['outcome'] = outcome_measures_nona.loc[:,'d_cancer_promoting_minus_preventing_FFQ']\n",
    "graph_data['group'] = group_assignments_nona\n",
    "\n",
    "df_melted = pd.melt(graph_data, id_vars=['outcome', 'group'], var_name='columns')\n",
    "\n",
    "\n",
    "# Create a FacetGrid with scatter plots\n",
    "#wrap the facetgrid so that the rows and columns are equal, as much as is possible\n",
    "#allow the axes for each facet to vary freely to best display data in each facet\n",
    "#be sure to allow enough room for the title\n",
    "g = sns.FacetGrid(df_melted, col='columns', hue='group', col_wrap=3, height=3, aspect=1, sharex=False, sharey=False,margin_titles=True)\n",
    "#do the scatterplot; include a trendline for each group\n",
    "g.map(sns.regplot,  'value','outcome', ci=None, scatter_kws={'alpha':0.5}, line_kws={'alpha':0.5})\n",
    "# Add a title for the whole plot\n",
    "g.fig.suptitle('Outcome vs. each predictor variable, by group')\n",
    "\n",
    "# Add a legend\n",
    "g.add_legend()\n",
    "\n",
    "plt.subplots_adjust(top=0.9)\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion so far\n",
    "\n",
    "The feature selection applied here hasn't helped very much. That surprises me because in test_limited_predictors, I got clear evidence that cutting down irrelevant predictors improved model performance.\n",
    "\n",
    "One reason might be that we've actually cut down on useful predictors--unlike in `test_limited_predictors.ipynb`, we can't cheat by removing predictors we know to be irrelevant. That means we're left with less information in the model itself.\n",
    "\n",
    "We've only really tried SelectKBest(); there might be other feature selection mechanisms that could do the job. But I don't know yet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataanalysis3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "014247d405695287815678bf9349a8dffb2674e9fe9a5bd4bb9820af018d638d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
