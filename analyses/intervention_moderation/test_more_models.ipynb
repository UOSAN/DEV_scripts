{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from socket import gethostname\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from dev_interaction_util import generate_synthetic_dev_outcomes, generate_synthetic_dev_data, set_up_interactions\n",
    "from dev_interaction_util import do_scoring_loop, get_best_model, summarize_overall_df_results, do_final_fit, present_model_results, present_results_vs_ground_truth_cors\n",
    "from ml_util import *\n",
    "# Imputing with MICE\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn import linear_model\n",
    "from ml_util import get_data_for_imputation\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.base import clone\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benjamins-MacBook-Pro-2.local\n",
      "{'dropbox_data_dir': '/Users/benjaminsmith/Dropbox (University of Oregon)/UO-SAN Lab/Berkman Lab/Devaluation/analysis_files/data/'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(gethostname())\n",
    "# Open the file and load the file\n",
    "with open('config.yml') as f:\n",
    "    all_yaml = yaml.load(f, Loader=SafeLoader)\n",
    "    if gethostname() in all_yaml.keys():\n",
    "        config = all_yaml[gethostname()]\n",
    "    else:\n",
    "        config = all_yaml['default']\n",
    "        \n",
    "print(config)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is derived from pre_registered_preview.ipynb.\n",
    "\n",
    "The aim is to look at how the model pipeline does with different sets of ground truths. If we plug in five actual effects, or ten, or twenty, how many are actually identified and how many irrelevant effects are identified?\n",
    "\n",
    "This can't be too black and white, because of course in real life, teh features are correlated iwth one another. But at least, the features we select to be correlated should _actually be_ the most correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropbox_data_dir = config['dropbox_data_dir']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is a pre-registered analysis for measuring moderations of the intervention.\n",
    "\n",
    "We'll cross-validate the intervention moderations.\n",
    "\n",
    "For this analysis, we'll try to make predictions based on some synthetic data. we'll take wave 1 data and randomly mix in changes based on our predictors, then try to model how we would predict those things. Finally, we'll make the predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_ppt_path = dropbox_data_dir + '/data_by_ppt.csv'\n",
    "data_codebook_path = dropbox_data_dir + 'data_codebook.csv'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_ppt = pd.read_csv(data_by_ppt_path)\n",
    "data_codebook = pd.read_csv(data_codebook_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out which columns in data_by_ppt are missing from the codebook\n",
    "data_by_ppt.columns.difference(data_codebook['VarName'])\n",
    "\n",
    "\n",
    "#copy our outcome measures, bf_1 and FFQ_1, into a new dataframe\n",
    "data_by_ppt['bf_2'] = data_by_ppt.bf_1\n",
    "#need to decide what sort of FFQ we want to use\n",
    "data_by_ppt['cancer_promoting_minus_preventing_FFQ_1'] = data_by_ppt.cancer_promoting_minus_preventing_FFQ\n",
    "data_by_ppt['cancer_promoting_minus_preventing_FFQ_2'] = data_by_ppt.cancer_promoting_minus_preventing_FFQ\n",
    "\n",
    "# do a report on missing data\n",
    "analysis_data  = data_by_ppt.loc[:,data_codebook.loc[data_codebook.IsSelectedPredictor,\"VarName\"]].copy()\n",
    "outcome_measures = data_by_ppt.loc[:,data_codebook.loc[data_codebook.IsSelectedOutcomeMeasure,\"VarName\"]].copy()\n",
    "\n",
    "na_values = pd.DataFrame(data_by_ppt.isna().sum())\n",
    "na_values.columns = ['NA_Count']\n",
    "na_values['prop_NA'] = na_values.NA_Count / data_by_ppt.shape[0]\n",
    "data_codebook = data_codebook.merge(na_values, left_on='VarName', right_index=True)\n",
    "\n",
    "data_codebook.to_csv(dropbox_data_dir + 'data_metadata.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to count the number of valid and missing entries in each of our data predictors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting data to numeric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_vals = pd.get_dummies(analysis_data.birthsex_factor)\n",
    "#there's only two variables here so we can convert this into a dummy variable\n",
    "analysis_data.drop(columns=['birthsex_factor'], inplace=True)\n",
    "one_hot_vals.columns = ['birthsex_factor_' + str(col) for col in one_hot_vals.columns]\n",
    "analysis_data = analysis_data.join(one_hot_vals.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BSCS</th>\n",
       "      <th>EDM</th>\n",
       "      <th>BIS_11</th>\n",
       "      <th>PCS</th>\n",
       "      <th>RS</th>\n",
       "      <th>TRSQ</th>\n",
       "      <th>ACES_neglectful_parenting</th>\n",
       "      <th>ACES_abuse</th>\n",
       "      <th>ACES_sum</th>\n",
       "      <th>ACES_divorced_separated</th>\n",
       "      <th>...</th>\n",
       "      <th>zipcode_median_income_acs</th>\n",
       "      <th>household_income_per_person</th>\n",
       "      <th>SST_prop_successful_stops</th>\n",
       "      <th>SST_GRTmean</th>\n",
       "      <th>SST_SSD</th>\n",
       "      <th>SST_PostErrorSlowW1_mean</th>\n",
       "      <th>SST_mean_ssrt_0</th>\n",
       "      <th>ROC_Crave_Regulate_Minus_Look</th>\n",
       "      <th>WTP_unhealthy_minus_healthy</th>\n",
       "      <th>birthsex_factor_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.538462</td>\n",
       "      <td>3.250</td>\n",
       "      <td>72</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.5125</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.384615</td>\n",
       "      <td>1.750</td>\n",
       "      <td>89</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.440524</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.384615</td>\n",
       "      <td>2.500</td>\n",
       "      <td>63</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>533.315052</td>\n",
       "      <td>284.375</td>\n",
       "      <td>0.058297</td>\n",
       "      <td>0.247061</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>-0.190476</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.076923</td>\n",
       "      <td>2.800</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>498.167248</td>\n",
       "      <td>103.125</td>\n",
       "      <td>0.027730</td>\n",
       "      <td>0.446583</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>0.170363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.307692</td>\n",
       "      <td>2.750</td>\n",
       "      <td>64</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>626.507764</td>\n",
       "      <td>250.000</td>\n",
       "      <td>0.105660</td>\n",
       "      <td>0.369308</td>\n",
       "      <td>-1.5500</td>\n",
       "      <td>-0.494624</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>3.461538</td>\n",
       "      <td>4.000</td>\n",
       "      <td>58</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.690347</td>\n",
       "      <td>1.768485</td>\n",
       "      <td>0.523438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.357362</td>\n",
       "      <td>-0.0125</td>\n",
       "      <td>-1.008152</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>3.692308</td>\n",
       "      <td>3.875</td>\n",
       "      <td>54</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.511475</td>\n",
       "      <td>-0.234851</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.335849</td>\n",
       "      <td>-0.1500</td>\n",
       "      <td>-1.889247</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>3.461538</td>\n",
       "      <td>3.125</td>\n",
       "      <td>69</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.335248</td>\n",
       "      <td>0.099038</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.273736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>2.846154</td>\n",
       "      <td>3.000</td>\n",
       "      <td>62</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.855379</td>\n",
       "      <td>-0.234851</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.401098</td>\n",
       "      <td>-0.9875</td>\n",
       "      <td>-0.151210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>3.230769</td>\n",
       "      <td>2.500</td>\n",
       "      <td>52</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.834004</td>\n",
       "      <td>1.768485</td>\n",
       "      <td>0.476562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.481932</td>\n",
       "      <td>-0.5500</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         BSCS    EDM  BIS_11   PCS    RS  TRSQ  ACES_neglectful_parenting  \\\n",
       "0    2.538462  3.250      72   7.0  20.0  63.0                        NaN   \n",
       "1    2.384615  1.750      89   9.0  22.0  63.0                        NaN   \n",
       "2    3.384615  2.500      63   9.0  18.0  57.0                        NaN   \n",
       "3    3.076923  2.800      75   NaN   NaN  64.0                        NaN   \n",
       "4    3.307692  2.750      64  12.0  21.0  55.0                        NaN   \n",
       "..        ...    ...     ...   ...   ...   ...                        ...   \n",
       "270  3.461538  4.000      58  18.0  17.0  54.0                        0.0   \n",
       "271  3.692308  3.875      54  17.0  13.0  55.0                        2.0   \n",
       "272  3.461538  3.125      69  11.0  13.0  53.0                        1.0   \n",
       "273  2.846154  3.000      62  15.0  22.0  84.0                        0.0   \n",
       "274  3.230769  2.500      52   9.0  15.0  59.0                        0.0   \n",
       "\n",
       "     ACES_abuse  ACES_sum  ACES_divorced_separated  ...  \\\n",
       "0           NaN       NaN                      NaN  ...   \n",
       "1           NaN       NaN                      NaN  ...   \n",
       "2           NaN       NaN                      NaN  ...   \n",
       "3           NaN       NaN                      NaN  ...   \n",
       "4           NaN       NaN                      NaN  ...   \n",
       "..          ...       ...                      ...  ...   \n",
       "270         1.0       3.0                      1.0  ...   \n",
       "271         2.0       5.0                      0.0  ...   \n",
       "272         1.0       6.0                      1.0  ...   \n",
       "273         1.0       4.0                      1.0  ...   \n",
       "274         0.0       3.0                      1.0  ...   \n",
       "\n",
       "     zipcode_median_income_acs  household_income_per_person  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          NaN                          NaN   \n",
       "4                          NaN                          NaN   \n",
       "..                         ...                          ...   \n",
       "270                  -0.690347                     1.768485   \n",
       "271                  -0.511475                    -0.234851   \n",
       "272                   1.335248                     0.099038   \n",
       "273                   0.855379                    -0.234851   \n",
       "274                   0.834004                     1.768485   \n",
       "\n",
       "     SST_prop_successful_stops  SST_GRTmean  SST_SSD  \\\n",
       "0                          NaN          NaN      NaN   \n",
       "1                          NaN          NaN      NaN   \n",
       "2                     0.500000   533.315052  284.375   \n",
       "3                     0.312500   498.167248  103.125   \n",
       "4                     0.562500   626.507764  250.000   \n",
       "..                         ...          ...      ...   \n",
       "270                   0.523438          NaN      NaN   \n",
       "271                   0.492188          NaN      NaN   \n",
       "272                   0.507812          NaN      NaN   \n",
       "273                   0.479167          NaN      NaN   \n",
       "274                   0.476562          NaN      NaN   \n",
       "\n",
       "     SST_PostErrorSlowW1_mean  SST_mean_ssrt_0  ROC_Crave_Regulate_Minus_Look  \\\n",
       "0                         NaN              NaN                        -0.5125   \n",
       "1                         NaN              NaN                            NaN   \n",
       "2                    0.058297         0.247061                        -0.8000   \n",
       "3                    0.027730         0.446583                        -0.8000   \n",
       "4                    0.105660         0.369308                        -1.5500   \n",
       "..                        ...              ...                            ...   \n",
       "270                       NaN         0.357362                        -0.0125   \n",
       "271                       NaN         0.335849                        -0.1500   \n",
       "272                       NaN         0.273736                            NaN   \n",
       "273                       NaN         0.401098                        -0.9875   \n",
       "274                       NaN         0.481932                        -0.5500   \n",
       "\n",
       "     WTP_unhealthy_minus_healthy  birthsex_factor_Male  \n",
       "0                      -0.312500                     1  \n",
       "1                       0.440524                     0  \n",
       "2                      -0.190476                     0  \n",
       "3                       0.170363                     0  \n",
       "4                      -0.494624                     0  \n",
       "..                           ...                   ...  \n",
       "270                    -1.008152                     1  \n",
       "271                    -1.889247                     1  \n",
       "272                     0.516129                     1  \n",
       "273                    -0.151210                     0  \n",
       "274                     0.343750                     1  \n",
       "\n",
       "[275 rows x 76 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data \n",
    "\n",
    "Apply missing data imputation to columns including cSES, ACES_sum, ses_aggregate, zipcode_median_income_acs, IMI, mcarthur social standing, based on demographic and self-report predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(ml_util)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this experiment, I'm going for Ridge regression with 10 nearest features. The values it imputes are a compromise between simply using the nearest mean, which is conservative when using these values for prediction because it doesn't introduce erroneous variance, but isn't very informative, and then using all available information, which Ridge regression with an unlimited number of features would do. It's a tough choice between this and KNN, which doesn't assume normality. Overall I'm going with KNN, because it picks up on relationships between the two variables while not generating extreme values like KNN seems to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/impute/_iterative.py:699: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "imputer = IterativeImputer(estimator=linear_model.Ridge(),n_nearest_features=10,max_iter=100,random_state=0)\n",
    "analysis_data_imputed = get_data_for_imputation(analysis_data)\n",
    "\n",
    "#this dataset is already filtered for columns so we don't need to filter those further.\n",
    "analysis_data_imputed = pd.DataFrame(imputer.fit_transform(analysis_data_imputed), columns=analysis_data_imputed.columns)\n",
    "imputed_datapoint = analysis_data.isna()\n",
    "# do_aces_cses_imputation_diagnostic(analysis_data_imputed, imputed_datapoint,'ridge_10')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 1 - base with Lasso and Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_hyperparameter_selection_on_fold_grid_and_lasso(X, y,cv,alpha_range):\n",
    "    if alpha_range is None:\n",
    "        alpha_10pow_lower = 6\n",
    "        alpha_10pow_upper = -1\n",
    "        alpha_increments=1\n",
    "        alpha_range = np.power(10,np.linspace(-alpha_10pow_lower,alpha_10pow_upper,(alpha_10pow_lower+alpha_10pow_upper)*alpha_increments+1))\n",
    "\n",
    "    ############\n",
    "    #RIDGE\n",
    "    ridge_parameters = {'alpha':alpha_range}\n",
    "    ridge_model = linear_model.Ridge()\n",
    "    print(ridge_parameters)\n",
    "    #do a gridsearch, using the same folds as the outer loop\n",
    "    ridge_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(ridge_model), param_grid = get_param_grid_with_preprocessing(ridge_parameters), cv=cv,scoring='neg_mean_absolute_error')\n",
    "    ridge_grid_search_cv.fit(X,y)\n",
    "\n",
    "    ############\n",
    "    #LASSO\n",
    "    lasso_parameters = {'alpha':alpha_range}\n",
    "    lasso_model = linear_model.Lasso()\n",
    "    print(lasso_parameters)\n",
    "    lasso_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(lasso_model), param_grid = get_param_grid_with_preprocessing(lasso_parameters), cv=cv,scoring='neg_mean_absolute_error')\n",
    "    lasso_grid_search_cv.fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "    all_cv_results = [ridge_grid_search_cv, lasso_grid_search_cv]\n",
    "\n",
    "    #create a dataframe with the best parameters, best mean_test_score, and name of the model\n",
    "\n",
    "    best_params_df = pd.DataFrame({\n",
    "        'model': [cv_result.estimator for cv_result in all_cv_results],\n",
    "        'model_name': [cv_result.estimator.__class__.__name__ for cv_result in all_cv_results],\n",
    "        'best_params': [extract_estimator_params_from_gridsearch(cv_result.best_params_) for cv_result in all_cv_results],\n",
    "        'best_score': [cv_result.best_score_ for cv_result in all_cv_results],\n",
    "        'best_raw_params' : [cv_result.best_params_ for cv_result in all_cv_results]\n",
    "        })\n",
    "    \n",
    "    best_params_df = best_params_df.sort_values('best_score',ascending=False).reset_index(drop=True)\n",
    "\n",
    "    best_model = clone(best_params_df['model'][0])\n",
    "    best_model_params = best_params_df['best_raw_params'][0]\n",
    "    best_model.set_params(**best_model_params)\n",
    "\n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_params_df':best_params_df,\n",
    "        'raw_cv_results':all_cv_results\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_hyperparameter_selection_on_fold_grid_and_lasso_a2(X,y,cv):\n",
    "    alpha_10pow_lower = 2\n",
    "    alpha_10pow_upper = 0\n",
    "    alpha_increments=1\n",
    "    alpha_range = np.concatenate([np.power(10,np.linspace(-alpha_10pow_lower,alpha_10pow_upper,(alpha_10pow_lower+alpha_10pow_upper)*alpha_increments+1)),\n",
    "        [0.2,0.4,0.6,0.8,1.0]])\n",
    "    return(do_hyperparameter_selection_on_fold_grid_and_lasso(X,y,cv,alpha_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ni' 'san']\n",
      "[1.28335298 0.42953651]\n",
      "['san' 'san' 'ni' 'ichi' 'san' 'san' 'ichi' 'san' 'san' 'san' 'ni' 'ichi'\n",
      " 'ichi' 'ichi' 'ichi' 'san' 'san' 'san' 'ichi' 'ichi' 'san' 'san' 'ni'\n",
      " 'ni' 'ni' 'ni' 'ni' 'ni' 'ni' 'san' 'ni' 'san' 'ni' 'ichi' 'ni' 'san'\n",
      " 'ni' 'ichi' 'san' 'ni' 'ni' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ni' 'ni'\n",
      " 'san' 'ichi' 'ichi' 'ni' 'san' 'ni' 'ichi' 'ni' 'ni' 'ni' 'ichi' 'san'\n",
      " 'ni' 'ni' 'ichi' 'ni' 'ichi' 'san' 'ni' 'ni' 'ni' 'san' 'ichi' 'ni' 'san'\n",
      " 'ichi' 'san' 'ni' 'san' 'ni' 'ichi' 'ichi' 'san' 'ichi' 'san' 'san' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'san' 'ni' 'san' 'ni' 'ichi' 'san' 'san' 'san' 'ichi'\n",
      " 'ni' 'san' 'ichi' 'ichi' 'san' 'ni' 'ichi' 'san' 'ni' 'ni' 'san' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'ichi' 'ni' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ichi'\n",
      " 'ni' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ichi' 'san' 'san' 'ni' 'ichi' 'ni'\n",
      " 'ichi' 'ichi' 'san' 'ichi' 'ni' 'san' 'san' 'ni' 'ni' 'san' 'san' 'san'\n",
      " 'ichi' 'san' 'ni' 'san' 'ichi' 'ichi' 'ichi' 'ni' 'san' 'ni' 'ni' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'san' 'ni' 'san' 'ichi' 'ni' 'ni' 'ni' 'ichi' 'ni'\n",
      " 'ichi' 'san' 'san' 'san' 'san' 'ichi' 'ni' 'san' 'san' 'san' 'ichi' 'san'\n",
      " 'ni' 'ni' 'san' 'ichi' 'ichi' 'san' 'ni' 'ni' 'san' 'ni' 'san' 'san' 'ni'\n",
      " 'san' 'ni' 'ni' 'san' 'ichi' 'san' 'ichi' 'san' 'ni' 'ni' 'ni' 'ichi'\n",
      " 'ni' 'ni' 'san' 'ni' 'ni' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ni' 'ni' 'san'\n",
      " 'ichi' 'ni' 'ichi' 'ichi' 'ichi' 'ichi' 'ichi' 'ichi' 'san' 'ichi' 'san'\n",
      " 'ichi' 'ni' 'san' 'san' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ni' 'san' 'san'\n",
      " 'ichi' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ni' 'ni' 'ni' 'san' 'ichi'\n",
      " 'ichi' 'ichi' 'ni' 'ichi' 'ni' 'san' 'ichi' 'san' 'san' 'ni' 'san' 'san'\n",
      " 'san' 'san' 'ichi' 'ichi' 'ichi' 'ni' 'ni' 'ichi' 'san' 'san' 'san']\n",
      "ni\n",
      "                       feature_name  interaction_effect\n",
      "0                              BSCS                0.15\n",
      "3                               PCS               -0.15\n",
      "1                               EDM                0.15\n",
      "2                            BIS_11               -0.15\n",
      "74      WTP_unhealthy_minus_healthy                0.00\n",
      "55     TESQ_E_goal_and_rule_setting                0.00\n",
      "54               TESQ_E_distraction                0.00\n",
      "53  TESQ_E_avoidance_of_temptations                0.00\n",
      "52   TESQ_E_controlling_temptations                0.00\n",
      "51                         SRHI_sum                0.00\n",
      "50                   SRHI_unhealthy                0.00\n",
      "49                     SRHI_healthy                0.00\n",
      "48                    RTFS_factor_2                0.00\n",
      "47                    RTFS_factor_1                0.00\n",
      "46                   RMQ_locomotion                0.00\n",
      "45                   RMQ_assessment                0.00\n",
      "44                          RMQ_lie                0.00\n",
      "43        PLAN_temporal_orientation                0.00\n",
      "42          PLAN_mental_forecasting                0.00\n",
      "41        PLAN_cognitive_strategies                0.00\n",
      "bf_2\n",
      "cancer_promoting_minus_preventing_FFQ_w2\n",
      "FFQ_v2_Mean_Energy_w2\n",
      "san\n",
      "                       feature_name  interaction_effect\n",
      "4                                RS                0.15\n",
      "5                              TRSQ                0.15\n",
      "6         ACES_neglectful_parenting               -0.15\n",
      "7                        ACES_abuse               -0.15\n",
      "0                              BSCS                0.00\n",
      "49                     SRHI_healthy                0.00\n",
      "56         TESQ_E_goal_deliberation                0.00\n",
      "55     TESQ_E_goal_and_rule_setting                0.00\n",
      "54               TESQ_E_distraction                0.00\n",
      "53  TESQ_E_avoidance_of_temptations                0.00\n",
      "52   TESQ_E_controlling_temptations                0.00\n",
      "51                         SRHI_sum                0.00\n",
      "50                   SRHI_unhealthy                0.00\n",
      "47                    RTFS_factor_1                0.00\n",
      "48                    RTFS_factor_2                0.00\n",
      "58               TESQ_E_suppression                0.00\n",
      "46                   RMQ_locomotion                0.00\n",
      "45                   RMQ_assessment                0.00\n",
      "44                          RMQ_lie                0.00\n",
      "43        PLAN_temporal_orientation                0.00\n",
      "bf_2\n",
      "cancer_promoting_minus_preventing_FFQ_w2\n",
      "FFQ_v2_Mean_Energy_w2\n",
      "(275, 76)\n",
      "(275, 76)\n",
      "outer split0\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.920e+02, tolerance: 4.427e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.197e+02, tolerance: 4.541e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.120e+02, tolerance: 4.505e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.134e+02, tolerance: 4.726e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+02, tolerance: 4.691e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.994e+02, tolerance: 4.514e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.516e+02, tolerance: 4.643e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.814e+02, tolerance: 4.504e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.341e+02, tolerance: 4.752e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer split1\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.683e+02, tolerance: 4.729e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.295e+02, tolerance: 4.613e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.751e+02, tolerance: 4.771e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.439e+02, tolerance: 4.652e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.170e+02, tolerance: 4.524e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.000e+02, tolerance: 4.617e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.809e+02, tolerance: 4.868e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.447e+02, tolerance: 4.802e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.210e+02, tolerance: 4.861e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer split2\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.536e+02, tolerance: 4.398e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.425e+02, tolerance: 4.412e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.118e+02, tolerance: 4.668e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.824e+02, tolerance: 4.301e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.522e+02, tolerance: 4.388e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.669e+02, tolerance: 4.273e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.734e+02, tolerance: 4.530e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.318e+02, tolerance: 4.469e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.451e+02, tolerance: 4.335e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer split3\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.376e+02, tolerance: 4.328e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.425e+02, tolerance: 4.408e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.262e+02, tolerance: 4.606e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.855e+02, tolerance: 4.477e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+02, tolerance: 4.389e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.547e+02, tolerance: 4.312e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.183e+02, tolerance: 4.516e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.840e+02, tolerance: 4.280e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.631e+02, tolerance: 4.373e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer split4\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.010e+02, tolerance: 4.516e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.919e+02, tolerance: 4.471e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.205e+02, tolerance: 4.466e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.207e+02, tolerance: 4.735e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.022e+02, tolerance: 4.492e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.608e+02, tolerance: 4.685e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.830e+02, tolerance: 4.283e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.820e+02, tolerance: 4.182e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.655e+02, tolerance: 4.524e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer split5\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.899e+02, tolerance: 4.688e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.440e+02, tolerance: 4.757e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.434e+02, tolerance: 5.001e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.700e+02, tolerance: 4.937e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.188e+02, tolerance: 4.932e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.590e+02, tolerance: 4.854e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.857e+02, tolerance: 4.610e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.152e+02, tolerance: 4.662e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.427e+02, tolerance: 4.622e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer split6\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.174e+02, tolerance: 4.704e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.866e+02, tolerance: 4.403e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.615e+02, tolerance: 4.669e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.452e+02, tolerance: 4.982e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.714e+02, tolerance: 4.623e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.910e+02, tolerance: 4.875e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.092e+02, tolerance: 4.566e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.153e+02, tolerance: 4.467e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.088e+02, tolerance: 4.516e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer split7\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.260e+02, tolerance: 4.692e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.007e+02, tolerance: 4.534e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.294e+02, tolerance: 4.816e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.839e+02, tolerance: 4.798e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.081e+02, tolerance: 4.502e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.464e+02, tolerance: 4.513e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.305e+02, tolerance: 4.689e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.793e+02, tolerance: 4.440e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+02, tolerance: 4.761e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer split8\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.048e+02, tolerance: 4.827e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.870e+02, tolerance: 4.682e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.626e+02, tolerance: 4.805e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.744e+02, tolerance: 4.677e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.694e+02, tolerance: 4.726e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.004e+02, tolerance: 4.533e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.613e+02, tolerance: 4.513e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.687e+02, tolerance: 4.506e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.570e+02, tolerance: 4.577e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer split9\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.430e+02, tolerance: 4.553e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.211e+02, tolerance: 4.729e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.675e+02, tolerance: 4.481e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.757e+02, tolerance: 4.722e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.964e+02, tolerance: 4.520e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.337e+02, tolerance: 4.242e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.393e+02, tolerance: 4.866e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.261e+02, tolerance: 4.414e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.061e+02, tolerance: 4.371e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:\n",
      "[0.1274999616527538, -0.06874259686177808, -0.12140026760493905, 0.1346752210401354, 0.14266708486256807, -0.34680626449621577, 0.1639186985513973, -0.011859369652341467, -0.32995614971410037, 0.07945506380367717]\n",
      "overall_score:\n",
      "-0.0230548618418843\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean_test_score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">std_test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_description</th>\n",
       "      <th>params_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.2}</th>\n",
       "      <td>-3.570679</td>\n",
       "      <td>0.048386</td>\n",
       "      <td>0.473254</td>\n",
       "      <td>0.101980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4}</th>\n",
       "      <td>-3.603683</td>\n",
       "      <td>0.032778</td>\n",
       "      <td>0.457453</td>\n",
       "      <td>0.149324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6}</th>\n",
       "      <td>-3.656658</td>\n",
       "      <td>0.045859</td>\n",
       "      <td>0.461504</td>\n",
       "      <td>0.156509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1}</th>\n",
       "      <td>-3.690697</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.481553</td>\n",
       "      <td>0.118551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8}</th>\n",
       "      <td>-3.694966</td>\n",
       "      <td>0.043170</td>\n",
       "      <td>0.467687</td>\n",
       "      <td>0.150113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 1.0}</th>\n",
       "      <td>-3.719681</td>\n",
       "      <td>0.047648</td>\n",
       "      <td>0.474915</td>\n",
       "      <td>0.134263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.01}</th>\n",
       "      <td>-5.258356</td>\n",
       "      <td>0.147121</td>\n",
       "      <td>0.716611</td>\n",
       "      <td>0.167308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">dict_values([StandardScaler(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 1.0}</th>\n",
       "      <td>-5.674324</td>\n",
       "      <td>0.203889</td>\n",
       "      <td>0.747942</td>\n",
       "      <td>0.184292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8}</th>\n",
       "      <td>-5.855230</td>\n",
       "      <td>0.227252</td>\n",
       "      <td>0.765743</td>\n",
       "      <td>0.188165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6}</th>\n",
       "      <td>-6.096521</td>\n",
       "      <td>0.252987</td>\n",
       "      <td>0.793524</td>\n",
       "      <td>0.186907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4}</th>\n",
       "      <td>-6.458223</td>\n",
       "      <td>0.301549</td>\n",
       "      <td>0.828741</td>\n",
       "      <td>0.182128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.2}</th>\n",
       "      <td>-7.140921</td>\n",
       "      <td>0.380793</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.157523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1}</th>\n",
       "      <td>-7.915941</td>\n",
       "      <td>0.429971</td>\n",
       "      <td>1.030900</td>\n",
       "      <td>0.142350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.01}</th>\n",
       "      <td>-11.077672</td>\n",
       "      <td>0.804085</td>\n",
       "      <td>1.628760</td>\n",
       "      <td>0.409948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing permutation test on importance; this may take time.\n",
      "Number of selected features: 37\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictor</th>\n",
       "      <th>coef</th>\n",
       "      <th>feature_importance</th>\n",
       "      <th>fa_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>ACES_neglectful_parenting*san</td>\n",
       "      <td>-0.695990</td>\n",
       "      <td>0.054959</td>\n",
       "      <td>0.054959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>RS*san</td>\n",
       "      <td>0.701057</td>\n",
       "      <td>0.053638</td>\n",
       "      <td>0.053638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>EDM*ni</td>\n",
       "      <td>0.700994</td>\n",
       "      <td>0.051069</td>\n",
       "      <td>0.051069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PCS</td>\n",
       "      <td>-0.549016</td>\n",
       "      <td>0.043063</td>\n",
       "      <td>0.043063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>ACES_abuse*san</td>\n",
       "      <td>-0.536262</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>0.033398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>TESQ_E_controlling_temptations*san</td>\n",
       "      <td>0.515337</td>\n",
       "      <td>0.031561</td>\n",
       "      <td>0.031561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>RTFS_f1_minus_f2</td>\n",
       "      <td>-0.435506</td>\n",
       "      <td>0.026611</td>\n",
       "      <td>0.026611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>household_income_per_person*ni</td>\n",
       "      <td>0.336127</td>\n",
       "      <td>0.019956</td>\n",
       "      <td>0.019956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>RMQ_locomotion</td>\n",
       "      <td>0.337818</td>\n",
       "      <td>0.016616</td>\n",
       "      <td>0.016616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NCS_like_responsibility</td>\n",
       "      <td>-0.285976</td>\n",
       "      <td>0.013954</td>\n",
       "      <td>0.013954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BIS_11</td>\n",
       "      <td>-0.227643</td>\n",
       "      <td>0.010262</td>\n",
       "      <td>0.010262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>TESQ_E_goal_and_rule_setting</td>\n",
       "      <td>0.240673</td>\n",
       "      <td>0.009606</td>\n",
       "      <td>0.009606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>cancer_promoting_minus_preventing_liked_FCI*san</td>\n",
       "      <td>-0.198827</td>\n",
       "      <td>0.009003</td>\n",
       "      <td>0.009003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BFI_agreeableness</td>\n",
       "      <td>-0.161926</td>\n",
       "      <td>0.006976</td>\n",
       "      <td>0.006976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NCS_think_minimally</td>\n",
       "      <td>-0.189456</td>\n",
       "      <td>0.006706</td>\n",
       "      <td>0.006706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>TESQ_E_avoidance_of_temptations</td>\n",
       "      <td>-0.166799</td>\n",
       "      <td>0.005972</td>\n",
       "      <td>0.005972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>BSCS*ni</td>\n",
       "      <td>0.208324</td>\n",
       "      <td>0.005304</td>\n",
       "      <td>0.005304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>PLAN_temporal_orientation</td>\n",
       "      <td>0.177573</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.005222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>RTFS_factor_1</td>\n",
       "      <td>-0.138983</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.004545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NCS_thinking_not_exciting</td>\n",
       "      <td>-0.126157</td>\n",
       "      <td>0.004299</td>\n",
       "      <td>0.004299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/Google Drive/oregon/code/DEV_scripts/analyses/intervention_moderation/dev_interaction_util.py:326: FutureWarning: merging between different levels is deprecated and will be removed in a future version. (2 levels on the left, 1 on the right)\n",
      "  results_vs_cors = final_results_wide.merge(group_correlations, left_index=True, right_index=True, how='outer')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(coef, base)</th>\n",
       "      <th>(coef, ni)</th>\n",
       "      <th>(coef, san)</th>\n",
       "      <th>(feature_importance, base)</th>\n",
       "      <th>(feature_importance, ni)</th>\n",
       "      <th>(feature_importance, san)</th>\n",
       "      <th>ichi_cor</th>\n",
       "      <th>ni_cor</th>\n",
       "      <th>san_cor</th>\n",
       "      <th>abs_effect_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACES_neglectful_parenting</th>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.696</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RS</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDM</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.701</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCS</th>\n",
       "      <td>-0.549</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACES_abuse</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TESQ_E_controlling_temptations</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RTFS_f1_minus_f2</th>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>household_income_per_person</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMQ_locomotion</th>\n",
       "      <td>0.338</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_like_responsibility</th>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIS_11</th>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TESQ_E_goal_and_rule_setting</th>\n",
       "      <td>0.241</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancer_promoting_minus_preventing_liked_FCI</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BFI_agreeableness</th>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_think_minimally</th>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TESQ_E_avoidance_of_temptations</th>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BSCS</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAN_temporal_orientation</th>\n",
       "      <td>0.178</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RTFS_factor_1</th>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_thinking_not_exciting</th>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_own</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SST_mean_ssrt_0</th>\n",
       "      <td>0.134</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_intellectual_task</th>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_deliberating_issues</th>\n",
       "      <td>0.111</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SST_PostErrorSlowW1_mean</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WTP_unhealthy_minus_healthy</th>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#set np random seed\n",
    "np.random.seed(3161527)\n",
    "\n",
    "group_names = ['ichi','ni','san']\n",
    "#assign each row randomly to a group\n",
    "group_assignments = np.random.choice(group_names,analysis_data_imputed.shape[0])\n",
    "\n",
    "#synthetic outcomes\n",
    "outcome_measures = generate_synthetic_dev_outcomes(outcome_measures)\n",
    "\n",
    "# add synthetic primary and interaction effects\n",
    "\n",
    "\n",
    "#set up the interaction effects\n",
    "custom_interaction_effects_g1 = [0]*analysis_data_imputed.shape[1]\n",
    "custom_interaction_effects_g1[0] = 0.15\n",
    "custom_interaction_effects_g1[1] = 0.15\n",
    "custom_interaction_effects_g1[2] = -0.15\n",
    "custom_interaction_effects_g1[3] = -0.15\n",
    "\n",
    "custom_interaction_effects_g2 = [0]*analysis_data_imputed.shape[1]\n",
    "custom_interaction_effects_g2[4] = 0.15\n",
    "custom_interaction_effects_g2[5] = 0.15\n",
    "custom_interaction_effects_g2[6] = -0.15\n",
    "custom_interaction_effects_g2[7] = -0.15\n",
    "\n",
    "custom_interaction_effects = {'ni':custom_interaction_effects_g1,'san':custom_interaction_effects_g2}\n",
    "\n",
    "\n",
    "\n",
    "synthetic_data = generate_synthetic_dev_data(analysis_data_imputed, group_assignments,outcome_measures, group_interaction_effects = custom_interaction_effects)\n",
    "interaction_effect_df = synthetic_data['X_weights']\n",
    "outcome_measures = synthetic_data['y']\n",
    "\n",
    "# Set up outcome measures and group assignment one-hot\n",
    "\n",
    "outcome_measures = calculate_outcome_changes(outcome_measures)\n",
    "group_assignment_onehots = pd.get_dummies(group_assignments).loc[:,['ni','san']]\n",
    "\n",
    "predictor_data = set_up_interactions(analysis_data_imputed, group_assignment_onehots)\n",
    "\n",
    "\n",
    "#remove any NA values for this outcome measure in both the predictor data and the outcome data\n",
    "outcome_nas = outcome_measures['d_bf'].isna()\n",
    "\n",
    "outcome_measures_nona = outcome_measures.loc[~outcome_nas,:]\n",
    "predictor_data_nona = predictor_data.loc[~outcome_nas,:]\n",
    "group_assignment_onehots_nonan = group_assignment_onehots.loc[~outcome_nas,:]\n",
    "group_assignments_nona = group_assignments[~outcome_nas]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Try out CV with simple gridsearch\n",
    "\n",
    "\n",
    "scoring_data = do_scoring_loop(X=predictor_data_nona, y= outcome_measures_nona['d_bf'], \n",
    "                groups = group_assignments_nona, \n",
    "                hyperparameter_selection_on_fold=do_hyperparameter_selection_on_fold_grid_and_lasso_a2,\n",
    "                outer_folds=10)\n",
    "\n",
    "scores = scoring_data['scores']\n",
    "best_models = scoring_data['best_models']\n",
    "best_params_df_list = scoring_data['best_params_df_list']\n",
    "raw_cv_results_list = scoring_data['raw_cv_results_list']\n",
    "\n",
    "print(\"scores:\")\n",
    "print(scores)\n",
    "overall_score = np.mean(scores)\n",
    "print(\"overall_score:\")\n",
    "print(overall_score)\n",
    "\n",
    "\n",
    "\n",
    "best_model = get_best_model(summarize_overall_df_results(raw_cv_results_list))\n",
    "final_fit = do_final_fit(X=predictor_data_nona, y= outcome_measures_nona['d_bf'], final_model=best_model)\n",
    "final_results = present_model_results(X=predictor_data_nona, final_fit=final_fit, y=outcome_measures_nona['d_bf'])\n",
    "\n",
    "#print rows of final_results where feature_name is the list of features to check\n",
    "base_regressors = interaction_effect_df.predictor[interaction_effect_df.interaction_effect!=0]\n",
    "regressors_to_check = [x+y for y in ['','*ni','*san'] for x in base_regressors]\n",
    "final_results['planned_regression'] = final_results['predictor'].isin(regressors_to_check)\n",
    "\n",
    "present_results_vs_ground_truth_cors(predictor_data_nona,outcome_measures_nona,group_assignments_nona,final_results,base_regressors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 2: Add KNN, Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def do_hyperparameter_selection_on_2(X, y,cv):\n",
    "    #alpha parameters for Ridge and Lasso\n",
    "    alpha_10pow_lower = 2\n",
    "    alpha_10pow_upper = 0\n",
    "    alpha_increments=1\n",
    "    alpha_range = np.concatenate([np.power(10,np.linspace(-alpha_10pow_lower,alpha_10pow_upper,(alpha_10pow_lower+alpha_10pow_upper)*alpha_increments+1)),\n",
    "        [0.2,0.4,0.6,0.8,1.0]])\n",
    "\n",
    "    ############\n",
    "    #RIDGE\n",
    "    ridge_parameters = {'alpha':alpha_range}\n",
    "    ridge_model = linear_model.Ridge()\n",
    "    print(ridge_parameters)\n",
    "    #do a gridsearch, using the same folds as the outer loop\n",
    "    ridge_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(ridge_model), param_grid = get_param_grid_with_preprocessing(ridge_parameters), cv=cv,scoring='neg_mean_absolute_error')\n",
    "    ridge_grid_search_cv.fit(X,y)\n",
    "\n",
    "    ############\n",
    "    #LASSO\n",
    "    lasso_parameters = {'alpha':alpha_range}\n",
    "    lasso_model = linear_model.Lasso()\n",
    "    print(lasso_parameters)\n",
    "    lasso_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(lasso_model), param_grid = get_param_grid_with_preprocessing(lasso_parameters), cv=cv,scoring='neg_mean_absolute_error')\n",
    "    lasso_grid_search_cv.fit(X,y)\n",
    "\n",
    "    #######\n",
    "    #KNN\n",
    "    knn_parameters = {'n_neighbors':np.unique(np.round(np.power(10,np.linspace(0,2,2*5+1)))).astype(int)}\n",
    "    knn_model = KNeighborsRegressor()\n",
    "    print(knn_parameters)\n",
    "    knn_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(knn_model), param_grid = get_param_grid_with_preprocessing(knn_parameters), cv=cv,scoring='neg_mean_absolute_error')\n",
    "    knn_grid_search_cv.fit(X,y)\n",
    "\n",
    "\n",
    "    ###########\n",
    "    #Decision tree regressor\n",
    "    dt_regressor_parameters = {\n",
    "        'max_depth':[2, 3,5,10],\n",
    "        'min_samples_split':[5,20,50],\n",
    "        'min_samples_leaf':[5,20,50]\n",
    "    }             \n",
    "    print(dt_regressor_parameters)\n",
    "    dt_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(DecisionTreeRegressor()), param_grid = get_param_grid_with_preprocessing(dt_regressor_parameters), cv=cv,scoring='neg_mean_absolute_error')\n",
    "    dt_grid_search_cv.fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "    #all_cv_results = [ridge_grid_search_cv, lasso_grid_search_cv, knn_grid_search_cv, dt_grid_search_cv, rf_grid_search_cv, gb_grid_search_cv]\n",
    "    all_cv_results = [ridge_grid_search_cv, lasso_grid_search_cv, knn_grid_search_cv, dt_grid_search_cv]\n",
    "    #all_cv_results = [ridge_grid_search_cv, lasso_grid_search_cv]\n",
    "\n",
    "    #create a dataframe with the best parameters, best mean_test_score, and name of the model\n",
    "\n",
    "    best_params_df = pd.DataFrame({\n",
    "        'model': [cv_result.estimator for cv_result in all_cv_results],\n",
    "        'model_name': [cv_result.estimator.__class__.__name__ for cv_result in all_cv_results],\n",
    "        'best_params': [extract_estimator_params_from_gridsearch(cv_result.best_params_) for cv_result in all_cv_results],\n",
    "        'best_score': [cv_result.best_score_ for cv_result in all_cv_results],\n",
    "        'best_raw_params' : [cv_result.best_params_ for cv_result in all_cv_results]\n",
    "        })\n",
    "    \n",
    "    best_params_df = best_params_df.sort_values('best_score',ascending=False).reset_index(drop=True)\n",
    "\n",
    "    best_model = clone(best_params_df['model'][0])\n",
    "    best_model_params = best_params_df['best_raw_params'][0]\n",
    "    best_model.set_params(**best_model_params)\n",
    "\n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_params_df':best_params_df,\n",
    "        'raw_cv_results':all_cv_results\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ni' 'san']\n",
      "[1.28335298 0.42953651]\n",
      "['san' 'san' 'ni' 'ichi' 'san' 'san' 'ichi' 'san' 'san' 'san' 'ni' 'ichi'\n",
      " 'ichi' 'ichi' 'ichi' 'san' 'san' 'san' 'ichi' 'ichi' 'san' 'san' 'ni'\n",
      " 'ni' 'ni' 'ni' 'ni' 'ni' 'ni' 'san' 'ni' 'san' 'ni' 'ichi' 'ni' 'san'\n",
      " 'ni' 'ichi' 'san' 'ni' 'ni' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ni' 'ni'\n",
      " 'san' 'ichi' 'ichi' 'ni' 'san' 'ni' 'ichi' 'ni' 'ni' 'ni' 'ichi' 'san'\n",
      " 'ni' 'ni' 'ichi' 'ni' 'ichi' 'san' 'ni' 'ni' 'ni' 'san' 'ichi' 'ni' 'san'\n",
      " 'ichi' 'san' 'ni' 'san' 'ni' 'ichi' 'ichi' 'san' 'ichi' 'san' 'san' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'san' 'ni' 'san' 'ni' 'ichi' 'san' 'san' 'san' 'ichi'\n",
      " 'ni' 'san' 'ichi' 'ichi' 'san' 'ni' 'ichi' 'san' 'ni' 'ni' 'san' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'ichi' 'ni' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ichi'\n",
      " 'ni' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ichi' 'san' 'san' 'ni' 'ichi' 'ni'\n",
      " 'ichi' 'ichi' 'san' 'ichi' 'ni' 'san' 'san' 'ni' 'ni' 'san' 'san' 'san'\n",
      " 'ichi' 'san' 'ni' 'san' 'ichi' 'ichi' 'ichi' 'ni' 'san' 'ni' 'ni' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'san' 'ni' 'san' 'ichi' 'ni' 'ni' 'ni' 'ichi' 'ni'\n",
      " 'ichi' 'san' 'san' 'san' 'san' 'ichi' 'ni' 'san' 'san' 'san' 'ichi' 'san'\n",
      " 'ni' 'ni' 'san' 'ichi' 'ichi' 'san' 'ni' 'ni' 'san' 'ni' 'san' 'san' 'ni'\n",
      " 'san' 'ni' 'ni' 'san' 'ichi' 'san' 'ichi' 'san' 'ni' 'ni' 'ni' 'ichi'\n",
      " 'ni' 'ni' 'san' 'ni' 'ni' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ni' 'ni' 'san'\n",
      " 'ichi' 'ni' 'ichi' 'ichi' 'ichi' 'ichi' 'ichi' 'ichi' 'san' 'ichi' 'san'\n",
      " 'ichi' 'ni' 'san' 'san' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ni' 'san' 'san'\n",
      " 'ichi' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ni' 'ni' 'ni' 'san' 'ichi'\n",
      " 'ichi' 'ichi' 'ni' 'ichi' 'ni' 'san' 'ichi' 'san' 'san' 'ni' 'san' 'san'\n",
      " 'san' 'san' 'ichi' 'ichi' 'ichi' 'ni' 'ni' 'ichi' 'san' 'san' 'san']\n",
      "ni\n",
      "                       feature_name  interaction_effect\n",
      "0                              BSCS                0.15\n",
      "3                               PCS               -0.15\n",
      "1                               EDM                0.15\n",
      "2                            BIS_11               -0.15\n",
      "74      WTP_unhealthy_minus_healthy                0.00\n",
      "55     TESQ_E_goal_and_rule_setting                0.00\n",
      "54               TESQ_E_distraction                0.00\n",
      "53  TESQ_E_avoidance_of_temptations                0.00\n",
      "52   TESQ_E_controlling_temptations                0.00\n",
      "51                         SRHI_sum                0.00\n",
      "50                   SRHI_unhealthy                0.00\n",
      "49                     SRHI_healthy                0.00\n",
      "48                    RTFS_factor_2                0.00\n",
      "47                    RTFS_factor_1                0.00\n",
      "46                   RMQ_locomotion                0.00\n",
      "45                   RMQ_assessment                0.00\n",
      "44                          RMQ_lie                0.00\n",
      "43        PLAN_temporal_orientation                0.00\n",
      "42          PLAN_mental_forecasting                0.00\n",
      "41        PLAN_cognitive_strategies                0.00\n",
      "bf_2\n",
      "cancer_promoting_minus_preventing_FFQ_w2\n",
      "FFQ_v2_Mean_Energy_w2\n",
      "san\n",
      "                       feature_name  interaction_effect\n",
      "4                                RS                0.15\n",
      "5                              TRSQ                0.15\n",
      "6         ACES_neglectful_parenting               -0.15\n",
      "7                        ACES_abuse               -0.15\n",
      "0                              BSCS                0.00\n",
      "49                     SRHI_healthy                0.00\n",
      "56         TESQ_E_goal_deliberation                0.00\n",
      "55     TESQ_E_goal_and_rule_setting                0.00\n",
      "54               TESQ_E_distraction                0.00\n",
      "53  TESQ_E_avoidance_of_temptations                0.00\n",
      "52   TESQ_E_controlling_temptations                0.00\n",
      "51                         SRHI_sum                0.00\n",
      "50                   SRHI_unhealthy                0.00\n",
      "47                    RTFS_factor_1                0.00\n",
      "48                    RTFS_factor_2                0.00\n",
      "58               TESQ_E_suppression                0.00\n",
      "46                   RMQ_locomotion                0.00\n",
      "45                   RMQ_assessment                0.00\n",
      "44                          RMQ_lie                0.00\n",
      "43        PLAN_temporal_orientation                0.00\n",
      "bf_2\n",
      "cancer_promoting_minus_preventing_FFQ_w2\n",
      "FFQ_v2_Mean_Energy_w2\n",
      "(275, 76)\n",
      "(275, 76)\n",
      "outer split0\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.920e+02, tolerance: 4.427e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.197e+02, tolerance: 4.541e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.120e+02, tolerance: 4.505e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.134e+02, tolerance: 4.726e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+02, tolerance: 4.691e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.994e+02, tolerance: 4.514e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.516e+02, tolerance: 4.643e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.814e+02, tolerance: 4.504e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.341e+02, tolerance: 4.752e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split1\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.683e+02, tolerance: 4.729e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.295e+02, tolerance: 4.613e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.751e+02, tolerance: 4.771e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.439e+02, tolerance: 4.652e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.170e+02, tolerance: 4.524e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.000e+02, tolerance: 4.617e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.809e+02, tolerance: 4.868e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.447e+02, tolerance: 4.802e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.210e+02, tolerance: 4.861e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split2\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.536e+02, tolerance: 4.398e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.425e+02, tolerance: 4.412e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.118e+02, tolerance: 4.668e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.824e+02, tolerance: 4.301e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.522e+02, tolerance: 4.388e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.669e+02, tolerance: 4.273e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.734e+02, tolerance: 4.530e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.318e+02, tolerance: 4.469e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.451e+02, tolerance: 4.335e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split3\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.376e+02, tolerance: 4.328e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.425e+02, tolerance: 4.408e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.262e+02, tolerance: 4.606e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.855e+02, tolerance: 4.477e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+02, tolerance: 4.389e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.547e+02, tolerance: 4.312e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.183e+02, tolerance: 4.516e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.840e+02, tolerance: 4.280e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.631e+02, tolerance: 4.373e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split4\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.010e+02, tolerance: 4.516e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.919e+02, tolerance: 4.471e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.205e+02, tolerance: 4.466e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.207e+02, tolerance: 4.735e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.022e+02, tolerance: 4.492e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.608e+02, tolerance: 4.685e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.830e+02, tolerance: 4.283e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.820e+02, tolerance: 4.182e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.655e+02, tolerance: 4.524e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split5\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.899e+02, tolerance: 4.688e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.440e+02, tolerance: 4.757e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.434e+02, tolerance: 5.001e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.700e+02, tolerance: 4.937e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.188e+02, tolerance: 4.932e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.590e+02, tolerance: 4.854e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.857e+02, tolerance: 4.610e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.152e+02, tolerance: 4.662e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.427e+02, tolerance: 4.622e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split6\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.174e+02, tolerance: 4.704e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.866e+02, tolerance: 4.403e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.615e+02, tolerance: 4.669e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.452e+02, tolerance: 4.982e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.714e+02, tolerance: 4.623e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.910e+02, tolerance: 4.875e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.092e+02, tolerance: 4.566e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.153e+02, tolerance: 4.467e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.088e+02, tolerance: 4.516e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split7\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.260e+02, tolerance: 4.692e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.007e+02, tolerance: 4.534e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.294e+02, tolerance: 4.816e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.839e+02, tolerance: 4.798e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.081e+02, tolerance: 4.502e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.464e+02, tolerance: 4.513e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.305e+02, tolerance: 4.689e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.793e+02, tolerance: 4.440e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+02, tolerance: 4.761e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split8\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.048e+02, tolerance: 4.827e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.870e+02, tolerance: 4.682e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.626e+02, tolerance: 4.805e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.744e+02, tolerance: 4.677e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.694e+02, tolerance: 4.726e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.004e+02, tolerance: 4.533e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.613e+02, tolerance: 4.513e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.687e+02, tolerance: 4.506e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.570e+02, tolerance: 4.577e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split9\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.430e+02, tolerance: 4.553e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.211e+02, tolerance: 4.729e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.675e+02, tolerance: 4.481e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.757e+02, tolerance: 4.722e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.964e+02, tolerance: 4.520e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.337e+02, tolerance: 4.242e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.393e+02, tolerance: 4.866e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.261e+02, tolerance: 4.414e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.061e+02, tolerance: 4.371e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "scores:\n",
      "[0.18887905438132735, 0.022742377445344797, -0.14188282450331902, 0.15195767246541614, -0.002334168817955451, -0.34680626449621577, -0.1066757539084151, 0.11551380039900305, -0.04622236716585104, 0.051474484811547816]\n",
      "overall_score:\n",
      "-0.011335398938911723\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#set np random seed\n",
    "np.random.seed(3161527)\n",
    "\n",
    "group_names = ['ichi','ni','san']\n",
    "#assign each row randomly to a group\n",
    "group_assignments = np.random.choice(group_names,analysis_data_imputed.shape[0])\n",
    "\n",
    "#synthetic outcomes\n",
    "outcome_measures = generate_synthetic_dev_outcomes(outcome_measures)\n",
    "\n",
    "# add synthetic primary and interaction effects\n",
    "\n",
    "\n",
    "#set up the interaction effects\n",
    "custom_interaction_effects_g1 = [0]*analysis_data_imputed.shape[1]\n",
    "custom_interaction_effects_g1[0] = 0.15\n",
    "custom_interaction_effects_g1[1] = 0.15\n",
    "custom_interaction_effects_g1[2] = -0.15\n",
    "custom_interaction_effects_g1[3] = -0.15\n",
    "\n",
    "custom_interaction_effects_g2 = [0]*analysis_data_imputed.shape[1]\n",
    "custom_interaction_effects_g2[4] = 0.15\n",
    "custom_interaction_effects_g2[5] = 0.15\n",
    "custom_interaction_effects_g2[6] = -0.15\n",
    "custom_interaction_effects_g2[7] = -0.15\n",
    "\n",
    "custom_interaction_effects = {'ni':custom_interaction_effects_g1,'san':custom_interaction_effects_g2}\n",
    "\n",
    "\n",
    "\n",
    "synthetic_data = generate_synthetic_dev_data(analysis_data_imputed, group_assignments,outcome_measures, group_interaction_effects = custom_interaction_effects)\n",
    "interaction_effect_df = synthetic_data['X_weights']\n",
    "outcome_measures = synthetic_data['y']\n",
    "\n",
    "# Set up outcome measures and group assignment one-hot\n",
    "\n",
    "outcome_measures = calculate_outcome_changes(outcome_measures)\n",
    "group_assignment_onehots = pd.get_dummies(group_assignments).loc[:,['ni','san']]\n",
    "\n",
    "predictor_data = set_up_interactions(analysis_data_imputed, group_assignment_onehots)\n",
    "\n",
    "\n",
    "#remove any NA values for this outcome measure in both the predictor data and the outcome data\n",
    "outcome_nas = outcome_measures['d_bf'].isna()\n",
    "\n",
    "outcome_measures_nona = outcome_measures.loc[~outcome_nas,:]\n",
    "predictor_data_nona = predictor_data.loc[~outcome_nas,:]\n",
    "group_assignment_onehots_nonan = group_assignment_onehots.loc[~outcome_nas,:]\n",
    "group_assignments_nona = group_assignments[~outcome_nas]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Try out CV with simple gridsearch\n",
    "\n",
    "\n",
    "scoring_data = do_scoring_loop(X=predictor_data_nona, y= outcome_measures_nona['d_bf'], \n",
    "                groups = group_assignments_nona, \n",
    "                hyperparameter_selection_on_fold=do_hyperparameter_selection_on_2,\n",
    "                outer_folds=10)\n",
    "\n",
    "scores = scoring_data['scores']\n",
    "best_models = scoring_data['best_models']\n",
    "best_params_df_list = scoring_data['best_params_df_list']\n",
    "raw_cv_results_list = scoring_data['raw_cv_results_list']\n",
    "\n",
    "print(\"scores:\")\n",
    "print(scores)\n",
    "overall_score = np.mean(scores)\n",
    "print(\"overall_score:\")\n",
    "print(overall_score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean_test_score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">std_test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_description</th>\n",
       "      <th>params_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 25}</th>\n",
       "      <td>-3.522020</td>\n",
       "      <td>0.046123</td>\n",
       "      <td>0.476817</td>\n",
       "      <td>0.135261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.2}</th>\n",
       "      <td>-3.570679</td>\n",
       "      <td>0.048386</td>\n",
       "      <td>0.473254</td>\n",
       "      <td>0.101980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 40}</th>\n",
       "      <td>-3.572833</td>\n",
       "      <td>0.041259</td>\n",
       "      <td>0.458208</td>\n",
       "      <td>0.135367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__n_neighbors': 16}</th>\n",
       "      <td>-3.593369</td>\n",
       "      <td>0.053888</td>\n",
       "      <td>0.457388</td>\n",
       "      <td>0.112382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.4}</th>\n",
       "      <td>-3.603683</td>\n",
       "      <td>0.032778</td>\n",
       "      <td>0.457453</td>\n",
       "      <td>0.149324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 10}</th>\n",
       "      <td>-3.655157</td>\n",
       "      <td>0.036885</td>\n",
       "      <td>0.433918</td>\n",
       "      <td>0.146004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.6}</th>\n",
       "      <td>-3.656658</td>\n",
       "      <td>0.045859</td>\n",
       "      <td>0.461504</td>\n",
       "      <td>0.156509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 63}</th>\n",
       "      <td>-3.659670</td>\n",
       "      <td>0.050463</td>\n",
       "      <td>0.496013</td>\n",
       "      <td>0.142917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.1}</th>\n",
       "      <td>-3.690697</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.481553</td>\n",
       "      <td>0.118551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8}</th>\n",
       "      <td>-3.694966</td>\n",
       "      <td>0.043170</td>\n",
       "      <td>0.467687</td>\n",
       "      <td>0.150113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 100}</th>\n",
       "      <td>-3.708646</td>\n",
       "      <td>0.052578</td>\n",
       "      <td>0.488503</td>\n",
       "      <td>0.137954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 1.0}</th>\n",
       "      <td>-3.719681</td>\n",
       "      <td>0.047648</td>\n",
       "      <td>0.474915</td>\n",
       "      <td>0.134263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"20\" valign=\"top\">dict_values([StandardScaler(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.737236</td>\n",
       "      <td>0.111684</td>\n",
       "      <td>0.498190</td>\n",
       "      <td>0.109278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.737236</td>\n",
       "      <td>0.111684</td>\n",
       "      <td>0.498190</td>\n",
       "      <td>0.109278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.737236</td>\n",
       "      <td>0.111684</td>\n",
       "      <td>0.498190</td>\n",
       "      <td>0.109278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.777193</td>\n",
       "      <td>0.106825</td>\n",
       "      <td>0.428604</td>\n",
       "      <td>0.107087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.785071</td>\n",
       "      <td>0.126331</td>\n",
       "      <td>0.427741</td>\n",
       "      <td>0.126823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.785629</td>\n",
       "      <td>0.072315</td>\n",
       "      <td>0.490581</td>\n",
       "      <td>0.144449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.785629</td>\n",
       "      <td>0.072315</td>\n",
       "      <td>0.490581</td>\n",
       "      <td>0.144449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.788476</td>\n",
       "      <td>0.106610</td>\n",
       "      <td>0.449004</td>\n",
       "      <td>0.117667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.790754</td>\n",
       "      <td>0.095669</td>\n",
       "      <td>0.433426</td>\n",
       "      <td>0.115918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.793368</td>\n",
       "      <td>0.083849</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.143894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.794865</td>\n",
       "      <td>0.122039</td>\n",
       "      <td>0.438251</td>\n",
       "      <td>0.125421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 6}</th>\n",
       "      <td>-3.803051</td>\n",
       "      <td>0.060779</td>\n",
       "      <td>0.432516</td>\n",
       "      <td>0.107971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">dict_values([StandardScaler(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.812025</td>\n",
       "      <td>0.090158</td>\n",
       "      <td>0.491998</td>\n",
       "      <td>0.169718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.812025</td>\n",
       "      <td>0.090158</td>\n",
       "      <td>0.491998</td>\n",
       "      <td>0.169718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.812870</td>\n",
       "      <td>0.127387</td>\n",
       "      <td>0.458497</td>\n",
       "      <td>0.119762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.829674</td>\n",
       "      <td>0.100991</td>\n",
       "      <td>0.487691</td>\n",
       "      <td>0.174611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.883801</td>\n",
       "      <td>0.135435</td>\n",
       "      <td>0.496836</td>\n",
       "      <td>0.146136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.931707</td>\n",
       "      <td>0.154096</td>\n",
       "      <td>0.513077</td>\n",
       "      <td>0.127182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.934478</td>\n",
       "      <td>0.138726</td>\n",
       "      <td>0.508512</td>\n",
       "      <td>0.164795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.936575</td>\n",
       "      <td>0.115907</td>\n",
       "      <td>0.506634</td>\n",
       "      <td>0.156856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.936575</td>\n",
       "      <td>0.115907</td>\n",
       "      <td>0.506634</td>\n",
       "      <td>0.156856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 4}</th>\n",
       "      <td>-3.949757</td>\n",
       "      <td>0.056452</td>\n",
       "      <td>0.460496</td>\n",
       "      <td>0.125978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">dict_values([StandardScaler(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.980638</td>\n",
       "      <td>0.127521</td>\n",
       "      <td>0.517306</td>\n",
       "      <td>0.169344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.983786</td>\n",
       "      <td>0.173552</td>\n",
       "      <td>0.545651</td>\n",
       "      <td>0.127642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-4.000871</td>\n",
       "      <td>0.112683</td>\n",
       "      <td>0.527901</td>\n",
       "      <td>0.155778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-4.000871</td>\n",
       "      <td>0.112683</td>\n",
       "      <td>0.527901</td>\n",
       "      <td>0.155778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-4.023862</td>\n",
       "      <td>0.162327</td>\n",
       "      <td>0.530817</td>\n",
       "      <td>0.160722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 3}</th>\n",
       "      <td>-4.090235</td>\n",
       "      <td>0.071966</td>\n",
       "      <td>0.469199</td>\n",
       "      <td>0.107543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-4.263323</td>\n",
       "      <td>0.195655</td>\n",
       "      <td>0.604520</td>\n",
       "      <td>0.180698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 2}</th>\n",
       "      <td>-4.397137</td>\n",
       "      <td>0.090027</td>\n",
       "      <td>0.523513</td>\n",
       "      <td>0.144930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-4.406927</td>\n",
       "      <td>0.206068</td>\n",
       "      <td>0.645560</td>\n",
       "      <td>0.188635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 1}</th>\n",
       "      <td>-4.812014</td>\n",
       "      <td>0.078755</td>\n",
       "      <td>0.591860</td>\n",
       "      <td>0.142454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.01}</th>\n",
       "      <td>-5.258356</td>\n",
       "      <td>0.147121</td>\n",
       "      <td>0.716611</td>\n",
       "      <td>0.167308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">dict_values([StandardScaler(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 1.0}</th>\n",
       "      <td>-5.674324</td>\n",
       "      <td>0.203889</td>\n",
       "      <td>0.747942</td>\n",
       "      <td>0.184292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8}</th>\n",
       "      <td>-5.855230</td>\n",
       "      <td>0.227252</td>\n",
       "      <td>0.765743</td>\n",
       "      <td>0.188165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6}</th>\n",
       "      <td>-6.096521</td>\n",
       "      <td>0.252987</td>\n",
       "      <td>0.793524</td>\n",
       "      <td>0.186907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4}</th>\n",
       "      <td>-6.458223</td>\n",
       "      <td>0.301549</td>\n",
       "      <td>0.828741</td>\n",
       "      <td>0.182128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.2}</th>\n",
       "      <td>-7.140921</td>\n",
       "      <td>0.380793</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.157523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1}</th>\n",
       "      <td>-7.915941</td>\n",
       "      <td>0.429971</td>\n",
       "      <td>1.030900</td>\n",
       "      <td>0.142350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.01}</th>\n",
       "      <td>-11.077672</td>\n",
       "      <td>0.804085</td>\n",
       "      <td>1.628760</td>\n",
       "      <td>0.409948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing permutation test on importance; this may take time.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictor</th>\n",
       "      <th>coef</th>\n",
       "      <th>feature_importance</th>\n",
       "      <th>fa_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>ACES_abuse*san</td>\n",
       "      <td>None</td>\n",
       "      <td>0.011046</td>\n",
       "      <td>0.011046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RS</td>\n",
       "      <td>None</td>\n",
       "      <td>0.009779</td>\n",
       "      <td>0.009779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>ACES_neglectful_parenting*san</td>\n",
       "      <td>None</td>\n",
       "      <td>0.009589</td>\n",
       "      <td>0.009589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BSCS</td>\n",
       "      <td>None</td>\n",
       "      <td>0.008820</td>\n",
       "      <td>0.008820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BIS_11</td>\n",
       "      <td>None</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>0.007906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>ACES_neglectful_parenting*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.007100</td>\n",
       "      <td>0.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>RTFS_f1_minus_f2*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>0.006893</td>\n",
       "      <td>0.006893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>PLAN_temporal_orientation</td>\n",
       "      <td>None</td>\n",
       "      <td>0.006702</td>\n",
       "      <td>0.006702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>cSES</td>\n",
       "      <td>None</td>\n",
       "      <td>0.006590</td>\n",
       "      <td>0.006590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>TESQ_E_goal_deliberation*san</td>\n",
       "      <td>None</td>\n",
       "      <td>0.006364</td>\n",
       "      <td>0.006364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>WTP_unhealthy_minus_healthy</td>\n",
       "      <td>None</td>\n",
       "      <td>0.006165</td>\n",
       "      <td>0.006165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>NCS_satisfaction_in_deliberating</td>\n",
       "      <td>None</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>0.006136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ACES_abuse</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005873</td>\n",
       "      <td>0.005873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>TESQ_E_goal_deliberation</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.005776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>SRHI_unhealthy</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005715</td>\n",
       "      <td>0.005715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>NCS_thinking_not_fun*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005670</td>\n",
       "      <td>0.005670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>PLAN_cognitive_strategies</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>0.005598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>NCS_intellectual_task*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005563</td>\n",
       "      <td>0.005563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>cancer_promoting_minus_preventing_liked_FCI*san</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.005295</td>\n",
       "      <td>0.005295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>NCS_prefer_little_thought*san</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.005158</td>\n",
       "      <td>0.005158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model = get_best_model(summarize_overall_df_results(raw_cv_results_list))\n",
    "final_fit = do_final_fit(X=predictor_data_nona, y= outcome_measures_nona['d_bf'], final_model=best_model)\n",
    "final_results = present_model_results(X=predictor_data_nona, final_fit=final_fit,y=outcome_measures_nona['d_bf'])\n",
    "\n",
    "#print rows of final_results where feature_name is the list of features to check\n",
    "base_regressors = interaction_effect_df.predictor[interaction_effect_df.interaction_effect!=0]\n",
    "regressors_to_check = [x+y for y in ['','*ni','*san'] for x in base_regressors]\n",
    "final_results['planned_regression'] = final_results['predictor'].isin(regressors_to_check)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/Google Drive/oregon/code/DEV_scripts/analyses/intervention_moderation/dev_interaction_util.py:326: FutureWarning: merging between different levels is deprecated and will be removed in a future version. (2 levels on the left, 1 on the right)\n",
      "  results_vs_cors = final_results_wide.merge(group_correlations, left_index=True, right_index=True, how='outer')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(feature_importance, base)</th>\n",
       "      <th>(feature_importance, ni)</th>\n",
       "      <th>(feature_importance, san)</th>\n",
       "      <th>ichi_cor</th>\n",
       "      <th>ni_cor</th>\n",
       "      <th>san_cor</th>\n",
       "      <th>abs_effect_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACES_neglectful_parenting</th>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACES_abuse</th>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RTFS_f1_minus_f2</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TESQ_E_goal_deliberation</th>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_thinking_not_fun</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RS</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>household_income_per_person</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_prefer_little_thought</th>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACES_sum</th>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BSCS</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIS_11</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WTP_unhealthy_minus_healthy</th>\n",
       "      <td>0.006</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAN_temporal_orientation</th>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRSQ</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_relief_not_satisfaction</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_satisfaction_in_deliberating</th>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_intellectual_task</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SST_PostErrorSlowW1_mean</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_median_income_acs</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRHI_unhealthy</th>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_solve_puzzles</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SST_mean_ssrt_0</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancer_promoting_minus_preventing_craved_FCI</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancer_promoting_minus_preventing_liked_FCI</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cSES</th>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_prefer_complex</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TESQ_E_goal_and_rule_setting</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BFI_neuroticism</th>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_abstract_thinking</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_like_responsibility</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCS</th>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_thinking_not_exciting</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAN_cognitive_strategies</th>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACES_household_dysfunction</th>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RTFS_factor_1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAN_mental_forecasting</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMI_value_usefulness</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMI_perceived_choice</th>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_think_minimally</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_own</th>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BFI_conscientiousness</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDM</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "present_results_vs_ground_truth_cors(predictor_data_nona,outcome_measures_nona,group_assignments_nona,final_results,base_regressors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 3: Add ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def do_hyperparameter_selection3(X, y,cv):\n",
    "    #alpha parameters for Ridge and Lasso\n",
    "    alpha_10pow_lower = 2\n",
    "    alpha_10pow_upper = 0\n",
    "    alpha_increments=1\n",
    "    alpha_range = np.concatenate([np.power(10,np.linspace(-alpha_10pow_lower,alpha_10pow_upper,(alpha_10pow_lower+alpha_10pow_upper)*alpha_increments+1)),\n",
    "        [0.2,0.4,0.6,0.8,1.0]])\n",
    "\n",
    "    ############\n",
    "    #RIDGE\n",
    "    ridge_parameters = {'alpha':alpha_range}\n",
    "    ridge_model = linear_model.Ridge()\n",
    "    print(ridge_parameters)\n",
    "    #do a gridsearch, using the same folds as the outer loop\n",
    "    ridge_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(ridge_model), param_grid = get_param_grid_with_preprocessing(ridge_parameters), cv=cv,scoring='neg_mean_absolute_error')\n",
    "    ridge_grid_search_cv.fit(X,y)\n",
    "\n",
    "    ############\n",
    "    #LASSO\n",
    "    lasso_parameters = {'alpha':alpha_range}\n",
    "    lasso_model = linear_model.Lasso()\n",
    "    print(lasso_parameters)\n",
    "    lasso_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(lasso_model), param_grid = get_param_grid_with_preprocessing(lasso_parameters), cv=cv,scoring='neg_mean_absolute_error')\n",
    "    lasso_grid_search_cv.fit(X,y)\n",
    "\n",
    "    #######\n",
    "    #KNN\n",
    "    knn_parameters = {'n_neighbors':np.unique(np.round(np.power(10,np.linspace(0,2,2*5+1)))).astype(int)}\n",
    "    knn_model = KNeighborsRegressor()\n",
    "    print(knn_parameters)\n",
    "    knn_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(knn_model), param_grid = get_param_grid_with_preprocessing(knn_parameters), cv=cv,scoring='neg_mean_absolute_error')\n",
    "    knn_grid_search_cv.fit(X,y)\n",
    "\n",
    "\n",
    "    ###########\n",
    "    #Decision tree regressor\n",
    "    dt_regressor_parameters = {\n",
    "        'max_depth':[2, 3,5,10],\n",
    "        'min_samples_split':[5,20,50],\n",
    "        'min_samples_leaf':[5,20,50]\n",
    "    }             \n",
    "    print(dt_regressor_parameters)\n",
    "    dt_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(DecisionTreeRegressor()), param_grid = get_param_grid_with_preprocessing(dt_regressor_parameters), cv=cv,scoring='neg_mean_absolute_error')\n",
    "    dt_grid_search_cv.fit(X,y)\n",
    "\n",
    "    ###########\n",
    "    #Random forest regressor\n",
    "    rf_regressor_parameters = {\n",
    "        'n_estimators':[10,25,50],\n",
    "        'max_depth':[2, 3,5,10],\n",
    "        'min_samples_split':[5,20,50],\n",
    "        'min_samples_leaf':[5,20,50]\n",
    "    }\n",
    "    print(rf_regressor_parameters)\n",
    "    rf_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(RandomForestRegressor()), param_grid = get_param_grid_with_preprocessing(rf_regressor_parameters), cv=cv,scoring='neg_mean_absolute_error')\n",
    "    rf_grid_search_cv.fit(X,y)\n",
    "\n",
    "    ###########\n",
    "    #Gradient boosting regressor\n",
    "    gb_regressor_parameters = {\n",
    "        'n_estimators':[10,20,50],\n",
    "        'max_depth':[2, 3,5,10],\n",
    "        'min_samples_split':[5,20,50],\n",
    "        'min_samples_leaf':[5,20,50]\n",
    "    }\n",
    "    print(gb_regressor_parameters)\n",
    "    gb_grid_search_cv = GridSearchCV(estimator=get_estimator_with_preprocessing(GradientBoostingRegressor()), param_grid = get_param_grid_with_preprocessing(gb_regressor_parameters), cv=cv,scoring='neg_mean_absolute_error')\n",
    "    gb_grid_search_cv.fit(X,y)\n",
    "\n",
    "\n",
    "    all_cv_results = [ridge_grid_search_cv, lasso_grid_search_cv, knn_grid_search_cv, dt_grid_search_cv, rf_grid_search_cv, gb_grid_search_cv]\n",
    "    #all_cv_results = [ridge_grid_search_cv, lasso_grid_search_cv, knn_grid_search_cv, dt_grid_search_cv]\n",
    "    #all_cv_results = [ridge_grid_search_cv, lasso_grid_search_cv]\n",
    "\n",
    "    #create a dataframe with the best parameters, best mean_test_score, and name of the model\n",
    "\n",
    "    best_params_df = pd.DataFrame({\n",
    "        'model': [cv_result.estimator for cv_result in all_cv_results],\n",
    "        'model_name': [cv_result.estimator.__class__.__name__ for cv_result in all_cv_results],\n",
    "        'best_params': [extract_estimator_params_from_gridsearch(cv_result.best_params_) for cv_result in all_cv_results],\n",
    "        'best_score': [cv_result.best_score_ for cv_result in all_cv_results],\n",
    "        'best_raw_params' : [cv_result.best_params_ for cv_result in all_cv_results]\n",
    "        })\n",
    "    \n",
    "    best_params_df = best_params_df.sort_values('best_score',ascending=False).reset_index(drop=True)\n",
    "\n",
    "    best_model = clone(best_params_df['model'][0])\n",
    "    best_model_params = best_params_df['best_raw_params'][0]\n",
    "    best_model.set_params(**best_model_params)\n",
    "\n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_params_df':best_params_df,\n",
    "        'raw_cv_results':all_cv_results\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ni' 'san']\n",
      "[1.28335298 0.42953651]\n",
      "['san' 'san' 'ni' 'ichi' 'san' 'san' 'ichi' 'san' 'san' 'san' 'ni' 'ichi'\n",
      " 'ichi' 'ichi' 'ichi' 'san' 'san' 'san' 'ichi' 'ichi' 'san' 'san' 'ni'\n",
      " 'ni' 'ni' 'ni' 'ni' 'ni' 'ni' 'san' 'ni' 'san' 'ni' 'ichi' 'ni' 'san'\n",
      " 'ni' 'ichi' 'san' 'ni' 'ni' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ni' 'ni'\n",
      " 'san' 'ichi' 'ichi' 'ni' 'san' 'ni' 'ichi' 'ni' 'ni' 'ni' 'ichi' 'san'\n",
      " 'ni' 'ni' 'ichi' 'ni' 'ichi' 'san' 'ni' 'ni' 'ni' 'san' 'ichi' 'ni' 'san'\n",
      " 'ichi' 'san' 'ni' 'san' 'ni' 'ichi' 'ichi' 'san' 'ichi' 'san' 'san' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'san' 'ni' 'san' 'ni' 'ichi' 'san' 'san' 'san' 'ichi'\n",
      " 'ni' 'san' 'ichi' 'ichi' 'san' 'ni' 'ichi' 'san' 'ni' 'ni' 'san' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'ichi' 'ni' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ichi'\n",
      " 'ni' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ichi' 'san' 'san' 'ni' 'ichi' 'ni'\n",
      " 'ichi' 'ichi' 'san' 'ichi' 'ni' 'san' 'san' 'ni' 'ni' 'san' 'san' 'san'\n",
      " 'ichi' 'san' 'ni' 'san' 'ichi' 'ichi' 'ichi' 'ni' 'san' 'ni' 'ni' 'ni'\n",
      " 'ichi' 'ni' 'ichi' 'san' 'ni' 'san' 'ichi' 'ni' 'ni' 'ni' 'ichi' 'ni'\n",
      " 'ichi' 'san' 'san' 'san' 'san' 'ichi' 'ni' 'san' 'san' 'san' 'ichi' 'san'\n",
      " 'ni' 'ni' 'san' 'ichi' 'ichi' 'san' 'ni' 'ni' 'san' 'ni' 'san' 'san' 'ni'\n",
      " 'san' 'ni' 'ni' 'san' 'ichi' 'san' 'ichi' 'san' 'ni' 'ni' 'ni' 'ichi'\n",
      " 'ni' 'ni' 'san' 'ni' 'ni' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ni' 'ni' 'san'\n",
      " 'ichi' 'ni' 'ichi' 'ichi' 'ichi' 'ichi' 'ichi' 'ichi' 'san' 'ichi' 'san'\n",
      " 'ichi' 'ni' 'san' 'san' 'ni' 'ichi' 'ni' 'ni' 'ichi' 'ni' 'san' 'san'\n",
      " 'ichi' 'ichi' 'ichi' 'ichi' 'san' 'san' 'ni' 'ni' 'ni' 'san' 'ichi'\n",
      " 'ichi' 'ichi' 'ni' 'ichi' 'ni' 'san' 'ichi' 'san' 'san' 'ni' 'san' 'san'\n",
      " 'san' 'san' 'ichi' 'ichi' 'ichi' 'ni' 'ni' 'ichi' 'san' 'san' 'san']\n",
      "ni\n",
      "                       feature_name  interaction_effect\n",
      "0                              BSCS                0.15\n",
      "3                               PCS               -0.15\n",
      "1                               EDM                0.15\n",
      "2                            BIS_11               -0.15\n",
      "74      WTP_unhealthy_minus_healthy                0.00\n",
      "55     TESQ_E_goal_and_rule_setting                0.00\n",
      "54               TESQ_E_distraction                0.00\n",
      "53  TESQ_E_avoidance_of_temptations                0.00\n",
      "52   TESQ_E_controlling_temptations                0.00\n",
      "51                         SRHI_sum                0.00\n",
      "50                   SRHI_unhealthy                0.00\n",
      "49                     SRHI_healthy                0.00\n",
      "48                    RTFS_factor_2                0.00\n",
      "47                    RTFS_factor_1                0.00\n",
      "46                   RMQ_locomotion                0.00\n",
      "45                   RMQ_assessment                0.00\n",
      "44                          RMQ_lie                0.00\n",
      "43        PLAN_temporal_orientation                0.00\n",
      "42          PLAN_mental_forecasting                0.00\n",
      "41        PLAN_cognitive_strategies                0.00\n",
      "bf_2\n",
      "cancer_promoting_minus_preventing_FFQ_w2\n",
      "FFQ_v2_Mean_Energy_w2\n",
      "san\n",
      "                       feature_name  interaction_effect\n",
      "4                                RS                0.15\n",
      "5                              TRSQ                0.15\n",
      "6         ACES_neglectful_parenting               -0.15\n",
      "7                        ACES_abuse               -0.15\n",
      "0                              BSCS                0.00\n",
      "49                     SRHI_healthy                0.00\n",
      "56         TESQ_E_goal_deliberation                0.00\n",
      "55     TESQ_E_goal_and_rule_setting                0.00\n",
      "54               TESQ_E_distraction                0.00\n",
      "53  TESQ_E_avoidance_of_temptations                0.00\n",
      "52   TESQ_E_controlling_temptations                0.00\n",
      "51                         SRHI_sum                0.00\n",
      "50                   SRHI_unhealthy                0.00\n",
      "47                    RTFS_factor_1                0.00\n",
      "48                    RTFS_factor_2                0.00\n",
      "58               TESQ_E_suppression                0.00\n",
      "46                   RMQ_locomotion                0.00\n",
      "45                   RMQ_assessment                0.00\n",
      "44                          RMQ_lie                0.00\n",
      "43        PLAN_temporal_orientation                0.00\n",
      "bf_2\n",
      "cancer_promoting_minus_preventing_FFQ_w2\n",
      "FFQ_v2_Mean_Energy_w2\n",
      "(275, 76)\n",
      "(275, 76)\n",
      "outer split0\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.920e+02, tolerance: 4.427e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.197e+02, tolerance: 4.541e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.120e+02, tolerance: 4.505e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.134e+02, tolerance: 4.726e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+02, tolerance: 4.691e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.994e+02, tolerance: 4.514e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.516e+02, tolerance: 4.643e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.814e+02, tolerance: 4.504e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.341e+02, tolerance: 4.752e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 25, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 20, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split1\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.683e+02, tolerance: 4.729e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.295e+02, tolerance: 4.613e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.751e+02, tolerance: 4.771e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.439e+02, tolerance: 4.652e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.170e+02, tolerance: 4.524e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.000e+02, tolerance: 4.617e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.809e+02, tolerance: 4.868e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.447e+02, tolerance: 4.802e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.210e+02, tolerance: 4.861e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 25, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 20, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split2\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.536e+02, tolerance: 4.398e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.425e+02, tolerance: 4.412e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.118e+02, tolerance: 4.668e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.824e+02, tolerance: 4.301e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.522e+02, tolerance: 4.388e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.669e+02, tolerance: 4.273e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.734e+02, tolerance: 4.530e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.318e+02, tolerance: 4.469e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.451e+02, tolerance: 4.335e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 25, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 20, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split3\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.376e+02, tolerance: 4.328e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.425e+02, tolerance: 4.408e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.262e+02, tolerance: 4.606e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.855e+02, tolerance: 4.477e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+02, tolerance: 4.389e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.547e+02, tolerance: 4.312e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.183e+02, tolerance: 4.516e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.840e+02, tolerance: 4.280e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.631e+02, tolerance: 4.373e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 25, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 20, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split4\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.010e+02, tolerance: 4.516e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.919e+02, tolerance: 4.471e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.205e+02, tolerance: 4.466e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.207e+02, tolerance: 4.735e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.022e+02, tolerance: 4.492e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.608e+02, tolerance: 4.685e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.830e+02, tolerance: 4.283e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.820e+02, tolerance: 4.182e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.655e+02, tolerance: 4.524e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 25, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 20, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split5\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.899e+02, tolerance: 4.688e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.440e+02, tolerance: 4.757e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.434e+02, tolerance: 5.001e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.700e+02, tolerance: 4.937e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.188e+02, tolerance: 4.932e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.590e+02, tolerance: 4.854e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.857e+02, tolerance: 4.610e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.152e+02, tolerance: 4.662e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.427e+02, tolerance: 4.622e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 25, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 20, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split6\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.174e+02, tolerance: 4.704e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.866e+02, tolerance: 4.403e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.615e+02, tolerance: 4.669e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.452e+02, tolerance: 4.982e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.714e+02, tolerance: 4.623e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.910e+02, tolerance: 4.875e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.092e+02, tolerance: 4.566e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.153e+02, tolerance: 4.467e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.088e+02, tolerance: 4.516e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 25, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 20, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split7\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.260e+02, tolerance: 4.692e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.007e+02, tolerance: 4.534e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.294e+02, tolerance: 4.816e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.839e+02, tolerance: 4.798e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.081e+02, tolerance: 4.502e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.464e+02, tolerance: 4.513e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.305e+02, tolerance: 4.689e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.793e+02, tolerance: 4.440e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+02, tolerance: 4.761e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 25, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 20, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split8\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.048e+02, tolerance: 4.827e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.870e+02, tolerance: 4.682e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.626e+02, tolerance: 4.805e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.744e+02, tolerance: 4.677e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.694e+02, tolerance: 4.726e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.004e+02, tolerance: 4.533e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.613e+02, tolerance: 4.513e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.687e+02, tolerance: 4.506e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.570e+02, tolerance: 4.577e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 25, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 20, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "outer split9\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n",
      "{'alpha': array([0.01, 0.1 , 1.  , 0.2 , 0.4 , 0.6 , 0.8 , 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.430e+02, tolerance: 4.553e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.211e+02, tolerance: 4.729e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.675e+02, tolerance: 4.481e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.757e+02, tolerance: 4.722e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.964e+02, tolerance: 4.520e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.337e+02, tolerance: 4.242e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.393e+02, tolerance: 4.866e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.261e+02, tolerance: 4.414e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/benjaminsmith/opt/anaconda3/envs/dataanalysis3_10/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.061e+02, tolerance: 4.371e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': array([  1,   2,   3,   4,   6,  10,  16,  25,  40,  63, 100])}\n",
      "{'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 25, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "{'n_estimators': [10, 20, 50], 'max_depth': [2, 3, 5, 10], 'min_samples_split': [5, 20, 50], 'min_samples_leaf': [5, 20, 50]}\n",
      "scores:\n",
      "[0.14570889633271344, 0.022742377445344797, -0.11088661094519092, 0.15195767246541614, -0.002334168817955451, -0.17501960910903525, 0.024359206831576663, 0.003100690678360052, -0.21485603642571127, 0.039961914483410466]\n",
      "overall_score:\n",
      "-0.011526566706107133\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#set np random seed\n",
    "np.random.seed(3161527)\n",
    "\n",
    "group_names = ['ichi','ni','san']\n",
    "#assign each row randomly to a group\n",
    "group_assignments = np.random.choice(group_names,analysis_data_imputed.shape[0])\n",
    "\n",
    "#synthetic outcomes\n",
    "outcome_measures = generate_synthetic_dev_outcomes(outcome_measures)\n",
    "\n",
    "# add synthetic primary and interaction effects\n",
    "\n",
    "\n",
    "#set up the interaction effects\n",
    "custom_interaction_effects_g1 = [0]*analysis_data_imputed.shape[1]\n",
    "custom_interaction_effects_g1[0] = 0.15\n",
    "custom_interaction_effects_g1[1] = 0.15\n",
    "custom_interaction_effects_g1[2] = -0.15\n",
    "custom_interaction_effects_g1[3] = -0.15\n",
    "\n",
    "custom_interaction_effects_g2 = [0]*analysis_data_imputed.shape[1]\n",
    "custom_interaction_effects_g2[4] = 0.15\n",
    "custom_interaction_effects_g2[5] = 0.15\n",
    "custom_interaction_effects_g2[6] = -0.15\n",
    "custom_interaction_effects_g2[7] = -0.15\n",
    "\n",
    "custom_interaction_effects = {'ni':custom_interaction_effects_g1,'san':custom_interaction_effects_g2}\n",
    "\n",
    "\n",
    "\n",
    "synthetic_data = generate_synthetic_dev_data(analysis_data_imputed, group_assignments,outcome_measures, group_interaction_effects = custom_interaction_effects)\n",
    "interaction_effect_df = synthetic_data['X_weights']\n",
    "outcome_measures = synthetic_data['y']\n",
    "\n",
    "# Set up outcome measures and group assignment one-hot\n",
    "\n",
    "outcome_measures = calculate_outcome_changes(outcome_measures)\n",
    "group_assignment_onehots = pd.get_dummies(group_assignments).loc[:,['ni','san']]\n",
    "\n",
    "predictor_data = set_up_interactions(analysis_data_imputed, group_assignment_onehots)\n",
    "\n",
    "\n",
    "#remove any NA values for this outcome measure in both the predictor data and the outcome data\n",
    "outcome_nas = outcome_measures['d_bf'].isna()\n",
    "\n",
    "outcome_measures_nona = outcome_measures.loc[~outcome_nas,:]\n",
    "predictor_data_nona = predictor_data.loc[~outcome_nas,:]\n",
    "group_assignment_onehots_nonan = group_assignment_onehots.loc[~outcome_nas,:]\n",
    "group_assignments_nona = group_assignments[~outcome_nas]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Try out CV with simple gridsearch\n",
    "\n",
    "\n",
    "scoring_data = do_scoring_loop(X=predictor_data_nona, y= outcome_measures_nona['d_bf'], \n",
    "                groups = group_assignments_nona, \n",
    "                hyperparameter_selection_on_fold=do_hyperparameter_selection3,\n",
    "                outer_folds=10)\n",
    "\n",
    "scores = scoring_data['scores']\n",
    "best_models = scoring_data['best_models']\n",
    "best_params_df_list = scoring_data['best_params_df_list']\n",
    "raw_cv_results_list = scoring_data['raw_cv_results_list']\n",
    "\n",
    "print(\"scores:\")\n",
    "print(scores)\n",
    "overall_score = np.mean(scores)\n",
    "print(\"overall_score:\")\n",
    "print(overall_score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean_test_score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">std_test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_description</th>\n",
       "      <th>params_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 25}</th>\n",
       "      <td>-3.522020</td>\n",
       "      <td>0.046123</td>\n",
       "      <td>0.476817</td>\n",
       "      <td>0.135261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.536705</td>\n",
       "      <td>0.046388</td>\n",
       "      <td>0.468030</td>\n",
       "      <td>0.154991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.536963</td>\n",
       "      <td>0.094484</td>\n",
       "      <td>0.445854</td>\n",
       "      <td>0.168284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.547760</td>\n",
       "      <td>0.058720</td>\n",
       "      <td>0.459199</td>\n",
       "      <td>0.163647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.548086</td>\n",
       "      <td>0.075975</td>\n",
       "      <td>0.450669</td>\n",
       "      <td>0.144703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.552207</td>\n",
       "      <td>0.063260</td>\n",
       "      <td>0.450955</td>\n",
       "      <td>0.168243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.555798</td>\n",
       "      <td>0.053225</td>\n",
       "      <td>0.468279</td>\n",
       "      <td>0.150676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.557039</td>\n",
       "      <td>0.053247</td>\n",
       "      <td>0.435255</td>\n",
       "      <td>0.149365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.557052</td>\n",
       "      <td>0.050776</td>\n",
       "      <td>0.475360</td>\n",
       "      <td>0.153762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.558018</td>\n",
       "      <td>0.077880</td>\n",
       "      <td>0.475142</td>\n",
       "      <td>0.168075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.559029</td>\n",
       "      <td>0.078206</td>\n",
       "      <td>0.477579</td>\n",
       "      <td>0.178226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.560603</td>\n",
       "      <td>0.052862</td>\n",
       "      <td>0.430967</td>\n",
       "      <td>0.152527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.560603</td>\n",
       "      <td>0.052862</td>\n",
       "      <td>0.430967</td>\n",
       "      <td>0.152527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.561152</td>\n",
       "      <td>0.103015</td>\n",
       "      <td>0.435131</td>\n",
       "      <td>0.166099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.562102</td>\n",
       "      <td>0.096339</td>\n",
       "      <td>0.451796</td>\n",
       "      <td>0.155090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.564930</td>\n",
       "      <td>0.057949</td>\n",
       "      <td>0.474502</td>\n",
       "      <td>0.149059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.565233</td>\n",
       "      <td>0.052649</td>\n",
       "      <td>0.474379</td>\n",
       "      <td>0.166894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.565331</td>\n",
       "      <td>0.043593</td>\n",
       "      <td>0.472091</td>\n",
       "      <td>0.160450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.566454</td>\n",
       "      <td>0.063972</td>\n",
       "      <td>0.469056</td>\n",
       "      <td>0.174124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.566775</td>\n",
       "      <td>0.044985</td>\n",
       "      <td>0.481801</td>\n",
       "      <td>0.169611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.568588</td>\n",
       "      <td>0.051538</td>\n",
       "      <td>0.466939</td>\n",
       "      <td>0.157175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.569528</td>\n",
       "      <td>0.057342</td>\n",
       "      <td>0.474331</td>\n",
       "      <td>0.176106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.2}</th>\n",
       "      <td>-3.570679</td>\n",
       "      <td>0.048386</td>\n",
       "      <td>0.473254</td>\n",
       "      <td>0.101980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.570761</td>\n",
       "      <td>0.089426</td>\n",
       "      <td>0.466229</td>\n",
       "      <td>0.183359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.571008</td>\n",
       "      <td>0.059971</td>\n",
       "      <td>0.454020</td>\n",
       "      <td>0.151686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.571843</td>\n",
       "      <td>0.044703</td>\n",
       "      <td>0.477741</td>\n",
       "      <td>0.153381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 40}</th>\n",
       "      <td>-3.572833</td>\n",
       "      <td>0.041259</td>\n",
       "      <td>0.458208</td>\n",
       "      <td>0.135367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.574411</td>\n",
       "      <td>0.069875</td>\n",
       "      <td>0.453511</td>\n",
       "      <td>0.147276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.574751</td>\n",
       "      <td>0.098506</td>\n",
       "      <td>0.467342</td>\n",
       "      <td>0.174130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.574842</td>\n",
       "      <td>0.132974</td>\n",
       "      <td>0.472454</td>\n",
       "      <td>0.151243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.574852</td>\n",
       "      <td>0.064038</td>\n",
       "      <td>0.483321</td>\n",
       "      <td>0.159668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.576603</td>\n",
       "      <td>0.044605</td>\n",
       "      <td>0.459838</td>\n",
       "      <td>0.149188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.578562</td>\n",
       "      <td>0.059060</td>\n",
       "      <td>0.452131</td>\n",
       "      <td>0.153247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.578710</td>\n",
       "      <td>0.069494</td>\n",
       "      <td>0.490722</td>\n",
       "      <td>0.140492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.579253</td>\n",
       "      <td>0.053279</td>\n",
       "      <td>0.468492</td>\n",
       "      <td>0.154802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.579661</td>\n",
       "      <td>0.093350</td>\n",
       "      <td>0.453348</td>\n",
       "      <td>0.157850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.579684</td>\n",
       "      <td>0.042235</td>\n",
       "      <td>0.470723</td>\n",
       "      <td>0.162402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.579950</td>\n",
       "      <td>0.053360</td>\n",
       "      <td>0.470654</td>\n",
       "      <td>0.151694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.580706</td>\n",
       "      <td>0.074022</td>\n",
       "      <td>0.473389</td>\n",
       "      <td>0.135135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.581596</td>\n",
       "      <td>0.063758</td>\n",
       "      <td>0.465130</td>\n",
       "      <td>0.158325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.581799</td>\n",
       "      <td>0.060024</td>\n",
       "      <td>0.446784</td>\n",
       "      <td>0.143829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.581929</td>\n",
       "      <td>0.063623</td>\n",
       "      <td>0.462886</td>\n",
       "      <td>0.139887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.582056</td>\n",
       "      <td>0.063648</td>\n",
       "      <td>0.462834</td>\n",
       "      <td>0.139966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.582442</td>\n",
       "      <td>0.051446</td>\n",
       "      <td>0.466703</td>\n",
       "      <td>0.150281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.582705</td>\n",
       "      <td>0.045886</td>\n",
       "      <td>0.466957</td>\n",
       "      <td>0.157243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.585086</td>\n",
       "      <td>0.083965</td>\n",
       "      <td>0.486294</td>\n",
       "      <td>0.127709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.585341</td>\n",
       "      <td>0.072276</td>\n",
       "      <td>0.464957</td>\n",
       "      <td>0.156612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.586692</td>\n",
       "      <td>0.076309</td>\n",
       "      <td>0.465583</td>\n",
       "      <td>0.175938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.587795</td>\n",
       "      <td>0.071586</td>\n",
       "      <td>0.486854</td>\n",
       "      <td>0.152996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.587876</td>\n",
       "      <td>0.075590</td>\n",
       "      <td>0.468010</td>\n",
       "      <td>0.144933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.587876</td>\n",
       "      <td>0.075590</td>\n",
       "      <td>0.468010</td>\n",
       "      <td>0.144933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.588705</td>\n",
       "      <td>0.052955</td>\n",
       "      <td>0.471723</td>\n",
       "      <td>0.154482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.589326</td>\n",
       "      <td>0.074251</td>\n",
       "      <td>0.447764</td>\n",
       "      <td>0.143037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.591358</td>\n",
       "      <td>0.111202</td>\n",
       "      <td>0.470003</td>\n",
       "      <td>0.153428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 16}</th>\n",
       "      <td>-3.593369</td>\n",
       "      <td>0.053888</td>\n",
       "      <td>0.457388</td>\n",
       "      <td>0.112382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.593811</td>\n",
       "      <td>0.055164</td>\n",
       "      <td>0.473134</td>\n",
       "      <td>0.156638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.594127</td>\n",
       "      <td>0.076664</td>\n",
       "      <td>0.462883</td>\n",
       "      <td>0.159959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.594796</td>\n",
       "      <td>0.088995</td>\n",
       "      <td>0.458874</td>\n",
       "      <td>0.150723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.594868</td>\n",
       "      <td>0.055413</td>\n",
       "      <td>0.466474</td>\n",
       "      <td>0.154495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.595468</td>\n",
       "      <td>0.060474</td>\n",
       "      <td>0.468090</td>\n",
       "      <td>0.153273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.595787</td>\n",
       "      <td>0.069743</td>\n",
       "      <td>0.448254</td>\n",
       "      <td>0.136478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.596163</td>\n",
       "      <td>0.067959</td>\n",
       "      <td>0.487681</td>\n",
       "      <td>0.158760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.597388</td>\n",
       "      <td>0.067075</td>\n",
       "      <td>0.475245</td>\n",
       "      <td>0.157024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.601040</td>\n",
       "      <td>0.066411</td>\n",
       "      <td>0.481154</td>\n",
       "      <td>0.175102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.601262</td>\n",
       "      <td>0.051242</td>\n",
       "      <td>0.470310</td>\n",
       "      <td>0.141800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.602234</td>\n",
       "      <td>0.037038</td>\n",
       "      <td>0.485273</td>\n",
       "      <td>0.153576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.4}</th>\n",
       "      <td>-3.603683</td>\n",
       "      <td>0.032778</td>\n",
       "      <td>0.457453</td>\n",
       "      <td>0.149324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.603748</td>\n",
       "      <td>0.036522</td>\n",
       "      <td>0.445143</td>\n",
       "      <td>0.144147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.607976</td>\n",
       "      <td>0.102530</td>\n",
       "      <td>0.468079</td>\n",
       "      <td>0.184352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.608545</td>\n",
       "      <td>0.052471</td>\n",
       "      <td>0.447552</td>\n",
       "      <td>0.147482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.608545</td>\n",
       "      <td>0.052471</td>\n",
       "      <td>0.447552</td>\n",
       "      <td>0.147482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.609158</td>\n",
       "      <td>0.040903</td>\n",
       "      <td>0.460319</td>\n",
       "      <td>0.129088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.609189</td>\n",
       "      <td>0.040820</td>\n",
       "      <td>0.460317</td>\n",
       "      <td>0.129248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.611183</td>\n",
       "      <td>0.059291</td>\n",
       "      <td>0.469669</td>\n",
       "      <td>0.148351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.614739</td>\n",
       "      <td>0.059089</td>\n",
       "      <td>0.470249</td>\n",
       "      <td>0.143686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.616605</td>\n",
       "      <td>0.130090</td>\n",
       "      <td>0.478626</td>\n",
       "      <td>0.187354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.618413</td>\n",
       "      <td>0.041720</td>\n",
       "      <td>0.466963</td>\n",
       "      <td>0.134473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.618484</td>\n",
       "      <td>0.053545</td>\n",
       "      <td>0.477215</td>\n",
       "      <td>0.147940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.618592</td>\n",
       "      <td>0.053952</td>\n",
       "      <td>0.469217</td>\n",
       "      <td>0.170962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.623219</td>\n",
       "      <td>0.063974</td>\n",
       "      <td>0.464054</td>\n",
       "      <td>0.156579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.623301</td>\n",
       "      <td>0.053256</td>\n",
       "      <td>0.470475</td>\n",
       "      <td>0.163497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.623383</td>\n",
       "      <td>0.063827</td>\n",
       "      <td>0.463896</td>\n",
       "      <td>0.156600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.625478</td>\n",
       "      <td>0.046405</td>\n",
       "      <td>0.453840</td>\n",
       "      <td>0.154598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.625856</td>\n",
       "      <td>0.047907</td>\n",
       "      <td>0.484699</td>\n",
       "      <td>0.141756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.625856</td>\n",
       "      <td>0.047907</td>\n",
       "      <td>0.484699</td>\n",
       "      <td>0.141756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.625856</td>\n",
       "      <td>0.047907</td>\n",
       "      <td>0.484699</td>\n",
       "      <td>0.141756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.627507</td>\n",
       "      <td>0.087102</td>\n",
       "      <td>0.450533</td>\n",
       "      <td>0.134914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.629955</td>\n",
       "      <td>0.052306</td>\n",
       "      <td>0.471593</td>\n",
       "      <td>0.138367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.629955</td>\n",
       "      <td>0.052306</td>\n",
       "      <td>0.471593</td>\n",
       "      <td>0.138367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.630395</td>\n",
       "      <td>0.046330</td>\n",
       "      <td>0.459682</td>\n",
       "      <td>0.137653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.633147</td>\n",
       "      <td>0.049014</td>\n",
       "      <td>0.463470</td>\n",
       "      <td>0.134342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.634991</td>\n",
       "      <td>0.044020</td>\n",
       "      <td>0.474040</td>\n",
       "      <td>0.124398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.634991</td>\n",
       "      <td>0.044020</td>\n",
       "      <td>0.474040</td>\n",
       "      <td>0.124398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.634991</td>\n",
       "      <td>0.044020</td>\n",
       "      <td>0.474040</td>\n",
       "      <td>0.124398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.634991</td>\n",
       "      <td>0.044020</td>\n",
       "      <td>0.474040</td>\n",
       "      <td>0.124398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.634991</td>\n",
       "      <td>0.044020</td>\n",
       "      <td>0.474040</td>\n",
       "      <td>0.124398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.634991</td>\n",
       "      <td>0.044020</td>\n",
       "      <td>0.474040</td>\n",
       "      <td>0.124398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.634991</td>\n",
       "      <td>0.044020</td>\n",
       "      <td>0.474040</td>\n",
       "      <td>0.124398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.634991</td>\n",
       "      <td>0.044020</td>\n",
       "      <td>0.474040</td>\n",
       "      <td>0.124398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.634991</td>\n",
       "      <td>0.044020</td>\n",
       "      <td>0.474040</td>\n",
       "      <td>0.124398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.635113</td>\n",
       "      <td>0.056451</td>\n",
       "      <td>0.482985</td>\n",
       "      <td>0.146111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.635723</td>\n",
       "      <td>0.047132</td>\n",
       "      <td>0.464963</td>\n",
       "      <td>0.150086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.635910</td>\n",
       "      <td>0.106572</td>\n",
       "      <td>0.469714</td>\n",
       "      <td>0.161429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.636916</td>\n",
       "      <td>0.033902</td>\n",
       "      <td>0.469955</td>\n",
       "      <td>0.149247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.636938</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.472835</td>\n",
       "      <td>0.136147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.636938</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.472835</td>\n",
       "      <td>0.136147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.636938</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.472835</td>\n",
       "      <td>0.136147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.636938</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.472835</td>\n",
       "      <td>0.136147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.636938</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.472835</td>\n",
       "      <td>0.136147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.636938</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.472835</td>\n",
       "      <td>0.136147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.636938</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.472835</td>\n",
       "      <td>0.136147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.636938</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.472835</td>\n",
       "      <td>0.136147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.636938</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.472835</td>\n",
       "      <td>0.136147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.637021</td>\n",
       "      <td>0.091622</td>\n",
       "      <td>0.497052</td>\n",
       "      <td>0.185409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.637464</td>\n",
       "      <td>0.127069</td>\n",
       "      <td>0.477476</td>\n",
       "      <td>0.137020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.638012</td>\n",
       "      <td>0.049446</td>\n",
       "      <td>0.468742</td>\n",
       "      <td>0.136110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.638661</td>\n",
       "      <td>0.045241</td>\n",
       "      <td>0.483934</td>\n",
       "      <td>0.136650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.639165</td>\n",
       "      <td>0.050655</td>\n",
       "      <td>0.474747</td>\n",
       "      <td>0.141173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.639963</td>\n",
       "      <td>0.059967</td>\n",
       "      <td>0.452444</td>\n",
       "      <td>0.148662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.640370</td>\n",
       "      <td>0.079273</td>\n",
       "      <td>0.478097</td>\n",
       "      <td>0.149810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.640416</td>\n",
       "      <td>0.127453</td>\n",
       "      <td>0.506762</td>\n",
       "      <td>0.164290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.641709</td>\n",
       "      <td>0.047348</td>\n",
       "      <td>0.478756</td>\n",
       "      <td>0.140487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.641709</td>\n",
       "      <td>0.047348</td>\n",
       "      <td>0.478756</td>\n",
       "      <td>0.140487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.641709</td>\n",
       "      <td>0.047348</td>\n",
       "      <td>0.478756</td>\n",
       "      <td>0.140487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.642187</td>\n",
       "      <td>0.050840</td>\n",
       "      <td>0.471802</td>\n",
       "      <td>0.156879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.642627</td>\n",
       "      <td>0.049214</td>\n",
       "      <td>0.470249</td>\n",
       "      <td>0.143827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.645904</td>\n",
       "      <td>0.062296</td>\n",
       "      <td>0.469016</td>\n",
       "      <td>0.138473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.646040</td>\n",
       "      <td>0.062422</td>\n",
       "      <td>0.468975</td>\n",
       "      <td>0.138393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.646430</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.474354</td>\n",
       "      <td>0.150508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.646717</td>\n",
       "      <td>0.046677</td>\n",
       "      <td>0.462007</td>\n",
       "      <td>0.140603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.646797</td>\n",
       "      <td>0.053587</td>\n",
       "      <td>0.457952</td>\n",
       "      <td>0.152378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.646864</td>\n",
       "      <td>0.067725</td>\n",
       "      <td>0.459860</td>\n",
       "      <td>0.163237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.646864</td>\n",
       "      <td>0.067725</td>\n",
       "      <td>0.459860</td>\n",
       "      <td>0.163237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.648666</td>\n",
       "      <td>0.058176</td>\n",
       "      <td>0.477834</td>\n",
       "      <td>0.153307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.648898</td>\n",
       "      <td>0.059324</td>\n",
       "      <td>0.471391</td>\n",
       "      <td>0.139342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.649219</td>\n",
       "      <td>0.042964</td>\n",
       "      <td>0.479465</td>\n",
       "      <td>0.150763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.649239</td>\n",
       "      <td>0.034699</td>\n",
       "      <td>0.465314</td>\n",
       "      <td>0.143930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.649239</td>\n",
       "      <td>0.034699</td>\n",
       "      <td>0.465314</td>\n",
       "      <td>0.143930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.649283</td>\n",
       "      <td>0.067529</td>\n",
       "      <td>0.483591</td>\n",
       "      <td>0.155951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.649893</td>\n",
       "      <td>0.041039</td>\n",
       "      <td>0.472109</td>\n",
       "      <td>0.157611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.650680</td>\n",
       "      <td>0.037192</td>\n",
       "      <td>0.465677</td>\n",
       "      <td>0.141426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.651523</td>\n",
       "      <td>0.038624</td>\n",
       "      <td>0.476751</td>\n",
       "      <td>0.147470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.651659</td>\n",
       "      <td>0.034542</td>\n",
       "      <td>0.482488</td>\n",
       "      <td>0.154267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.652860</td>\n",
       "      <td>0.044370</td>\n",
       "      <td>0.477516</td>\n",
       "      <td>0.150077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 10}</th>\n",
       "      <td>-3.655157</td>\n",
       "      <td>0.036885</td>\n",
       "      <td>0.433918</td>\n",
       "      <td>0.146004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.6}</th>\n",
       "      <td>-3.656658</td>\n",
       "      <td>0.045859</td>\n",
       "      <td>0.461504</td>\n",
       "      <td>0.156509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.657424</td>\n",
       "      <td>0.047759</td>\n",
       "      <td>0.472229</td>\n",
       "      <td>0.142556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.657620</td>\n",
       "      <td>0.045304</td>\n",
       "      <td>0.469308</td>\n",
       "      <td>0.162754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.657712</td>\n",
       "      <td>0.050640</td>\n",
       "      <td>0.479741</td>\n",
       "      <td>0.157379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.657725</td>\n",
       "      <td>0.059152</td>\n",
       "      <td>0.456006</td>\n",
       "      <td>0.160748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.658348</td>\n",
       "      <td>0.065244</td>\n",
       "      <td>0.475409</td>\n",
       "      <td>0.150532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 63}</th>\n",
       "      <td>-3.659670</td>\n",
       "      <td>0.050463</td>\n",
       "      <td>0.496013</td>\n",
       "      <td>0.142917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.661018</td>\n",
       "      <td>0.047081</td>\n",
       "      <td>0.474307</td>\n",
       "      <td>0.146209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.662988</td>\n",
       "      <td>0.070651</td>\n",
       "      <td>0.454029</td>\n",
       "      <td>0.160023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.666659</td>\n",
       "      <td>0.047112</td>\n",
       "      <td>0.485328</td>\n",
       "      <td>0.155122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.667068</td>\n",
       "      <td>0.070335</td>\n",
       "      <td>0.477675</td>\n",
       "      <td>0.144457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.667068</td>\n",
       "      <td>0.070335</td>\n",
       "      <td>0.477675</td>\n",
       "      <td>0.144457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.667068</td>\n",
       "      <td>0.070335</td>\n",
       "      <td>0.477675</td>\n",
       "      <td>0.144457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.667478</td>\n",
       "      <td>0.090425</td>\n",
       "      <td>0.463800</td>\n",
       "      <td>0.153872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.667822</td>\n",
       "      <td>0.090319</td>\n",
       "      <td>0.463472</td>\n",
       "      <td>0.153780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.668800</td>\n",
       "      <td>0.070657</td>\n",
       "      <td>0.487825</td>\n",
       "      <td>0.161117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.670821</td>\n",
       "      <td>0.093198</td>\n",
       "      <td>0.496207</td>\n",
       "      <td>0.175865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.671029</td>\n",
       "      <td>0.054229</td>\n",
       "      <td>0.461396</td>\n",
       "      <td>0.152152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.671936</td>\n",
       "      <td>0.039281</td>\n",
       "      <td>0.493608</td>\n",
       "      <td>0.148610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.674063</td>\n",
       "      <td>0.086578</td>\n",
       "      <td>0.477991</td>\n",
       "      <td>0.150742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.676214</td>\n",
       "      <td>0.067438</td>\n",
       "      <td>0.500953</td>\n",
       "      <td>0.144029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.676750</td>\n",
       "      <td>0.050882</td>\n",
       "      <td>0.485039</td>\n",
       "      <td>0.138003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.679226</td>\n",
       "      <td>0.066263</td>\n",
       "      <td>0.505404</td>\n",
       "      <td>0.163564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 20}</th>\n",
       "      <td>-3.680351</td>\n",
       "      <td>0.103033</td>\n",
       "      <td>0.473966</td>\n",
       "      <td>0.191273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.686577</td>\n",
       "      <td>0.099501</td>\n",
       "      <td>0.454637</td>\n",
       "      <td>0.160291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.686744</td>\n",
       "      <td>0.099526</td>\n",
       "      <td>0.454592</td>\n",
       "      <td>0.160110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.688109</td>\n",
       "      <td>0.058804</td>\n",
       "      <td>0.524074</td>\n",
       "      <td>0.148387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.1}</th>\n",
       "      <td>-3.690697</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.481553</td>\n",
       "      <td>0.118551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.690703</td>\n",
       "      <td>0.055788</td>\n",
       "      <td>0.482653</td>\n",
       "      <td>0.130521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.690703</td>\n",
       "      <td>0.055788</td>\n",
       "      <td>0.482653</td>\n",
       "      <td>0.130521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.690703</td>\n",
       "      <td>0.055788</td>\n",
       "      <td>0.482653</td>\n",
       "      <td>0.130521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.690703</td>\n",
       "      <td>0.055788</td>\n",
       "      <td>0.482653</td>\n",
       "      <td>0.130521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.690703</td>\n",
       "      <td>0.055788</td>\n",
       "      <td>0.482653</td>\n",
       "      <td>0.130521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.690703</td>\n",
       "      <td>0.055788</td>\n",
       "      <td>0.482653</td>\n",
       "      <td>0.130521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.690703</td>\n",
       "      <td>0.055788</td>\n",
       "      <td>0.482653</td>\n",
       "      <td>0.130521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.690703</td>\n",
       "      <td>0.055788</td>\n",
       "      <td>0.482653</td>\n",
       "      <td>0.130521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.690703</td>\n",
       "      <td>0.055788</td>\n",
       "      <td>0.482653</td>\n",
       "      <td>0.130521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.691454</td>\n",
       "      <td>0.130802</td>\n",
       "      <td>0.475893</td>\n",
       "      <td>0.165545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.694764</td>\n",
       "      <td>0.044882</td>\n",
       "      <td>0.473028</td>\n",
       "      <td>0.145811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.8}</th>\n",
       "      <td>-3.694966</td>\n",
       "      <td>0.043170</td>\n",
       "      <td>0.467687</td>\n",
       "      <td>0.150113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"23\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.696374</td>\n",
       "      <td>0.054660</td>\n",
       "      <td>0.476014</td>\n",
       "      <td>0.139078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.697978</td>\n",
       "      <td>0.056396</td>\n",
       "      <td>0.486868</td>\n",
       "      <td>0.136296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.699291</td>\n",
       "      <td>0.052608</td>\n",
       "      <td>0.471914</td>\n",
       "      <td>0.145605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.699589</td>\n",
       "      <td>0.053083</td>\n",
       "      <td>0.480378</td>\n",
       "      <td>0.135780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.700802</td>\n",
       "      <td>0.083325</td>\n",
       "      <td>0.468839</td>\n",
       "      <td>0.155949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.701240</td>\n",
       "      <td>0.051443</td>\n",
       "      <td>0.473666</td>\n",
       "      <td>0.136966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.701642</td>\n",
       "      <td>0.057346</td>\n",
       "      <td>0.481233</td>\n",
       "      <td>0.135938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.702039</td>\n",
       "      <td>0.054373</td>\n",
       "      <td>0.485229</td>\n",
       "      <td>0.133405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.702278</td>\n",
       "      <td>0.055888</td>\n",
       "      <td>0.478865</td>\n",
       "      <td>0.146158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.702345</td>\n",
       "      <td>0.052493</td>\n",
       "      <td>0.474740</td>\n",
       "      <td>0.132608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.703042</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>0.498116</td>\n",
       "      <td>0.128021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.703479</td>\n",
       "      <td>0.055890</td>\n",
       "      <td>0.473564</td>\n",
       "      <td>0.136859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.704116</td>\n",
       "      <td>0.056706</td>\n",
       "      <td>0.465925</td>\n",
       "      <td>0.132595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.705124</td>\n",
       "      <td>0.052084</td>\n",
       "      <td>0.477702</td>\n",
       "      <td>0.139675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.705146</td>\n",
       "      <td>0.048680</td>\n",
       "      <td>0.471853</td>\n",
       "      <td>0.138701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.705211</td>\n",
       "      <td>0.054619</td>\n",
       "      <td>0.475238</td>\n",
       "      <td>0.139394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.705608</td>\n",
       "      <td>0.062811</td>\n",
       "      <td>0.468908</td>\n",
       "      <td>0.163708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.706320</td>\n",
       "      <td>0.045525</td>\n",
       "      <td>0.478240</td>\n",
       "      <td>0.135704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.706716</td>\n",
       "      <td>0.057281</td>\n",
       "      <td>0.479390</td>\n",
       "      <td>0.133703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.707234</td>\n",
       "      <td>0.050575</td>\n",
       "      <td>0.478069</td>\n",
       "      <td>0.130530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.707608</td>\n",
       "      <td>0.050041</td>\n",
       "      <td>0.489612</td>\n",
       "      <td>0.138666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.707883</td>\n",
       "      <td>0.061835</td>\n",
       "      <td>0.485465</td>\n",
       "      <td>0.149853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.708211</td>\n",
       "      <td>0.054115</td>\n",
       "      <td>0.484227</td>\n",
       "      <td>0.138786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 100}</th>\n",
       "      <td>-3.708646</td>\n",
       "      <td>0.052578</td>\n",
       "      <td>0.488503</td>\n",
       "      <td>0.137954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.709052</td>\n",
       "      <td>0.052193</td>\n",
       "      <td>0.476818</td>\n",
       "      <td>0.141543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.710624</td>\n",
       "      <td>0.052304</td>\n",
       "      <td>0.483660</td>\n",
       "      <td>0.128863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.710914</td>\n",
       "      <td>0.053398</td>\n",
       "      <td>0.464892</td>\n",
       "      <td>0.143636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.711136</td>\n",
       "      <td>0.054141</td>\n",
       "      <td>0.471084</td>\n",
       "      <td>0.130686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.711810</td>\n",
       "      <td>0.064848</td>\n",
       "      <td>0.466802</td>\n",
       "      <td>0.140540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.713632</td>\n",
       "      <td>0.060928</td>\n",
       "      <td>0.464384</td>\n",
       "      <td>0.140669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.713716</td>\n",
       "      <td>0.061113</td>\n",
       "      <td>0.475898</td>\n",
       "      <td>0.123813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 25}</th>\n",
       "      <td>-3.713767</td>\n",
       "      <td>0.061073</td>\n",
       "      <td>0.476717</td>\n",
       "      <td>0.136099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), GradientBoostingRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.714369</td>\n",
       "      <td>0.107968</td>\n",
       "      <td>0.491822</td>\n",
       "      <td>0.182394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 50}</th>\n",
       "      <td>-3.715209</td>\n",
       "      <td>0.099009</td>\n",
       "      <td>0.506700</td>\n",
       "      <td>0.124892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.715331</td>\n",
       "      <td>0.071188</td>\n",
       "      <td>0.468676</td>\n",
       "      <td>0.137513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.716326</td>\n",
       "      <td>0.057565</td>\n",
       "      <td>0.473003</td>\n",
       "      <td>0.137855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.716330</td>\n",
       "      <td>0.063055</td>\n",
       "      <td>0.476180</td>\n",
       "      <td>0.142266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.717611</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.483339</td>\n",
       "      <td>0.140784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.719087</td>\n",
       "      <td>0.036808</td>\n",
       "      <td>0.482661</td>\n",
       "      <td>0.144154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 1.0}</th>\n",
       "      <td>-3.719681</td>\n",
       "      <td>0.047648</td>\n",
       "      <td>0.474915</td>\n",
       "      <td>0.134263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.724833</td>\n",
       "      <td>0.065898</td>\n",
       "      <td>0.478335</td>\n",
       "      <td>0.133790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.728235</td>\n",
       "      <td>0.061598</td>\n",
       "      <td>0.481078</td>\n",
       "      <td>0.135263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">dict_values([StandardScaler(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.737236</td>\n",
       "      <td>0.111684</td>\n",
       "      <td>0.498190</td>\n",
       "      <td>0.109278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.737236</td>\n",
       "      <td>0.111684</td>\n",
       "      <td>0.498190</td>\n",
       "      <td>0.109278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.737236</td>\n",
       "      <td>0.111684</td>\n",
       "      <td>0.498190</td>\n",
       "      <td>0.109278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), RandomForestRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 10}</th>\n",
       "      <td>-3.740851</td>\n",
       "      <td>0.053977</td>\n",
       "      <td>0.473938</td>\n",
       "      <td>0.136658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"17\" valign=\"top\">dict_values([StandardScaler(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.777193</td>\n",
       "      <td>0.106825</td>\n",
       "      <td>0.428604</td>\n",
       "      <td>0.107087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 50, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.780430</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.128238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.785071</td>\n",
       "      <td>0.126331</td>\n",
       "      <td>0.427741</td>\n",
       "      <td>0.126823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.785629</td>\n",
       "      <td>0.072315</td>\n",
       "      <td>0.490581</td>\n",
       "      <td>0.144449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.785629</td>\n",
       "      <td>0.072315</td>\n",
       "      <td>0.490581</td>\n",
       "      <td>0.144449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.788476</td>\n",
       "      <td>0.106610</td>\n",
       "      <td>0.449004</td>\n",
       "      <td>0.117667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.790754</td>\n",
       "      <td>0.095669</td>\n",
       "      <td>0.433426</td>\n",
       "      <td>0.115918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 2, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.793368</td>\n",
       "      <td>0.083849</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.143894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.794865</td>\n",
       "      <td>0.122039</td>\n",
       "      <td>0.438251</td>\n",
       "      <td>0.125421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 6}</th>\n",
       "      <td>-3.803051</td>\n",
       "      <td>0.060779</td>\n",
       "      <td>0.432516</td>\n",
       "      <td>0.107971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">dict_values([StandardScaler(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.812025</td>\n",
       "      <td>0.090158</td>\n",
       "      <td>0.491998</td>\n",
       "      <td>0.169718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.812025</td>\n",
       "      <td>0.090158</td>\n",
       "      <td>0.491998</td>\n",
       "      <td>0.169718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.812870</td>\n",
       "      <td>0.127387</td>\n",
       "      <td>0.458497</td>\n",
       "      <td>0.119762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 3, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.829674</td>\n",
       "      <td>0.100991</td>\n",
       "      <td>0.487691</td>\n",
       "      <td>0.174611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.883801</td>\n",
       "      <td>0.135435</td>\n",
       "      <td>0.496836</td>\n",
       "      <td>0.146136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.931707</td>\n",
       "      <td>0.154096</td>\n",
       "      <td>0.513077</td>\n",
       "      <td>0.127182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.934478</td>\n",
       "      <td>0.138726</td>\n",
       "      <td>0.508512</td>\n",
       "      <td>0.164795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-3.936575</td>\n",
       "      <td>0.115907</td>\n",
       "      <td>0.506634</td>\n",
       "      <td>0.156856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.936575</td>\n",
       "      <td>0.115907</td>\n",
       "      <td>0.506634</td>\n",
       "      <td>0.156856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 4}</th>\n",
       "      <td>-3.949757</td>\n",
       "      <td>0.056452</td>\n",
       "      <td>0.460496</td>\n",
       "      <td>0.125978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">dict_values([StandardScaler(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-3.980638</td>\n",
       "      <td>0.127521</td>\n",
       "      <td>0.517306</td>\n",
       "      <td>0.169344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-3.985437</td>\n",
       "      <td>0.167675</td>\n",
       "      <td>0.543280</td>\n",
       "      <td>0.125369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-4.000871</td>\n",
       "      <td>0.112683</td>\n",
       "      <td>0.527901</td>\n",
       "      <td>0.155778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 20, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-4.000871</td>\n",
       "      <td>0.112683</td>\n",
       "      <td>0.527901</td>\n",
       "      <td>0.155778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 50}</th>\n",
       "      <td>-4.020827</td>\n",
       "      <td>0.165868</td>\n",
       "      <td>0.534914</td>\n",
       "      <td>0.158730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 3}</th>\n",
       "      <td>-4.090235</td>\n",
       "      <td>0.071966</td>\n",
       "      <td>0.469199</td>\n",
       "      <td>0.107543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 20}</th>\n",
       "      <td>-4.267597</td>\n",
       "      <td>0.193532</td>\n",
       "      <td>0.603344</td>\n",
       "      <td>0.181270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 2}</th>\n",
       "      <td>-4.397137</td>\n",
       "      <td>0.090027</td>\n",
       "      <td>0.523513</td>\n",
       "      <td>0.144930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), DecisionTreeRegressor()])</th>\n",
       "      <th>{'estimator__max_depth': 10, 'estimator__min_samples_leaf': 5, 'estimator__min_samples_split': 5}</th>\n",
       "      <td>-4.397553</td>\n",
       "      <td>0.212128</td>\n",
       "      <td>0.645186</td>\n",
       "      <td>0.196618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), KNeighborsRegressor()])</th>\n",
       "      <th>{'estimator__n_neighbors': 1}</th>\n",
       "      <td>-4.812014</td>\n",
       "      <td>0.078755</td>\n",
       "      <td>0.591860</td>\n",
       "      <td>0.142454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict_values([StandardScaler(), Lasso()])</th>\n",
       "      <th>{'estimator__alpha': 0.01}</th>\n",
       "      <td>-5.258356</td>\n",
       "      <td>0.147121</td>\n",
       "      <td>0.716611</td>\n",
       "      <td>0.167308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">dict_values([StandardScaler(), Ridge()])</th>\n",
       "      <th>{'estimator__alpha': 1.0}</th>\n",
       "      <td>-5.674324</td>\n",
       "      <td>0.203889</td>\n",
       "      <td>0.747942</td>\n",
       "      <td>0.184292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.8}</th>\n",
       "      <td>-5.855230</td>\n",
       "      <td>0.227252</td>\n",
       "      <td>0.765743</td>\n",
       "      <td>0.188165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.6}</th>\n",
       "      <td>-6.096521</td>\n",
       "      <td>0.252987</td>\n",
       "      <td>0.793524</td>\n",
       "      <td>0.186907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.4}</th>\n",
       "      <td>-6.458223</td>\n",
       "      <td>0.301549</td>\n",
       "      <td>0.828741</td>\n",
       "      <td>0.182128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.2}</th>\n",
       "      <td>-7.140921</td>\n",
       "      <td>0.380793</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.157523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.1}</th>\n",
       "      <td>-7.915941</td>\n",
       "      <td>0.429971</td>\n",
       "      <td>1.030900</td>\n",
       "      <td>0.142350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'estimator__alpha': 0.01}</th>\n",
       "      <td>-11.077672</td>\n",
       "      <td>0.804085</td>\n",
       "      <td>1.628760</td>\n",
       "      <td>0.409948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing permutation test on importance; this may take time.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictor</th>\n",
       "      <th>coef</th>\n",
       "      <th>feature_importance</th>\n",
       "      <th>fa_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>ACES_neglectful_parenting*san</td>\n",
       "      <td>None</td>\n",
       "      <td>0.010747</td>\n",
       "      <td>0.010747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BSCS</td>\n",
       "      <td>None</td>\n",
       "      <td>0.010636</td>\n",
       "      <td>0.010636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>ACES_abuse*san</td>\n",
       "      <td>None</td>\n",
       "      <td>0.010211</td>\n",
       "      <td>0.010211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BIS_11</td>\n",
       "      <td>None</td>\n",
       "      <td>0.009116</td>\n",
       "      <td>0.009116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RS</td>\n",
       "      <td>None</td>\n",
       "      <td>0.007943</td>\n",
       "      <td>0.007943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>ACES_neglectful_parenting*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.007780</td>\n",
       "      <td>0.007780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>PLAN_temporal_orientation</td>\n",
       "      <td>None</td>\n",
       "      <td>0.007111</td>\n",
       "      <td>0.007111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ACES_abuse</td>\n",
       "      <td>None</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>0.006684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>PLAN_mental_forecasting</td>\n",
       "      <td>None</td>\n",
       "      <td>0.006547</td>\n",
       "      <td>0.006547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>SRHI_unhealthy</td>\n",
       "      <td>None</td>\n",
       "      <td>0.006543</td>\n",
       "      <td>0.006543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>PLAN_cognitive_strategies</td>\n",
       "      <td>None</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>NCS_satisfaction_in_deliberating</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005920</td>\n",
       "      <td>0.005920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NCS_prefer_little_thought</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.005809</td>\n",
       "      <td>0.005809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>TESQ_E_goal_deliberation*san</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005717</td>\n",
       "      <td>0.005717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EDM</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005678</td>\n",
       "      <td>0.005678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BFI_conscientiousness</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>0.005506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>RTFS_f1_minus_f2*san</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005497</td>\n",
       "      <td>0.005497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>TESQ_E_goal_deliberation</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005186</td>\n",
       "      <td>0.005186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>NCS_intellectual_task*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005183</td>\n",
       "      <td>0.005183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>NCS_thinking_not_fun*ni</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005165</td>\n",
       "      <td>0.005165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/Google Drive/oregon/code/DEV_scripts/analyses/intervention_moderation/dev_interaction_util.py:326: FutureWarning: merging between different levels is deprecated and will be removed in a future version. (2 levels on the left, 1 on the right)\n",
      "  results_vs_cors = final_results_wide.merge(group_correlations, left_index=True, right_index=True, how='outer')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(feature_importance, base)</th>\n",
       "      <th>(feature_importance, ni)</th>\n",
       "      <th>(feature_importance, san)</th>\n",
       "      <th>ichi_cor</th>\n",
       "      <th>ni_cor</th>\n",
       "      <th>san_cor</th>\n",
       "      <th>abs_effect_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACES_neglectful_parenting</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACES_abuse</th>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RTFS_f1_minus_f2</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TESQ_E_goal_deliberation</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BSCS</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_thinking_not_fun</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIS_11</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_relief_not_satisfaction</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RS</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SST_PostErrorSlowW1_mean</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_prefer_little_thought</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRSQ</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACES_sum</th>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAN_mental_forecasting</th>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_satisfaction_in_deliberating</th>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAN_temporal_orientation</th>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMI_perceived_choice</th>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>household_income_per_person</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_solve_puzzles</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCS_prefer_complex</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRHI_unhealthy</th>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDM</th>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode_median_income_acs</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMQ_locomotion</th>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SST_mean_ssrt_0</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCS</th>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model = get_best_model(summarize_overall_df_results(raw_cv_results_list))\n",
    "final_fit = do_final_fit(X=predictor_data_nona, y= outcome_measures_nona['d_bf'], final_model=best_model)\n",
    "final_results = present_model_results(X=predictor_data_nona, final_fit=final_fit,y=outcome_measures_nona['d_bf'])\n",
    "\n",
    "#print rows of final_results where feature_name is the list of features to check\n",
    "base_regressors = interaction_effect_df.predictor[interaction_effect_df.interaction_effect!=0]\n",
    "regressors_to_check = [x+y for y in ['','*ni','*san'] for x in base_regressors]\n",
    "final_results['planned_regression'] = final_results['predictor'].isin(regressors_to_check)\n",
    "\n",
    "\n",
    "present_results_vs_ground_truth_cors(predictor_data_nona,outcome_measures_nona,group_assignments_nona,final_results,base_regressors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do:\n",
    " - figure out and document why we're using different forms of error measurement\n",
    " - try adding feature selection to the pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataanalysis3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "014247d405695287815678bf9349a8dffb2674e9fe9a5bd4bb9820af018d638d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
