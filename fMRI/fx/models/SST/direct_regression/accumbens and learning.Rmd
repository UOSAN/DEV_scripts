---
title: "accumbens_and_learning"
author: "Ben Smith"
date: "2022-10-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We've noticed an accumbens signal that has a particularly pronounced CS-FS difference among late trials.

That led us to ask: is it correlated with reaction time; specifically, 

1) is accumbens BOLD CS-FS contrast at tone+5 s (which reacts negatively) negatively related to longer reaction times in subsequent trials, and 
2) is accumbens bold CS-FS contrast at tone+9 (which reacts positively) positively related to longer reaction times in subsequent trials?

We are really mostly interested in FS so at the trial level we'd do

1) is accumbens BOLD FS contrast at tone+5 s (which reacts positively) positively related to longer reaction times in subsequent trials, and 
2) is accumbens bold FS contrast at tone+9 (which reacts negatively) negatively related to longer reaction times in subsequent trials?


```{r}
library(tidyverse)
Sys.setenv(R_CONFIG_ACTIVE = Sys.info()["nodename"])
print(Sys.info()["nodename"])

```

```{r}
dropbbox_dir <- config::get("dev_analysis_data_dir")

```



```{r}
time_points <- readr::read_csv(paste0(dropbbox_dir,"/SST_roi_by_time_point.csv"))
```

So how do we design this experiment; what are we correlating over, exactly? Some designs are:

1. Across subject, so:
  a. apply the script I have created on late trials to get average measurements every 10 s--maybe the last half or third of the task, not just the last few, but do this at a per subject level
  b. take the MAXIMUM accumbens activity at the 1-9 s range across trials for each subject (to get the peak), and the MINIMUM accumbens activity at the 4-12 s range (to get the trough)
  c. correlate across subject average reaction times, or post-pre change in reaction times

2. Within subject, so:
  a. get peak and trough accumbens signal for each FS trial in the last third of the task. Perhaps use the difference between the peak and trough as your measure. Though we're really now kind of getting a crude HRF...the advantage of this measure is we aren't bound to a particular shape of it, which is probably useful.
  b. get (i) reaction time and (ii) post-pre difference for subsequent go trial
  c. correlate the difference of peak and trough gathered in (a) with the two measures in (b). Summarize in two ways:
    i. get average correlations within-subject, then average across subjects as a crude multi-level model to test across subject
    ii. apply a true multi-level model
    
We'll want to re-use existing pre-post data.

So--this will yield quite different data to the data we used previously!


```{r}
roi_cols <- colnames(time_points)[grepl("harvardoxford",colnames(time_points))]
time_points$tr_roi_mean <- rowMeans(time_points[,roi_cols])
time_points<- 
  time_points %>% group_by(subid,wave) %>%
  mutate(run_mean_across_rois = mean(tr_roi_mean, na.rm = TRUE))

#now mean-center the ROIs
time_points_c<-time_points
for (roi_col in roi_cols){
  time_points_c[,roi_col]<-time_points_c[,roi_col]-time_points_c$run_mean_across_rois
}

roi_cols <- colnames(time_points_c)[grepl("harvardoxford",colnames(time_points_c))]
new_roi_cols <- roi_cols %>% stringr::str_extract_all(pattern = "(?<=harvardoxford-.{0,3}cortical_prob_)(.*)") %>% unlist
colnames(time_points_c)[grepl("harvardoxford",colnames(time_points_c))] <- new_roi_cols

```

We need to group by subject, wave, and trial, select just the FailedStop trials, and just the high trial numbers, then get the min and max accumbens activity. We could even do a long transform across the ROIs, then also group by ROI and get the min and max for ROIs

So let's first long transform the ROIs
```{r}

tpc_long<-time_points_c %>% pivot_longer(cols=all_of(new_roi_cols),names_to="ROI",values_to="value")

```


Now do the grouping and summarizing.

We define a peak period and trough period based on the previously observed shape of the accumbens response in the FS trial. This will put a peak around 2-6 seconds and a trough around 7-10 seconds. These values were chosen subjectively to try to encompass the peak and trough allowing for as much variability as possible without grabbing unrelated data from other time points. With a TR of 2.0 we should always have one or two images to grab from within these ranges, too.

```{r}

FS_summary_data <- tpc_long %>% 
  filter(condition=="FailedStop" & trial_n>196) %>% 
  group_by(subid,wave,trial_n,ROI) %>%
  summarise(
    peak=max(value[offset>=2.0 & offset <=6.0],na.rm = TRUE),
    trough = min(value[offset>=6.0 & offset <=10.0],na.rm = TRUE)
            )

FS_summary_data$response_amplitude <-FS_summary_data$peak-FS_summary_data$trough

ggplot(FS_summary_data,aes(response_amplitude,group=ROI,color=ROI))+geom_density(adjust=0.5)
```

OK. So now, let's try to merge in that with post-pre RTs. Where do we get those from?


## get the SST Post-pre data

```{r}
source(paste0(config::get("ben_dev_data_analysis"),"SST_processing.R"))

dropbox_file_dir = config::get("dev_analysis_data_dir")
sst_all_data_filepath <- paste0(dropbox_file_dir,"sst_behavioral_data_all.csv")

sst_all_data_raw <- readr::read_csv(sst_all_data_filepath)
sst_all_data<-clean_sst_data(sst_all_data_raw)
sst_all_data <- get_sst_post_pre(sst_all_data)
```



```{r}
post_pre_rt_data <- sst_all_data%>% 
  filter(condition!="Cue") %>%
  select(trial_n,reaction_time,subid,waveid,runid,condition,post_pre_rt_change)
```


## Merge the datasets

```{r}

```


```{r}
FS_summary_data$ROI_delateralized <-  FS_summary_data$ROI %>% str_replace_all("(Left|Right)\\s","")


FS_summary_data_delat <- FS_summary_data %>% 
  #filter(ROI %in% c("Left Accumbens","Right Accumbens")) %>% 
  group_by(subid,wave,trial_n, ROI_delateralized) %>%
  summarize(peak=mean(peak),
            trough=mean(trough),
            response_amplitude=mean(response_amplitude))
FS_summary_data_delat <- merge(FS_summary_data_delat,post_pre_rt_data,by.x = c("subid","wave","trial_n"),by.y=c("subid","waveid","trial_n"),all.x = TRUE)
  
  
FS_summary_data_accumbens <- FS_summary_data_delat %>% filter(ROI_delateralized=="Accumbens") %>% select(-ROI_delateralized)

```


#now test. let's trun across all at first....

```{r}
FS_neural_behav_data_not_na<-FS_summary_data_accumbens[
  rowSums(is.na(FS_summary_data_accumbens[,c("response_amplitude","post_pre_rt_change")]))==0 & #no NA values in correlation and
    is.finite(FS_summary_data_accumbens$response_amplitude) & is.finite(FS_summary_data_accumbens$post_pre_rt_change) #no infinite values
    ,]
cor.test(FS_neural_behav_data_not_na$response_amplitude,FS_neural_behav_data_not_na$post_pre_rt_change)
cor.test(FS_neural_behav_data_not_na$peak,FS_neural_behav_data_not_na$post_pre_rt_change)
cor.test(FS_neural_behav_data_not_na$peak,FS_neural_behav_data_not_na$reaction_time)
cor_val<-cor.test(FS_neural_behav_data_not_na$trough,FS_neural_behav_data_not_na$post_pre_rt_change)
print(cor_val)
```



```{r}
FS_by_subj_raw <- FS_neural_behav_data_not_na %>% group_by(subid) %>%
  mutate(trial_count=n())
FS_by_subj <- FS_by_subj_raw %>% filter(trial_count >2) %>%
  summarize(peak_cor_est = cor.test(peak,post_pre_rt_change)$estimate,
            amplitude_cor_est = cor.test(response_amplitude,post_pre_rt_change)$estimate,
            mean_post_pre_rt_change=mean(post_pre_rt_change),
            mean_peak=mean(peak),
            mean_amplitude=mean(response_amplitude)
            )

t.test(FS_by_subj$peak_cor_est)
t.test(FS_by_subj$amplitude_cor_est)


```
OK. So our "amplitude" measure correlates within subjects (p<0.05), when measuring the correlations within subject and then taking the average.

Would strengthen the data

```{r}
hist(FS_by_subj$peak_cor_est,breaks=40)
```
```{r}
hist(FS_by_subj$mean_post_pre_rt_change)
hist(FS_by_subj$mean_peak)
```


```{r}
cor.test(FS_by_subj$mean_post_pre_rt_change,FS_by_subj$mean_peak)

cor.test(FS_by_subj$mean_post_pre_rt_change,FS_by_subj$mean_amplitude)
```


```{r}
cor.test(FS_by_subj$mean_post_pre_rt_change,FS_by_subj$mean_peak,method="spearman")
```

Not significant within subjects, either. OK. We can do a multi-level model but I don't think we're quite capable of pulling this out.


If you take away the accumbens selection, you do get some significant results, but these are taking false precision...

## Now do items other than accumbens.



```{r}

FS_neural_behav_data_not_na<-FS_summary_data_delat[
  rowSums(is.na(FS_summary_data_delat[,c("response_amplitude","post_pre_rt_change")]))==0 & #no NA values in correlation and
    is.finite(FS_summary_data_delat$response_amplitude) & is.finite(FS_summary_data_delat$post_pre_rt_change) #no infinite values
    ,]
```


```{r}

FS_by_subj_raw <- FS_neural_behav_data_not_na %>% group_by(ROI_delateralized, subid) %>%
  mutate(trial_count=n())
FS_by_subj <- FS_by_subj_raw %>% group_by(ROI_delateralized, subid) %>% filter(trial_count >2) %>%
  summarize(peak_cor_est = cor.test(peak,post_pre_rt_change)$estimate,
            amplitude_cor_est = cor.test(response_amplitude,post_pre_rt_change)$estimate,
            mean_post_pre_rt_change=mean(post_pre_rt_change),
            mean_peak=mean(peak),
            mean_amplitude=mean(response_amplitude)
            ) 

```

Per subject:

```{r}


for (roi_i in unique(FS_by_subj$ROI_delateralized)){
  cat("\n")
  cat(roi_i)
  cat("\n")
  print(t.test(FS_by_subj %>% filter(ROI_delateralized==roi_i) %>% .$peak_cor_est))
  print(t.test(FS_by_subj %>% filter(ROI_delateralized==roi_i) %>% .$amplitude_cor_est))
  cat("---\n")
}


```

Look
### across subjects

```{r}


for (roi_i in unique(FS_by_subj$ROI_delateralized)){
  cat("\n")
  cat(roi_i)
  cat("\n")
  FS_by_subj_i <- FS_by_subj %>% filter(ROI_delateralized==roi_i)
  
  print(cor.test(FS_by_subj_i$mean_post_pre_rt_change,FS_by_subj_i$mean_peak))

  print(cor.test(FS_by_subj_i$mean_post_pre_rt_change,FS_by_subj_i$mean_amplitude))
  print(cor.test(FS_by_subj_i$mean_post_pre_rt_change,FS_by_subj_i$mean_peak,method="spearman"))
  
  cat("---\n")
}

```

## Frontal orbital cortex

Of these, the strongest result was the frontal orbital cortex within-subject analysis. This should probably be followed up with a multi-level model.


```{r}
0.0009522*50
```

This result would withstand 50 multiple comparisons...so...I think we are good to interpret this..

Only trouble is that it wasn't really of theoretical interest from the start.

I'm also wary that I ahve tried to interpret strongly "significant results" using this sort of fishing method before and it didn't end up working.

Anyway--let's graph, then multi-level model.

```{r}
ggplot(FS_by_subj %>% filter(ROI_delateralized=="Frontal Orbital Cortex"),
       aes(amplitude_cor_est))+geom_histogram(binwidth=0.2)+
  labs(y="subjects",x="Within-subject FOC amplitude by post-pre RT change correlation")+
  geom_vline(xintercept = 0)

t.test(FS_by_subj %>% filter(ROI_delateralized=="Frontal Orbital Cortex") %>% .$amplitude_cor_est)
```

So we seem to have a within-subject effect, generalizable across subjects.

If we do a multi-level model, with response amplitude as a within-group slope, we need to be able to measure whether the estimate of that within-group slope differs from zero. If we add it as a random effect, I don't think we can do that...?


```{r}


library(lme4)

FS_neural_behav_FOC <- FS_neural_behav_data_not_na %>% filter(ROI_delateralized=="Frontal Orbital Cortex" & trial_n>196)

model_random_effects <- lme4::lmer(
   post_pre_rt_change ~ trial_n + response_amplitude + (1+ response_amplitude | subid),
  FS_neural_behav_FOC
  )
summary(model_random_effects)


```

That's a strongly significant fixed effect, and applies **across** individuals. That's interesting, because my test I did above didn't pick up an across-subject effect. I only got a within-subject effect. 

```{r}
model_fixed_effects <- lme4::lmer(
   post_pre_rt_change ~ trial_n + response_amplitude + (1 | subid),
  FS_neural_behav_FOC
  )
summary(model_fixed_effects)


```


If the random effects model is more predictive than the fixed effects model, we have significant random effects, too.

```{r}
anova(model_fixed_effects,model_random_effects)
```

OK, no significant. That suggests that there is no within-subject effect, but there is a between-subject effect.

One difference is that the earlier estimate looked at just the last portion of the task whereas we're now, in the multi-level model, analyzing all of it.


```{r}
FS_neural_behav_FOC$trial_n_r<-220-FS_neural_behav_FOC$trial_n
model_random_effects2 <- lme4::lmer(
   post_pre_rt_change ~ trial_n_r*response_amplitude + (1+ response_amplitude | subid),
  FS_neural_behav_FOC
  )
summary(model_random_effects2)


```


Questions I need to clear up on multi-level modeling:

(1) random effects are the slopes _within_ each subject, which means that they measure within-subject effects
(2) if a t-test shows those random effects are significantly above or below zero, that would mean there's a systematic effect *within* subjects 
(3) therefore, the fixed-effects measures whether, controlling for within-subject effects, there is a between-subject effect of post-pre-rt-change. right?
(4) is it enough to look at that p-value with the fixed effect to conclude we have a fixed-effect change? I suspect not; I'm inclined to think we ought to do models with and without fixed effects to see how the model changes.
```{r}
library(sp)
#showMethods(class=class(model_random_effects2))
```

```{r}

model_random_effects3 <- lme4::lmer(
   post_pre_rt_change ~ trial_n + response_amplitude + (1+ response_amplitude | subid),
  FS_neural_behav_FOC
  )
summary(model_random_effects3)


```


```{r}

model_random_effects3_no_fixed <- lme4::lmer(
   post_pre_rt_change ~ trial_n + (1+ response_amplitude | subid),
  FS_neural_behav_FOC
  )
summary(model_random_effects3_no_fixed)


```


```{r}
anova(model_random_effects3_no_fixed, model_random_effects3)
```

Well, that would confirm it--we have fixed effects (and therefore an effect _across_ subjects) but not within subjects.