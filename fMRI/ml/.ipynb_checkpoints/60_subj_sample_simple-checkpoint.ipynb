{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "younger-lincoln",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/datasets/__init__.py:87: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltools as nlt\n",
    "import nilearn as nil\n",
    "import nibabel as nib\n",
    "import warnings\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import dev_wtp_io_utils\n",
    "import gc #garbage collection\n",
    "from nilearn import plotting\n",
    "from dev_wtp_io_utils import cv_train_test_sets, asizeof_fmt\n",
    "from sklearn.model_selection import KFold,GroupKFold,LeaveOneOut,LeaveOneGroupOut\n",
    "import os, warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spiritual-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "checked for intersection and no intersection between the brain data and the subjects was found.\n",
      "there were 60 subjects overlapping between the subjects marked for train data and the training dump file itself.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#custom thing I have set in my jupyter notebook task.\n",
    "cpus_available = int(os.getenv('CPUS_PER_TASK'))\n",
    "print(cpus_available)\n",
    "\n",
    "pd.set_option('display.max_rows', 99)\n",
    "\n",
    "\n",
    "test_train_set = pd.read_csv(\"../data/train_test_markers_20210601T183243.csv\")\n",
    "\n",
    "with open('../data/Brain_Data_ns_6s_no_hrf_from_spm_60subs.pkl', 'rb') as pkl_file:\n",
    "    Brain_Data_allsubs = pickle.load(pkl_file)\n",
    "    \n",
    "dev_wtp_io_utils.check_BD_against_test_train_set(Brain_Data_allsubs,test_train_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "#######PRE-PROCESS\n",
    "\n",
    "Brain_Data_allsubs.Y = Brain_Data_allsubs.X.response.copy()\n",
    "print(Brain_Data_allsubs.Y.value_counts())\n",
    "Brain_Data_allsubs.Y[Brain_Data_allsubs.Y=='NULL']=None\n",
    "print(Brain_Data_allsubs.Y.value_counts())\n",
    "\n",
    "import sys\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:60]:\n",
    "    print(name + ': ' + str(size))\n",
    "print(Brain_Data_allsubs.Y.isnull().value_counts())\n",
    "Brain_Data_allsubs_nn = Brain_Data_allsubs[Brain_Data_allsubs.Y.isnull()==False]\n",
    "print(len(Brain_Data_allsubs_nn))\n",
    "print(len(Brain_Data_allsubs))\n",
    "\n",
    "\n",
    "all_subs_nn_nifti = Brain_Data_allsubs_nn.to_nifti()\n",
    "all_subs_nn_nifti_Y = Brain_Data_allsubs_nn.Y\n",
    "all_subs_nn_nifti_groups = Brain_Data_allsubs_nn.X.subject\n",
    "all_subs_nn_nifti_groups\n",
    "\n",
    "\n",
    "all_subs_nn_nifti_metadata = Brain_Data_allsubs_nn.X\n",
    "\n",
    "\n",
    "#################################################\n",
    "#######GET SUB-SET\n",
    "\n",
    "\n",
    "del Brain_Data_allsubs\n",
    "#del Brain_Data_allsubs_grouped\n",
    "gc.collect()\n",
    "\n",
    "from nilearn.decoding import DecoderRegressor\n",
    "dRegressor = DecoderRegressor(estimator = 'ridge_regressor', standardize= True,scoring=\"r2\")\n",
    "\n",
    "\n",
    "asizeof_fmt(Brain_Data_allsubs_nn)\n",
    "\n",
    "asizeof_fmt(all_subs_nn_nifti)\n",
    "\n",
    "print(\"cleaning\")\n",
    "train_y=all_subs_nn_nifti_Y\n",
    "train_X = nil.image.clean_img(all_subs_nn_nifti,detrend=False,standardize=True)\n",
    "train_groups = all_subs_nn_nifti_groups\n",
    "\n",
    "del Brain_Data_allsubs_nn\n",
    "gc.collect()\n",
    "\n",
    "first_subs_nifti_metadata = all_subs_nn_nifti_metadata\n",
    "\n",
    "cv_outer = LeaveOneGroupOut()\n",
    "\n",
    "print(\"finished preprocessing\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "occupational-facial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "earlier-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = DecoderRegressor(standardize= True,cv=LeaveOneGroupOut(),scoring=\"r2\",\n",
    "                                  n_jobs=cpus_available-1,verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-density",
   "metadata": {},
   "source": [
    "This is the simpler alternative to nested CV. We set LOGO CV, and tell the regressor the groups, and let it figure it out.\n",
    "\n",
    "This may overtrain the hyperparameter selection against the data, but if we're not doing much hyper-parameter selection, that's probably OK.\n",
    "\n",
    "When using LOGO, the whole task will take a long time, so it's probably for the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-shelf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NiftiMasker.fit] Loading data from Nifti1Image(\n",
      "shape=(91, 109, 91, 3690),\n",
      "affine=array([[  -2.,    0.,    0.,   90.],\n",
      "       [   0.,    2.,    0., -126.],\n",
      "       [   0.,    0.,    2.,  -72.],\n",
      "       [   0.,    0.,    0.,    1.]])\n",
      ")\n",
      "[NiftiMasker.fit] Computing the mask\n",
      "[NiftiMasker.fit] Resampling mask\n",
      "[NiftiMasker.transform_single_imgs] Loading data from Nifti1Image(\n",
      "shape=(91, 109, 91, 3690),\n",
      "affine=array([[  -2.,    0.,    0.,   90.],\n",
      "       [   0.,    2.,    0., -126.],\n",
      "       [   0.,    0.,    2.,  -72.],\n",
      "       [   0.,    0.,    0.,    1.]])\n",
      ")\n",
      "[NiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[NiftiMasker.transform_single_imgs] Cleaning extracted signals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "#We have put all the CV into the regressor itself. So we will not bother with any further cross-validation.\n",
    "\n",
    "regressor.fit(y=train_y,X=train_X,groups=train_groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-infrared",
   "metadata": {},
   "source": [
    "These are the r^2 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = regressor.cv_scores_['beta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../data/cv_train_test_60subjs_loocv_simple.pkl\", 'wb') as handle:\n",
    "    pickle.dump(scores,handle)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-christopher",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-neuralsignature]",
   "language": "python",
   "name": "conda-env-.conda-neuralsignature-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
