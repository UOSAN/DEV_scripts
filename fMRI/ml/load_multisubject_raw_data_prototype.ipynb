{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imported-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltools as nlt\n",
    "import nilearn as nil\n",
    "import nibabel as nib\n",
    "import warnings\n",
    "import glob\n",
    "import random\n",
    "from sys import getsizeof\n",
    "\n",
    "import pickle\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legendary-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the behavioral data and the list of subjects allocated for training\n",
    "wtpw1_behavdesign_clean = pd.read_csv(\"../data/wtpw1_behavdesign_clean.csv\")\n",
    "test_train_df = pd.read_csv(\"../data/train_test_markers_20210601T183243.csv\")\n",
    "train_subjs = test_train_df.loc[test_train_df.SplitGroup=='Train','sub_label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-uncertainty",
   "metadata": {},
   "source": [
    "A prior load file `load_multisubject_brain_data.ipynb` loaded subjects from betas that are already generated (see  https://docs.google.com/presentation/d/1K-nFrZYE6rR8t0myNyacB7frBzV3B1--nMqPhVkwL8E/edit#slide=id.gd9fcc4129a_0_0 for this process). We're going one level up the chain to look at the raw images those betas were generated from. We want to use the behavioral data to do another extraction from them.\n",
    "\n",
    "This file uses `load_filter_test_train_split` and replaces `load_multisubject_brain_data.ipynb` for loading raw data instead of the betas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-hamilton",
   "metadata": {},
   "source": [
    "Betaseries output files are stored in `/gpfs/projects/sanlab/shared/DEV/nonbids_data/fMRI/fx/models/WTP/wave1/betaseries/sub-DEV049/`, processed by files in `/gpfs/projects/sanlab/shared/DEV/DEV_scripts/fMRI/fx/models/WTP`.\n",
    "\n",
    "These are loaded from folders including: `/projects/sanlab/shared/DEV/bids_data/derivatives/fmriprep/sub-DEV082/ses-wave1/func/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-universe",
   "metadata": {},
   "source": [
    "### Example subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-sentence",
   "metadata": {},
   "source": [
    "For a given subject label, wave, and run, we can load their WTP file....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "satellite-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wtp_filepath_for_run(sub_label,wave,run):\n",
    "    folder_path = (\n",
    "        \"/gpfs/projects/sanlab/shared/DEV/bids_data/derivatives/fmriprep/sub-\"+\n",
    "        sub_label+\n",
    "        \"/ses-wave\"+str(wave)+\"/func/\"\n",
    "    )\n",
    "    filename = (\n",
    "        's6_sub-' + sub_label + '_ses-wave' + str(wave) + \n",
    "        '_task-WTP_acq-' + str(run) +'_bold_space-MNI152NLin2009cAsym_preproc.nii'\n",
    "    )\n",
    "    \n",
    "    return(folder_path+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-missouri",
   "metadata": {},
   "source": [
    "...and we can load it into a brain data file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "conservative-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_label = 'DEV081'\n",
    "subj_raw_data_bd = nlt.Brain_Data(get_wtp_filepath_for_run(sub_label,1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "musical-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_raw_data_nii = nib.load(get_wtp_filepath_for_run(sub_label,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-collins",
   "metadata": {},
   "source": [
    "...but now we have to figure out what to do with that! What sort of pre-processing do we need to do, and how should we do it?\n",
    "\n",
    "We probably don't want to run a haemodynamic convolution because we don't wanna mix up the pre- and post- periods.\n",
    "\n",
    "I think the SPM script does:\n",
    "\n",
    " - high-pass filter\n",
    " - convolution\n",
    " - hrf\n",
    " - serial correlations\n",
    " - ...?\n",
    " \n",
    "What if we just used literally raw data--any scans with a time within the period? If we're skipping hrf convolution...we might as well skip this other stuff? Might need to come back and add it later but let's see how we go.\n",
    "\n",
    "The time period is 4 seconds. Therefore if we get an image taken in the first 2 seconds we can guarantee it was completed before the subject was given a chance to select. The signal will be weak but if it works it'll be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-colony",
   "metadata": {},
   "source": [
    "So going back to our example subject, we get their design data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hungarian-expert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV081\n"
     ]
    }
   ],
   "source": [
    "print(sub_label)\n",
    "subj_behav_design = wtpw1_behavdesign_clean[\n",
    "    (wtpw1_behavdesign_clean.subject==sub_label) &\n",
    "    (wtpw1_behavdesign_clean.wave==1) &\n",
    "    (wtpw1_behavdesign_clean.run=='run1')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-craft",
   "metadata": {},
   "source": [
    "because this has TR=2, and because we want the image taken in the first two seconds of each event, we can just grab the image two seconds after the start of the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sophisticated-chinese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nibabel.nifti1.Nifti1Image at 0x2aaaeb71d730>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_events_from_nii(subj_raw_data_nii, subj_behav_design):\n",
    "    slice_series = []\n",
    "    for event_i, event_r in subj_behav_design.iterrows():\n",
    "        #print(event_r['onset'])\n",
    "        #the event tr is the first TR AFTER the onset; so we need to find the point and round-up\n",
    "\n",
    "        event_tr = (int)(np.ceil(event_r['onset']/2))\n",
    "        if event_tr<subj_raw_data_nii.shape[3]:\n",
    "            slice_series = slice_series + [subj_raw_data_nii.slicer[...,event_tr]]\n",
    "        else:\n",
    "            raise IndexError(\"The behavioral design refers to a slice (\" + str(event_tr) + \") that is not present in the dataset. This may indicate bad data or a truncated run. The dataset has shape: \" + str(subj_raw_data_nii.shape))\n",
    "\n",
    "        #print(event_tr)\n",
    "\n",
    "    event_related_nii = nib.funcs.concat_images(slice_series)\n",
    "    return(event_related_nii)\n",
    "\n",
    "extract_events_from_nii(subj_raw_data_nii, subj_behav_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spare-poland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta</th>\n",
       "      <th>type</th>\n",
       "      <th>task</th>\n",
       "      <th>event_id</th>\n",
       "      <th>isi_pre</th>\n",
       "      <th>onset</th>\n",
       "      <th>duration</th>\n",
       "      <th>food_pic</th>\n",
       "      <th>food_num</th>\n",
       "      <th>cond</th>\n",
       "      <th>health_cond</th>\n",
       "      <th>liking_cond</th>\n",
       "      <th>liking_rating</th>\n",
       "      <th>response</th>\n",
       "      <th>isi_post</th>\n",
       "      <th>end</th>\n",
       "      <th>run</th>\n",
       "      <th>wave</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>beta_0001.nii</td>\n",
       "      <td>run1</td>\n",
       "      <td>WTP betas</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.081474</td>\n",
       "      <td>6.565242</td>\n",
       "      <td>Fritos.bmp</td>\n",
       "      <td>1</td>\n",
       "      <td>unhealthy_liked</td>\n",
       "      <td>unhealthy</td>\n",
       "      <td>liked</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.646716</td>\n",
       "      <td>run1</td>\n",
       "      <td>1</td>\n",
       "      <td>DEV081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>beta_0002.nii</td>\n",
       "      <td>run1</td>\n",
       "      <td>WTP betas</td>\n",
       "      <td>2</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>18.686977</td>\n",
       "      <td>6.523985</td>\n",
       "      <td>CoconutChips.bmp</td>\n",
       "      <td>2</td>\n",
       "      <td>healthy_liked</td>\n",
       "      <td>healthy</td>\n",
       "      <td>liked</td>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.780283</td>\n",
       "      <td>25.210962</td>\n",
       "      <td>run1</td>\n",
       "      <td>1</td>\n",
       "      <td>DEV081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>beta_0003.nii</td>\n",
       "      <td>run1</td>\n",
       "      <td>WTP betas</td>\n",
       "      <td>3</td>\n",
       "      <td>1.780283</td>\n",
       "      <td>30.036748</td>\n",
       "      <td>6.523213</td>\n",
       "      <td>OatmealBites.bmp</td>\n",
       "      <td>3</td>\n",
       "      <td>healthy_liked</td>\n",
       "      <td>healthy</td>\n",
       "      <td>liked</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.095117</td>\n",
       "      <td>36.559961</td>\n",
       "      <td>run1</td>\n",
       "      <td>1</td>\n",
       "      <td>DEV081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>beta_0004.nii</td>\n",
       "      <td>run1</td>\n",
       "      <td>WTP betas</td>\n",
       "      <td>4</td>\n",
       "      <td>0.095117</td>\n",
       "      <td>39.693001</td>\n",
       "      <td>6.526969</td>\n",
       "      <td>JellyBeans.bmp</td>\n",
       "      <td>4</td>\n",
       "      <td>unhealthy_disliked</td>\n",
       "      <td>unhealthy</td>\n",
       "      <td>disliked</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.680966</td>\n",
       "      <td>46.219969</td>\n",
       "      <td>run1</td>\n",
       "      <td>1</td>\n",
       "      <td>DEV081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>beta_0005.nii</td>\n",
       "      <td>run1</td>\n",
       "      <td>WTP betas</td>\n",
       "      <td>5</td>\n",
       "      <td>2.680966</td>\n",
       "      <td>51.942914</td>\n",
       "      <td>6.531816</td>\n",
       "      <td>PeanutButterCup.bmp</td>\n",
       "      <td>5</td>\n",
       "      <td>unhealthy_liked</td>\n",
       "      <td>unhealthy</td>\n",
       "      <td>liked</td>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.319097</td>\n",
       "      <td>58.474729</td>\n",
       "      <td>run1</td>\n",
       "      <td>1</td>\n",
       "      <td>DEV081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10863</th>\n",
       "      <td>beta_0075.nii</td>\n",
       "      <td>run4</td>\n",
       "      <td>WTP betas</td>\n",
       "      <td>12</td>\n",
       "      <td>0.925946</td>\n",
       "      <td>127.198770</td>\n",
       "      <td>6.531133</td>\n",
       "      <td>chilifritos.bmp</td>\n",
       "      <td>12</td>\n",
       "      <td>unhealthy_disliked</td>\n",
       "      <td>unhealthy</td>\n",
       "      <td>disliked</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.670865</td>\n",
       "      <td>133.729903</td>\n",
       "      <td>run4</td>\n",
       "      <td>1</td>\n",
       "      <td>DEV081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11045</th>\n",
       "      <td>beta_0076.nii</td>\n",
       "      <td>run4</td>\n",
       "      <td>WTP betas</td>\n",
       "      <td>13</td>\n",
       "      <td>0.670865</td>\n",
       "      <td>137.447447</td>\n",
       "      <td>6.531707</td>\n",
       "      <td>Kiwi.bmp</td>\n",
       "      <td>13</td>\n",
       "      <td>healthy_disliked</td>\n",
       "      <td>healthy</td>\n",
       "      <td>disliked</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.859947</td>\n",
       "      <td>143.979153</td>\n",
       "      <td>run4</td>\n",
       "      <td>1</td>\n",
       "      <td>DEV081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11227</th>\n",
       "      <td>beta_0077.nii</td>\n",
       "      <td>run4</td>\n",
       "      <td>WTP betas</td>\n",
       "      <td>14</td>\n",
       "      <td>0.859947</td>\n",
       "      <td>147.879711</td>\n",
       "      <td>6.531721</td>\n",
       "      <td>KaleChips.bmp</td>\n",
       "      <td>14</td>\n",
       "      <td>healthy_disliked</td>\n",
       "      <td>healthy</td>\n",
       "      <td>disliked</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.333113</td>\n",
       "      <td>154.411431</td>\n",
       "      <td>run4</td>\n",
       "      <td>1</td>\n",
       "      <td>DEV081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11409</th>\n",
       "      <td>beta_0078.nii</td>\n",
       "      <td>run4</td>\n",
       "      <td>WTP betas</td>\n",
       "      <td>15</td>\n",
       "      <td>0.333113</td>\n",
       "      <td>157.794491</td>\n",
       "      <td>6.531670</td>\n",
       "      <td>VeggieStraws.bmp</td>\n",
       "      <td>15</td>\n",
       "      <td>healthy_liked</td>\n",
       "      <td>healthy</td>\n",
       "      <td>liked</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.421652</td>\n",
       "      <td>164.326160</td>\n",
       "      <td>run4</td>\n",
       "      <td>1</td>\n",
       "      <td>DEV081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11591</th>\n",
       "      <td>beta_0079.nii</td>\n",
       "      <td>run4</td>\n",
       "      <td>WTP betas</td>\n",
       "      <td>16</td>\n",
       "      <td>1.421652</td>\n",
       "      <td>168.794299</td>\n",
       "      <td>6.531507</td>\n",
       "      <td>crackerjacks.bmp</td>\n",
       "      <td>16</td>\n",
       "      <td>unhealthy_liked</td>\n",
       "      <td>unhealthy</td>\n",
       "      <td>liked</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>175.325807</td>\n",
       "      <td>run4</td>\n",
       "      <td>1</td>\n",
       "      <td>DEV081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                beta  type       task  event_id   isi_pre       onset  \\\n",
       "125    beta_0001.nii  run1  WTP betas         1  3.000000    6.081474   \n",
       "307    beta_0002.nii  run1  WTP betas         2  3.000000   18.686977   \n",
       "489    beta_0003.nii  run1  WTP betas         3  1.780283   30.036748   \n",
       "671    beta_0004.nii  run1  WTP betas         4  0.095117   39.693001   \n",
       "853    beta_0005.nii  run1  WTP betas         5  2.680966   51.942914   \n",
       "...              ...   ...        ...       ...       ...         ...   \n",
       "10863  beta_0075.nii  run4  WTP betas        12  0.925946  127.198770   \n",
       "11045  beta_0076.nii  run4  WTP betas        13  0.670865  137.447447   \n",
       "11227  beta_0077.nii  run4  WTP betas        14  0.859947  147.879711   \n",
       "11409  beta_0078.nii  run4  WTP betas        15  0.333113  157.794491   \n",
       "11591  beta_0079.nii  run4  WTP betas        16  1.421652  168.794299   \n",
       "\n",
       "       duration             food_pic  food_num                cond  \\\n",
       "125    6.565242           Fritos.bmp         1     unhealthy_liked   \n",
       "307    6.523985     CoconutChips.bmp         2       healthy_liked   \n",
       "489    6.523213     OatmealBites.bmp         3       healthy_liked   \n",
       "671    6.526969       JellyBeans.bmp         4  unhealthy_disliked   \n",
       "853    6.531816  PeanutButterCup.bmp         5     unhealthy_liked   \n",
       "...         ...                  ...       ...                 ...   \n",
       "10863  6.531133      chilifritos.bmp        12  unhealthy_disliked   \n",
       "11045  6.531707             Kiwi.bmp        13    healthy_disliked   \n",
       "11227  6.531721        KaleChips.bmp        14    healthy_disliked   \n",
       "11409  6.531670     VeggieStraws.bmp        15       healthy_liked   \n",
       "11591  6.531507     crackerjacks.bmp        16     unhealthy_liked   \n",
       "\n",
       "      health_cond liking_cond  liking_rating  response  isi_post         end  \\\n",
       "125     unhealthy       liked              4       NaN  3.000000   12.646716   \n",
       "307       healthy       liked              4       8.0  1.780283   25.210962   \n",
       "489       healthy       liked              4       5.0  0.095117   36.559961   \n",
       "671     unhealthy    disliked              1       5.0  2.680966   46.219969   \n",
       "853     unhealthy       liked              4       8.0  0.319097   58.474729   \n",
       "...           ...         ...            ...       ...       ...         ...   \n",
       "10863   unhealthy    disliked              1       5.0  0.670865  133.729903   \n",
       "11045     healthy    disliked              1       5.0  0.859947  143.979153   \n",
       "11227     healthy    disliked              1       5.0  0.333113  154.411431   \n",
       "11409     healthy       liked              4       7.0  1.421652  164.326160   \n",
       "11591   unhealthy       liked              3       6.0       NaN  175.325807   \n",
       "\n",
       "        run  wave subject  \n",
       "125    run1     1  DEV081  \n",
       "307    run1     1  DEV081  \n",
       "489    run1     1  DEV081  \n",
       "671    run1     1  DEV081  \n",
       "853    run1     1  DEV081  \n",
       "...     ...   ...     ...  \n",
       "10863  run4     1  DEV081  \n",
       "11045  run4     1  DEV081  \n",
       "11227  run4     1  DEV081  \n",
       "11409  run4     1  DEV081  \n",
       "11591  run4     1  DEV081  \n",
       "\n",
       "[64 rows x 19 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtpw1_behavdesign_clean[wtpw1_behavdesign_clean.subject=='DEV081']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "invisible-literature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(wtpw1_behavdesign_clean.subject=='DEV081')\n",
    "#def get_raw_betas(run_data,betas):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-stephen",
   "metadata": {},
   "source": [
    "Now let's put it together -- create one function that will run through that subject again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "closing-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_related_Brain_Data_for_sub_run(subj_label,wave,run,all_behav_design):\n",
    "    #load the raw run file\n",
    "    subj_raw_data_nii = nib.load(get_wtp_filepath_for_run(sub_label,1,1))\n",
    "    \n",
    "    #subset the behavioral data\n",
    "    subj_behav_design = all_behav_design[\n",
    "        (all_behav_design.subject==sub_label) &\n",
    "        (all_behav_design.wave==1) &\n",
    "        (all_behav_design.run=='run' + str(run))\n",
    "    ]\n",
    "    \n",
    "    #go through the event file an extract the appropriate nii for each event\n",
    "    event_related_nii = extract_events_from_nii(subj_raw_data_nii, subj_behav_design)\n",
    "    \n",
    "    #if as_Brain_Data:\n",
    "    #create the nlt brain_Data file\n",
    "    event_related_bd = nlt.Brain_Data(event_related_nii)\n",
    "    event_related_bd.X = subj_behav_design\n",
    "    return(event_related_bd)\n",
    "    \n",
    "    #return(event_related_nii)\n",
    "    \n",
    "    \n",
    "sub_041 = get_event_related_Brain_Data_for_sub_run('DEV041',1,1,wtpw1_behavdesign_clean)\n",
    "    \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-connectivity",
   "metadata": {},
   "source": [
    "Now let's expand that one more level by looping through all of a subject's runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "continent-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_related_Brain_Data_for_sub_all_runs(subj_label,wave,all_behav_design):\n",
    "    event_related_run_list = []\n",
    "    #loop through each run\n",
    "    for run in [1,2,3,4]:\n",
    "        event_related_nii_run = (\n",
    "            get_event_related_Brain_Data_for_sub_run(\n",
    "                subj_label,wave,run, all_behav_design)\n",
    "        )\n",
    "        event_related_run_list = event_related_run_list + [event_related_nii_run]\n",
    "    \n",
    "    #concatenate the data from each run into a single file\n",
    "    subj_run_Brain_Data = nlt.utils.concatenate(event_related_run_list)\n",
    "    \n",
    "    return(subj_run_Brain_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-capability",
   "metadata": {},
   "source": [
    "### Load all subjects - attempt 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-think",
   "metadata": {},
   "source": [
    "...and one level further still, by iterating through a list of subjects from the training set list."
   ]
  },
  {
   "cell_type": "raw",
   "id": "aware-storm",
   "metadata": {},
   "source": [
    "training_Brain_Data_list = []\n",
    "for sub_label in train_subjs:\n",
    "    print(sub_label)\n",
    "    subj_data = get_event_related_Brain_Data_for_sub_all_runs(sub_label,1,wtpw1_behavdesign_clean)\n",
    "    training_Brain_Data_list = training_Brain_Data_list + [subj_data]\n",
    "\n",
    "    \n",
    "training_Brain_Data = nlt.utils.concatenate(training_Brain_Data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-sense",
   "metadata": {},
   "source": [
    "This takes a long time, because it iterates through and runs the Brain_Data operation (which is quite slow) for every subject. That's unnecessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-dutch",
   "metadata": {},
   "source": [
    "### Reorganising this: loading subjects attempt 2\n",
    "\n",
    "This is too slow and messy. We're going to do another way that iterates through all the data BEFORE creating the bd file. This may end up faster because wrapping the nifti in the brain data format is the slowest step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "automatic-warning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_related_Brain_Data_for_all_subs_all_runs_fast(subj_list, wave,all_behav_design):\n",
    "    \"\"\"\n",
    "    Developed in load_multisubject_raw_data.ipynb. Gets raw brain data from raw files and concatenates into a Brain_Data file.\n",
    "    \"\"\"\n",
    "    training_data_list = []\n",
    "    behavioral_design_list_in_order = []\n",
    "    for sub_label in subj_list:\n",
    "        print(sub_label + \" (\",end='',flush=True)\n",
    "\n",
    "        #def get_event_related_Brain_Data_for_sub_all_runs(subj_label,wave,all_behav_design):\n",
    "        #loop through each run\n",
    "        for run in [1,2,3,4]:\n",
    "            print(str(run) + \" \", end='',flush=True)\n",
    "            #def get_event_related_Brain_Data_for_sub_run(subj_label,wave,run,all_behav_design):\n",
    "            #load the raw run file\n",
    "            raw_filepath = get_wtp_filepath_for_run(sub_label,wave,run)\n",
    "            #print(raw_filepath)\n",
    "            subj_raw_data_nii = nib.load(raw_filepath)\n",
    "\n",
    "            #subset the behavioral data\n",
    "            subj_behav_design = all_behav_design[\n",
    "                (all_behav_design.subject==sub_label) &\n",
    "                (all_behav_design.wave==wave) &\n",
    "                (all_behav_design.run=='run' + str(run))\n",
    "            ]\n",
    "            \n",
    "            \n",
    "            #go through the event file and extract the appropriate nii for each event\n",
    "            try:\n",
    "                \n",
    "                event_related_nii = extract_events_from_nii(subj_raw_data_nii, subj_behav_design)\n",
    "                \n",
    "                #this is a good place to convert the data from 64 to 32bit. we don't need 64-bit float. might only need 16-bit\n",
    "                #https://stackoverflow.com/questions/44397617/change-data-type-in-numpy-and-nibabel/45589431\n",
    "#                 event_related_nii_32b = event_related_nii\n",
    "#                 event_related_nii_32b\n",
    "#                 event_related_nii.get_fdata().astype(np.float32)\n",
    "                training_data_list = training_data_list + [event_related_nii]\n",
    "                behavioral_design_list_in_order= behavioral_design_list_in_order + [subj_behav_design]\n",
    "            except IndexError:\n",
    "                print(\"For subject \" + sub_label + \", run \" + str(run) + \", there was a mismatch between behavioral and data. Skipping this run.\")\n",
    "                \n",
    "        print(\")\")\n",
    "                \n",
    "            \n",
    "    print(\"extracted all data. concatenating...\",flush=True)\n",
    "    #concatenate the data from each run into a single file\n",
    "    all_nii = nib.funcs.concat_images(training_data_list,axis=3)\n",
    "    del training_data_list\n",
    "    print(\"...concatenated.\",flush=True)\n",
    "    behavioral_design = pd.concat(behavioral_design_list_in_order)\n",
    "    behavioral_design.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    print(\"combining into a Brain_Data file....\",flush=True)\n",
    "    #combine as a Brain_Data file.\n",
    "    all_bd = nlt.Brain_Data(all_nii)\n",
    "    all_bd.X = behavioral_design\n",
    "    print(\"...done.\",flush=True)\n",
    "    \n",
    "    return(all_bd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "balanced-checklist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV001 (1 2 3 4 )\n",
      "DEV005 (1 2 3 4 )\n",
      "DEV006 (1 2 3 4 )\n",
      "DEV009 (1 2 3 4 )\n",
      "DEV010 (1 2 3 4 )\n",
      "extracted all data. concatenating...\n",
      "combining into a Brain_Data file....\n",
      "...done.\n"
     ]
    }
   ],
   "source": [
    "training_Brain_Data_5 = get_event_related_Brain_Data_for_all_subs_all_runs_fast(train_subjs[0:5],1,wtpw1_behavdesign_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "artificial-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_Brain_Data_20 = get_event_related_Brain_Data_for_all_subs_all_runs_fast(train_subjs[0:20],1,wtpw1_behavdesign_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "minute-police",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV001 (1 2 3 4 )\n",
      "DEV005 (1 2 3 4 )\n",
      "DEV006 (1 2 3 4 )\n",
      "DEV009 (1 2 3 4 )\n",
      "DEV010 (1 2 3 4 )\n",
      "DEV012 (1 2 3 4 )\n",
      "DEV013 (1 2 3 4 )\n",
      "DEV014 (1 2 3 4 )\n",
      "DEV015 (1 2 3 4 )\n",
      "DEV016 (1 2 3 4 )\n",
      "DEV017 (1 2 3 4 )\n",
      "DEV018 (1 2 3 4 )\n",
      "DEV019 (1 2 3 4 )\n",
      "DEV021 (1 2 3 4 )\n",
      "DEV022 (1 2 3 4 )\n",
      "DEV023 (1 2 3 4 )\n",
      "DEV024 (1 2 3 4 )\n",
      "DEV025 (1 2 For subject DEV025, run 2, there was a mismatch between behavioral and data. Skipping this run.\n",
      "3 4 )\n",
      "DEV026 (1 2 3 4 )\n",
      "DEV027 (1 2 3 4 )\n",
      "DEV028 (1 2 3 4 )\n",
      "DEV029 (1 2 3 4 )\n",
      "DEV030 (1 2 3 4 )\n",
      "DEV034 (1 2 3 4 )\n",
      "DEV035 (1 2 3 4 )\n",
      "DEV036 (1 2 3 4 )\n",
      "DEV039 (1 2 3 4 )\n",
      "DEV040 (1 2 3 4 )\n",
      "DEV041 (1 2 3 4 )\n",
      "DEV042 (1 2 3 4 )\n",
      "DEV043 (1 2 3 4 )\n",
      "DEV046 (1 2 3 4 )\n",
      "DEV048 (1 2 3 4 )\n",
      "DEV049 (1 2 3 4 )\n",
      "DEV050 (1 2 3 4 )\n",
      "DEV051 (1 2 3 4 )\n",
      "DEV052 (1 2 3 4 )\n",
      "DEV053 (1 2 3 4 )\n",
      "DEV055 (1 2 3 4 )\n",
      "DEV056 (1 2 3 4 )\n",
      "DEV057 (1 2 3 4 )\n",
      "DEV058 (1 2 3 4 )\n",
      "DEV059 (1 2 3 4 )\n",
      "DEV060 (1 2 3 4 )\n",
      "DEV061 (1 2 3 4 )\n",
      "DEV062 (1 2 3 4 )\n",
      "DEV068 (1 2 3 4 )\n",
      "DEV069 (1 2 3 For subject DEV069, run 3, there was a mismatch between behavioral and data. Skipping this run.\n",
      "4 )\n",
      "DEV071 (1 2 3 4 )\n",
      "DEV073 (1 2 3 4 )\n",
      "DEV074 (1 2 3 4 )\n",
      "DEV076 (1 2 3 4 )\n",
      "DEV077 (1 2 3 4 )\n",
      "DEV079 (1 2 3 4 )\n",
      "DEV083 (1 2 3 4 )\n",
      "DEV084 (1 2 3 4 )\n",
      "DEV085 (1 For subject DEV085, run 1, there was a mismatch between behavioral and data. Skipping this run.\n",
      "2 3 4 )\n",
      "DEV086 (1 2 3 4 )\n",
      "DEV087 (1 2 3 4 )\n",
      "DEV089 (1 2 3 4 )\n",
      "extracted all data. concatenating...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 30.6 GiB for an array with shape (97, 115, 97, 3792) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5781397b17c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_Brain_Data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_event_related_Brain_Data_for_all_subs_all_runs_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwtpw1_behavdesign_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-26b57598038c>\u001b[0m in \u001b[0;36mget_event_related_Brain_Data_for_all_subs_all_runs_fast\u001b[0;34m(subj_list, wave, all_behav_design)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"extracted all data. concatenating...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#concatenate the data from each run into a single file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mall_nii\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuncs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mbehavioral_design\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbehavioral_design_list_in_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mbehavioral_design\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neuralsignature/lib/python3.8/site-packages/nibabel/funcs.py\u001b[0m in \u001b[0;36mconcat_images\u001b[0;34m(images, check_affines, axis)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mout_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mout_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 30.6 GiB for an array with shape (97, 115, 97, 3792) and data type float64"
     ]
    }
   ],
   "source": [
    "training_Brain_Data = get_event_related_Brain_Data_for_all_subs_all_runs_fast(train_subjs,1,wtpw1_behavdesign_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "\n",
    "plotting.plot_stat_map(training_Brain_Data_20.mean().to_nifti())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vocal-mapping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.310730384"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "getsizeof(training_Brain_Data_5.to_nifti().get_fdata())/ math.pow(10,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-mouth",
   "metadata": {},
   "source": [
    "That's about 2.3 GB for 5 subjects. So for 60 subjects, we can expect\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "blind-teacher",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.728764608"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getsizeof(training_Brain_Data_5.to_nifti().get_fdata()) /5 * 60 / math.pow(10,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-nashville",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-bundle",
   "metadata": {},
   "source": [
    "Now we have a complete set of data based on raw extractions of events that have not had high pass filter or HRF applied.\n",
    "\n",
    "We might really need to work out how to apply high-pass filter, but I think extracting without application of HRF is probably the right approach.\n",
    "\n",
    "It seems haphazard. If the data was anything but a 4 second event with 2 second TRs where we want the last TR that doesn't go into the choice phase, we'd have a problem. But because that's precisely what we want, this code will grab exactly the right data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/Brain_Data_75subs.pkl', 'rb') as pkl_file:\n",
    "    Brain_Data_allsubs = pickle.load(pkl_file)\n",
    "    \n",
    "with open('../data/Brain_Data_raw_5subs.pkl', 'rb') as pkl_file:\n",
    "    Brain_Data_raw_allsubs = pickle.load(pkl_file)\n",
    "    \n",
    "Brain_Data_raw_allsubs.dtype()\n",
    "\n",
    "Brain_Data_allsubs.dtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/Brain_Data_raw_20subs.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(training_Brain_Data_20,pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "wrong-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/Brain_Data_raw_5subs.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(training_Brain_Data_5,pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wrong-whale",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_Brain_Data_5.dtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "grave-amazon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_Brain_Data_5.to_nifti().get_fdata().astype(np.float32).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "stuffed-print",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "test_nifti=training_Brain_Data_5.to_nifti()\n",
    "print(test_nifti.get_fdata().dtype)\n",
    "#test_nifti.get_fdata().astype(np.float32)\n",
    "test_nifti.header.set_data_dtype(np.float64)\n",
    "test_nifti_out = nib.Nifti1Image(\n",
    "    test_nifti.get_fdata().astype(np.float64),\n",
    "    test_nifti.affine,\n",
    "    header=test_nifti.header)\n",
    "print(test_nifti_out.get_fdata().dtype)\n",
    "#print(test_nifti.get_fdata().dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cultural-teens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
       "        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
       "        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,\n",
       "        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,\n",
       "        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,\n",
       "        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n",
       "        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,\n",
       "        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,\n",
       "        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,\n",
       "        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,\n",
       "       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,\n",
       "       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,\n",
       "       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,\n",
       "       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,\n",
       "       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,\n",
       "       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,\n",
       "       176., 177., 178., 179., 180., 181., 182., 183., 184., 185., 186.,\n",
       "       187., 188., 189., 190., 191., 192., 193., 194., 195., 196., 197.,\n",
       "       198., 199., 200., 201., 202., 203., 204., 205., 206., 207., 208.,\n",
       "       209., 210., 211., 212., 213., 214., 215., 216., 217., 218., 219.,\n",
       "       220., 221., 222., 223., 224., 225., 226., 227., 228., 229., 230.,\n",
       "       231., 232., 233., 234., 235., 236., 237., 238., 239., 240., 241.,\n",
       "       242., 243., 244., 245., 246., 247., 248., 249., 250., 251., 252.,\n",
       "       253., 254., 255.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_nifti_out.get_fdata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "indonesian-ceiling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "datatyped32=test_nifti.get_fdata().astype(np.float32)\n",
    "print(datatyped32.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-learning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "satisfied-dollar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nibabel.nifti1.Nifti1Header'> object, endian='<'\n",
      "sizeof_hdr      : 348\n",
      "data_type       : b''\n",
      "db_name         : b''\n",
      "extents         : 0\n",
      "session_error   : 0\n",
      "regular         : b''\n",
      "dim_info        : 0\n",
      "dim             : [  4  91 109  91 320   1   1   1]\n",
      "intent_p1       : 0.0\n",
      "intent_p2       : 0.0\n",
      "intent_p3       : 0.0\n",
      "intent_code     : none\n",
      "datatype        : float32\n",
      "bitpix          : 32\n",
      "slice_start     : 0\n",
      "pixdim          : [-1.  2.  2.  2.  1.  1.  1.  1.]\n",
      "vox_offset      : 0.0\n",
      "scl_slope       : nan\n",
      "scl_inter       : nan\n",
      "slice_end       : 0\n",
      "slice_code      : unknown\n",
      "xyzt_units      : 0\n",
      "cal_max         : 0.0\n",
      "cal_min         : 0.0\n",
      "slice_duration  : 0.0\n",
      "toffset         : 0.0\n",
      "glmax           : 0\n",
      "glmin           : 0\n",
      "descrip         : b''\n",
      "aux_file        : b''\n",
      "qform_code      : unknown\n",
      "sform_code      : aligned\n",
      "quatern_b       : 0.0\n",
      "quatern_c       : 1.0\n",
      "quatern_d       : 0.0\n",
      "qoffset_x       : 90.0\n",
      "qoffset_y       : -126.0\n",
      "qoffset_z       : -72.0\n",
      "srow_x          : [-2.  0.  0. 90.]\n",
      "srow_y          : [   0.    2.    0. -126.]\n",
      "srow_z          : [  0.   0.   2. -72.]\n",
      "intent_name     : b''\n",
      "magic           : b'n+1'\n"
     ]
    }
   ],
   "source": [
    "print(test_nifti.header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-smell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "natural-finance",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getsizeof' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bc0e45ddd620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetsizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_Brain_Data_5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_nifti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'getsizeof' is not defined"
     ]
    }
   ],
   "source": [
    "getsizeof(training_Brain_Data_5.to_nifti().get_fdata()) /5 * 60 / math.pow(10,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-screw",
   "metadata": {},
   "source": [
    "### Standalone script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "residential-excuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/datasets/__init__.py:87: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV001 (1 2 3 4 )\n",
      "DEV005 (1 2 3 4 )\n",
      "DEV006 (1 2 3 4 )\n",
      "DEV009 (1 2 3 4 )\n",
      "DEV010 (1 2 3 4 )\n",
      "extracted all data. concatenating...\n",
      "combining into a Brain_Data file....\n",
      "...done.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltools as nlt\n",
    "import nilearn as nil\n",
    "import nibabel as nib\n",
    "import warnings\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "from operator import itemgetter\n",
    "import dev_wtp_io_utils\n",
    "\n",
    "#get the behavioral data and the list of subjects allocated for training\n",
    "wtpw1_behavdesign_clean = pd.read_csv(\"../data/wtpw1_behavdesign_clean.csv\")\n",
    "test_train_df = pd.read_csv(\"../data/train_test_markers_20210601T183243.csv\")\n",
    "train_subjs = test_train_df.loc[test_train_df.SplitGroup=='Train','sub_label'].tolist()\n",
    "\n",
    "training_Brain_Data_5 = dev_wtp_io_utils.get_event_related_Brain_Data_for_all_subs_all_runs_fast(train_subjs[0:5],1,wtpw1_behavdesign_clean)\n",
    "\n",
    "with open('../data/Brain_Data_raw_5subs.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(training_Brain_Data_5,pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-channel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-arrow",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-neuralsignature]",
   "language": "python",
   "name": "conda-env-.conda-neuralsignature-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
