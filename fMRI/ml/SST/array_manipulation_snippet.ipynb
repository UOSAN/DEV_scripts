{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying trialwise CorrectGo and NoGo trials\n",
    "\n",
    "There are a number of steps to this. Hopefully we can recycle previous code and be up fairly quickly!\n",
    "\n",
    "1. Load beta data. Ideally this process should include a cache into a pure python object so we don't have to reload it each time.\n",
    "2. Preprocess the data.\n",
    "3. Do cross-validated training and testing. Ideally an inner loop to select best parameters, an outer loop to get cross-validated performance, and final training over all the data to get an image. The inner loop can be probably be handled within the package we use probably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import yaml\n",
    "hostname=socket.gethostname()\n",
    "hostname='zzz'\n",
    "with open('sst_config.yml', \"r\") as f:\n",
    "    test_config= yaml.safe_load(f)#[hostname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python initialized for apply_loocv_and_save\n",
      "cpus available; cpus to use:\n",
      "10 9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../../ml/\"))\n",
    "from apply_loocv_and_save import load_and_preprocess\n",
    "from dev_utils import read_yaml_for_host\n",
    "import warnings\n",
    "\n",
    "\n",
    "config_data = read_yaml_for_host(\"sst_config.yml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import math\n",
    "import nibabel as nib\n",
    "import nilearn as nl\n",
    "from nilearn.decoding import DecoderRegressor,Decoder\n",
    "from sklearn.model_selection import KFold,GroupKFold,LeaveOneOut\n",
    "cpus_available = multiprocessing.cpu_count()\n",
    "\n",
    "cpus_to_use = min(cpus_available-1,math.floor(0.9*cpus_available))\n",
    "print(cpus_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dev_wtp_io_utils import cv_train_test_sets, asizeof_fmt\n",
    "from nilearn.decoding import DecoderRegressor,Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonbids_data_path = config_data['nonbids_data_path']\n",
    "ml_data_folderpath = nonbids_data_path + \"fMRI/ml\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trialtype_resp_trans_func(X):\n",
    "    return(X.trial_type)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading beta data\n",
    "\n",
    "beta data is generally written in `load_multisubject_brain_data_sst_w1.ipynb`.\n",
    "\n",
    "We just have to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cj/4mb6t1f906j397tj71pxfxz00000gn/T/ipykernel_28423/2753699166.py:3: UserWarning: not sure if this file holds up--it was created in 2021; need to see if it's still valid\n",
      "  warnings.warn(\"not sure if this file holds up--it was created in 2021; need to see if it's still valid\")\n"
     ]
    }
   ],
   "source": [
    "#brain_data_filepath = ml_data_folderpath + '/SST/Brain_Data_betaseries_30subs_correct_cond_pfc.pkl'\n",
    "brain_data_filepath = ml_data_folderpath + '/SST/Brain_Data_betaseries_6subs_correct_cond.pkl'\n",
    "warnings.warn(\"not sure if this file holds up--it was created in 2021; need to see if it's still valid\")\n",
    "train_test_markers_filepath = ml_data_folderpath + \"/train_test_markers_20220818T144138.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked for intersection and no intersection between the brain data and the subjects was found.\n",
      "there were 6 subjects overlapping between the subjects marked for train data and the training dump file itself.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/opt/anaconda3/envs/neuralsignature/lib/python3.10/site-packages/nilearn/input_data/__init__.py:27: FutureWarning: The import path 'nilearn.input_data' is deprecated in version 0.9. Importing from 'nilearn.input_data' will be possible at least until release 0.13.0. Please import from 'nilearn.maskers' instead.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_train_set: 62918\n",
      "brain_data_filepath: 168\n",
      "pkl_file: 168\n",
      "train_test_markers_filepath: 158\n",
      "response_transform_func: 144\n",
      "sys: 72\n",
      "Brain_Data_allsubs: 48\n",
      "clean: 16\n",
      "subjs_to_use: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminsmith/Google Drive/oregon/code/DEV_scripts/fMRI/ml/apply_loocv_and_save.py:217: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Brain_Data_allsubs.Y[Brain_Data_allsubs.Y=='NULL']=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601\n",
      "601\n",
      "cleaning memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cj/4mb6t1f906j397tj71pxfxz00000gn/T/ipykernel_28423/1474138623.py:8: UserWarning: the data hasn't been cleaned at any point. the fMRIPrep cleaning pipeline has been applied; nothing else has been.\n",
      "  warnings.warn(\"the data hasn't been cleaned at any point. the fMRIPrep cleaning pipeline has been applied; nothing else has been.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "all_subjects = load_and_preprocess(\n",
    "    brain_data_filepath,\n",
    "    train_test_markers_filepath,\n",
    "    subjs_to_use = None,\n",
    "    response_transform_func = trialtype_resp_trans_func,\n",
    "    clean=None)\n",
    "\n",
    "warnings.warn(\"the data hasn't been cleaned at any point. the fMRIPrep cleaning pipeline has been applied; nothing else has been.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile,f_classif\n",
    "\n",
    "#convert the y array to an integer array representing the string values of the y array\n",
    "all_subjects['y_cat'] = all_subjects['y'].astype('category')\n",
    "all_subjects['y_int']=all_subjects['y_cat'].cat.codes\n",
    "\n",
    "# #get the X fdata\n",
    "# x_fdata = all_subjects['X'].get_fdata()\n",
    "# print(x_fdata.shape)\n",
    "# #flatten first 3 dims of X fdata to a single dim\n",
    "# x_fdata_1d = x_fdata.reshape([np.prod(x_fdata.shape[0:3]), x_fdata.shape[3]])\n",
    "# print(x_fdata_1d.shape)\n",
    "\n",
    "\n",
    "\n",
    "# #SelectPercentile(f_classif,percentile=5).fit_transform(,all_subjects['y_int'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_nifti = nib.load(ml_data_folderpath + '/prefrontal_cortex.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subjs = 6\n",
    "if num_subjs<len(all_subjects['metadata']['subject'].unique()):\n",
    "    #cut down to just 9 subjects\n",
    "    selected_subjs={}\n",
    "    print(all_subjects.keys())\n",
    "\n",
    "    #select subjs\n",
    "    subjs = all_subjects['metadata']['subject'].unique()\n",
    "    subjs.sort()\n",
    "    selected_sub_ids=subjs[0:num_subjs]\n",
    "    #get index of selected subjs\n",
    "    selected_subjs_idx_bool = all_subjects['metadata']['subject'].isin(selected_sub_ids)\n",
    "    #get list of True indices\n",
    "    selected_subjs_idx = selected_subjs_idx_bool[selected_subjs_idx_bool].index.tolist()\n",
    "\n",
    "    subjs = all_subjects['metadata']['subject'].unique()\n",
    "    selected_subjs['X'] = nib.funcs.concat_images([all_subjects['X'].slicer[...,s] for s in selected_subjs_idx])\n",
    "    selected_subjs['y'] = all_subjects['y'][selected_subjs_idx_bool]\n",
    "\n",
    "    selected_subjs['y'] = all_subjects['y'][selected_subjs_idx_bool]\n",
    "    selected_subjs['groups'] = all_subjects['groups'][selected_subjs_idx_bool]\n",
    "    selected_subjs['metadata'] = all_subjects['metadata'][selected_subjs_idx_bool]\n",
    "else:\n",
    "    selected_subjs = all_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the PFC mask\n",
    "mask_nifti = nib.load(ml_data_folderpath + '/prefrontal_cortex.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the y array to an integer array representing the string values of the y array\n",
    "selected_subjs['y_cat'] = selected_subjs['y'].astype('category')\n",
    "selected_subjs['y_int']=selected_subjs['y_cat'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 115, 97, 601)\n",
      "(601, 97, 115, 97)\n",
      "this is the form of the data that the decoder wants, (n_samples, n_features)\n",
      "(601, 1082035)\n",
      "(1082035, 601)\n",
      "(601, 1082035)\n",
      "(601, 97, 115, 97)\n",
      "(97, 115, 97, 601)\n",
      "finally test they're equivalent.\n",
      "650303035\n",
      "650303035\n"
     ]
    }
   ],
   "source": [
    "X=selected_subjs['X']\n",
    "myarr=X.get_fdata()\n",
    "#rotate my array so that the last dimension is first\n",
    "print(myarr.shape)\n",
    "myarr=np.moveaxis(myarr,-1,0)\n",
    "print(myarr.shape)\n",
    "pre_reshaped_shape = myarr.shape\n",
    "#now flatten dims 2-4 into a single dimension\n",
    "myarr2d=myarr.reshape([myarr.shape[0],np.prod(myarr.shape[1:4])])\n",
    "print(\"this is the form of the data that the decoder wants, (n_samples, n_features)\")\n",
    "print(myarr2d.shape)\n",
    "\n",
    "\n",
    "# print(\"stats on the \")\n",
    "# #check the means of each of the matrices\n",
    "# #just hte first 10 perhaps\n",
    "# myarr_2d_\n",
    "# print(len())\n",
    "# print(myarr.mean(axis=1)[0:10])\n",
    "#now rotate back\n",
    "myarr2d=np.moveaxis(myarr2d,0,-1)\n",
    "print(myarr2d.shape)\n",
    "\n",
    "#now undo the above operations to get the original matrix\n",
    "myarr_rev=np.moveaxis(myarr2d,-1,0)\n",
    "print(myarr_rev.shape)\n",
    "myarr_rev=myarr_rev.reshape(pre_reshaped_shape)\n",
    "print(myarr_rev.shape)\n",
    "myarr_rev=np.moveaxis(myarr,0,-1)\n",
    "print(myarr_rev.shape)\n",
    "print(\"finally test they're equivalent.\")\n",
    "myarr_orig=X.get_fdata()\n",
    "\n",
    "print(np.sum(myarr_orig==myarr_rev))\n",
    "\n",
    "print(np.prod(myarr_rev.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601\n",
      "601\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#and now test that in the consolidated array, the means for each image are the same as they are in the oriignal\n",
    "arr2d_means = myarr2d.mean(axis=0)\n",
    "print(len(arr2d_means))\n",
    "arrorig_means = myarr_orig.mean(axis=(0,1,2))\n",
    "print(len(arrorig_means))\n",
    "\n",
    "print(np.all((np.round(arr2d_means,3)==np.round(arrorig_means,3))))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that transforms an array into 2D n_samples, n_features,\n",
    "and another function that transforms it back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2DX_from_4DX(nifti_X):\n",
    "    #rotate my array so that the last dimension is first\n",
    "    print(nifti_X.shape)\n",
    "    nifti_X=np.moveaxis(nifti_X,-1,0)\n",
    "    print(nifti_X.shape)\n",
    "    pre_reshaped_shape = nifti_X.shape\n",
    "    #now flatten dims 2-4 into a single dimension\n",
    "    arr2d=myarr.reshape([nifti_X.shape[0],np.prod(nifti_X.shape[1:4])])\n",
    "    print(\"this is the form of the data that the decoder wants, (n_samples, n_features)\")\n",
    "    print(arr2d.shape)\n",
    "    return(arr2d)\n",
    "\n",
    "\n",
    "def get_4DX_from_2DX(arr2d,pre_reshaped_shape):\n",
    "    #now undo the above operations to get the original matrix\n",
    "    print(arr2d.shape)\n",
    "    arr4d=arr2d.reshape(pre_reshaped_shape)\n",
    "    print(myarr_rev.shape)\n",
    "    arr4d=np.moveaxis(arr4d,0,-1)\n",
    "    print(arr4d.shape)\n",
    "    return(arr4d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralsignature",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0cca8238de402a161b34e58e1e3b4d299a0bf48c5783c9dc7c173d82c36576a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
