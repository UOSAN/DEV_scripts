{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surface-heavy",
   "metadata": {},
   "source": [
    "This is intended to be a scratchpad for measures of between-subject SST inhibition.\n",
    "\n",
    "What are some approaches here?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-schedule",
   "metadata": {},
   "source": [
    "### SST as a measure of inhibition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-carol",
   "metadata": {},
   "source": [
    "Our subject-level behavioral measure is SSRT. But it seems like Post-Error slowing is another important measure. So what about subject-level?\n",
    "\n",
    "\n",
    "\n",
    " - Similarity of successful no-go trials to successful go trials\n",
    " - Similarity of successful no-go trials to failed no-go trials\n",
    " \n",
    " \n",
    " \n",
    " - Similarity of NoGo trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-tours",
   "metadata": {},
   "source": [
    "For each measure we want to:\n",
    "\n",
    "1. validate it works by showing whether we can do within-run LOOCV to identify nogo and go trials, on average, across the subject group\n",
    "2. Reduce to a numerical value for each subject for between-subject comparison\n",
    "3. Compare to ohter measures of inhibition\n",
    "4. predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-charity",
   "metadata": {},
   "source": [
    "### SST: regressing events directly on inhibition measures measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-system",
   "metadata": {},
   "source": [
    "So, we can regress No-Go trials on measures of inhibition including both trial level and subject-level values. These will be quite different processes.\n",
    "\n",
    "#### Subject level\n",
    "\n",
    "To regress at the subject level you would probably (?) take the average subject image and regress across subjects. This would be fairly straightforward. Then, you'd regress on:\n",
    "\n",
    " - various measures of TESQ-E\n",
    " - Other measures of inhibition? Bis11?\n",
    " - behavioral measures of the SST including mean session SSRT and PES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-bottom",
   "metadata": {},
   "source": [
    "Some basic steps to do the subject-level stuff:\n",
    "    \n",
    "1) Do a single regressor across trials for each trial type\n",
    "2) Across subjects, predict each of our measures of inhibition for each trial contrast we're interested in\n",
    "3) Finish\n",
    "\n",
    "That's really quite basic from an ML point of view but it might be all we're looking for here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-belarus",
   "metadata": {},
   "source": [
    "#### Trial level\n",
    "\n",
    "To regress at the trial level, we need a behavioral measure of inhibition that we can get for each trial, or a subset of trials. I'm not sure how this fits into the wider schema. Let's say we did have, for each subject, a signature that predicted their SSRT or PES. What would we do with it?\n",
    "\n",
    "We might measure distance between those signatures and the subject's 'Go' trials. We could then predict SST. I think that would be more difficult though, and we should start with the subject-level analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-cooperative",
   "metadata": {},
   "source": [
    "### Basic subject-level\n",
    "\n",
    "Let's try to imitate what we've already done at the subject level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "expanded-young",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python initialized for apply_loocv_and_save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/datasets/__init__.py:87: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../../ml/\"))\n",
    "\n",
    "from apply_loocv_and_save import *\n",
    "import gc\n",
    "import nibabel as nib\n",
    "\n",
    "nonbids_data_path = \"/gpfs/projects/sanlab/shared/DEV/nonbids_data/\"\n",
    "ml_data_folderpath = \"/gpfs/projects/sanlab/shared/DEV/nonbids_data/fMRI/ml\"\n",
    "test_train_df = pd.read_csv(nonbids_data_path + \"fMRI/ml/train_test_markers_20210601T183243.csv\")\n",
    "\n",
    "all_sst_events= pd.read_csv(ml_data_folderpath +\"/SST/\" + \"all_sst_events.csv\")\n",
    "\n",
    "\n",
    "dataset_name = 'conditions'\n",
    "brain_data_filepath = ml_data_folderpath + '/SST/Brain_Data_' + dataset_name + '_40subs.pkl'\n",
    "\n",
    "with open(brain_data_filepath, 'rb') as pkl_file:\n",
    "    Brain_Data_allsubs = pickle.load(pkl_file)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "from nilearn.decoding import DecoderRegressor, Decoder\n",
    "\n",
    "script_path = '/gpfs/projects/sanlab/shared/DEV/DEV_scripts/fMRI/ml'\n",
    "data_path = \"/gpfs/projects/sanlab/shared/DEV/nonbids_data/fMRI/ml/\"\n",
    "# HRF 2s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "graduate-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def condition_resp_trans_func(X):\n",
    "    return(X.condition_label)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "exempt-procurement",
   "metadata": {},
   "source": [
    "apply_loocv_and_save(\n",
    "    results_filepath=data_path + \"SST/train_test_results_\" + dataset_name + \"_6subs.pkl\",\n",
    "    brain_data_filepath = data_path + 'SST/Brain_Data_' + dataset_name + '_6subs.pkl',\n",
    "    train_test_markers_filepath = data_path + \"train_test_markers_20210601T183243.csv\",\n",
    "    subjs_to_use = 6,\n",
    "    response_transform_func = response_transform_func,\n",
    "    decoderConstructor=Decoder #for classification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eleven-apache",
   "metadata": {},
   "source": [
    "data_path\n",
    "apply_single_fit_and_save(\n",
    "    results_filepath=data_path + \"SST/train_test_results_\" + dataset_name + \"_6subs.pkl\",\n",
    "    brain_data_filepath = data_path + 'SST/Brain_Data_' + dataset_name + '_6subs.pkl',\n",
    "    train_test_markers_filepath = data_path + \"train_test_markers_20210601T183243.csv\",\n",
    "    subjs_to_use = 6,\n",
    "    response_transform_func = response_transform_func,\n",
    "    decoderConstructor=Decoder #for classification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-party",
   "metadata": {},
   "source": [
    "scaffold arguments for function in following paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "backed-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set vars for the function\n",
    "\n",
    "brain_data_filepath = brain_data_filepath\n",
    "train_test_markers_filepath = data_path + \"train_test_markers_20210601T183243.csv\"\n",
    "subjs_to_use = 40\n",
    "response_transform_func=condition_resp_trans_func\n",
    "decoder = Decoder(scoring='accuracy',cv=LeaveOneGroupOut(),verbose=1)\n",
    "mask=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "quality-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filepath=data_path + \"SST/train_test_results_\" + dataset_name + \"_40subs.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-ebony",
   "metadata": {},
   "source": [
    "### Designing a function \n",
    "\n",
    "the following block can be turned into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "headed-treaty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked for intersection and no intersection between the brain data and the subjects was found.\n",
      "there were 40 subjects overlapping between the subjects marked for train data and the training dump file itself.\n",
      "CorrectGo      40\n",
      "FailedStop     40\n",
      "Cue            40\n",
      "CorrectStop    37\n",
      "FailedGo       26\n",
      "Name: condition_label, dtype: int64\n",
      "CorrectGo      40\n",
      "FailedStop     40\n",
      "Cue            40\n",
      "CorrectStop    37\n",
      "FailedGo       26\n",
      "Name: condition_label, dtype: int64\n",
      "test_train_set: 9549\n",
      "pkl_file: 168\n",
      "train_test_markers_filepath: 141\n",
      "brain_data_filepath: 139\n",
      "response_transform_func: 136\n",
      "sys: 72\n",
      "Brain_Data_allsubs: 48\n",
      "subjs_to_use: 28\n",
      "False    183\n",
      "Name: condition_label, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/projects/sanlab/shared/DEV/DEV_scripts/fMRI/ml/apply_loocv_and_save.py:191: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Brain_Data_allsubs.Y[Brain_Data_allsubs.Y=='NULL']=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n",
      "183\n",
      "using 40 subjects\n",
      "starting LeaveOneOut\n",
      "finished preprocessing\n"
     ]
    }
   ],
   "source": [
    "from nilearn.decoding import DecoderRegressor\n",
    "\n",
    "preprocessed_data = load_and_preprocess(brain_data_filepath,train_test_markers_filepath,subjs_to_use,response_transform_func)\n",
    "\n",
    "train_y = preprocessed_data['y']\n",
    "train_X = preprocessed_data['X']\n",
    "train_groups = preprocessed_data['groups']\n",
    "\n",
    "if decoder is None:\n",
    "    decoder = decoderConstructor(\n",
    "            standardize= True,cv=LeaveOneGroupOut(),\n",
    "            mask=mask,\n",
    "            n_jobs=cpus_to_use,verbose=1)\n",
    "\n",
    "print(\"starting LeaveOneOut\")\n",
    "#in this design, we're actually dealing with groups\n",
    "#we select group IDs and then grab the subjects\n",
    "#so we don't need to use LeaveOneGroupOut\n",
    "#the grouping is implicit\n",
    "\n",
    "print(\"finished preprocessing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "royal-voltage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(cv=LeaveOneGroupOut(), scoring='accuracy', verbose=1)\n"
     ]
    }
   ],
   "source": [
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "referenced-diary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running one more time on whole dataset for beta map\n",
      "[NiftiMasker.fit] Loading data from Nifti1Image(\n",
      "shape=(97, 115, 97, 183),\n",
      "affine=array([[   2.,    0.,    0.,  -96.],\n",
      "       [   0.,    2.,    0., -132.],\n",
      "       [   0.,    0.,    2.,  -78.],\n",
      "       [   0.,    0.,    0.,    1.]])\n",
      ")\n",
      "[NiftiMasker.fit] Computing the mask\n",
      "[NiftiMasker.fit] Resampling mask\n",
      "[NiftiMasker.transform_single_imgs] Loading data from Nifti1Image(\n",
      "shape=(97, 115, 97, 183),\n",
      "affine=array([[   2.,    0.,    0.,  -96.],\n",
      "       [   0.,    2.,    0., -132.],\n",
      "       [   0.,    0.,    2.,  -78.],\n",
      "       [   0.,    0.,    0.,    1.]])\n",
      ")\n",
      "[NiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[NiftiMasker.transform_single_imgs] Cleaning extracted signals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed: 47.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NiftiMasker.transform_single_imgs] Loading data from Nifti1Image(\n",
      "shape=(97, 115, 97, 183),\n",
      "affine=array([[   2.,    0.,    0.,  -96.],\n",
      "       [   0.,    2.,    0., -132.],\n",
      "       [   0.,    0.,    2.,  -78.],\n",
      "       [   0.,    0.,    0.,    1.]])\n",
      ")\n",
      "[NiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[NiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "[NiftiMasker.transform_single_imgs] Loading data from Nifti1Image(\n",
      "shape=(97, 115, 97, 183),\n",
      "affine=array([[   2.,    0.,    0.,  -96.],\n",
      "       [   0.,    2.,    0., -132.],\n",
      "       [   0.,    0.,    2.,  -78.],\n",
      "       [   0.,    0.,    0.,    1.]])\n",
      ")\n",
      "[NiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[NiftiMasker.transform_single_imgs] Cleaning extracted signals\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print('running one more time on whole dataset for beta map')\n",
    "decoder.fit(\n",
    "y=train_y,X=train_X,groups=train_groups)\n",
    "\n",
    "train_score = decoder.score(train_X,train_y)\n",
    "train_y_pred = decoder.predict(train_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sharp-active",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished learning\n"
     ]
    }
   ],
   "source": [
    "if type(decoder)==DecoderRegressor:\n",
    "    weight_img = decoder.coef_img_['beta']\n",
    "else:\n",
    "    weight_img = decoder.coef_img_\n",
    "\n",
    "print('finished learning')\n",
    "\n",
    "with open(results_filepath, 'wb') as handle:\n",
    "    pickle.dump({\n",
    "        'decoder':decoder,\n",
    "        'weight_img':weight_img,\n",
    "        'obs':train_y,\n",
    "        'train_train_score':train_score,\n",
    "        'train_y_pred': train_y_pred,\n",
    "        'metadata':preprocessed_data['metadata']\n",
    "    },handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-litigation",
   "metadata": {},
   "source": [
    "END FUNCTION ################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-wallace",
   "metadata": {},
   "source": [
    "### Function this was modelled off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "special-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def apply_single_fit_and_save(\n",
    "    results_filepath,\n",
    "    brain_data_filepath = '../data/Brain_Data_2sns_60subs.pkl',\n",
    "    train_test_markers_filepath = \"../data/train_test_markers_20210601T183243.csv\",\n",
    "    subjs_to_use = None, #set this to get a subset, otherwise use all of them.\n",
    "    response_transform_func = None,\n",
    "    mask=None\n",
    "    ):\n",
    "    \n",
    "    \n",
    "\n",
    "    preprocessed_data = load_and_preprocess(brain_data_filepath,train_test_markers_filepath,subjs_to_use,response_transform_func)\n",
    "    \n",
    "    train_y = preprocessed_data['y']\n",
    "    train_X = preprocessed_data['X']\n",
    "    train_groups = preprocessed_data['groups']\n",
    "    \n",
    "\n",
    "    \n",
    "    from nilearn.decoding import DecoderRegressor\n",
    "    dRegressor = DecoderRegressor(\n",
    "        standardize= True,cv=LeaveOneGroupOut(),scoring=\"r2\",\n",
    "        mask=mask,\n",
    "        n_jobs=cpus_to_use,\n",
    "        verbose=1)\n",
    "\n",
    "\n",
    "    print(\"starting LeaveOneOut\")\n",
    "    #in this design, we're actually dealing with groups\n",
    "    #we select group IDs and then grab the subjects\n",
    "    #so we don't need to use LeaveOneGroupOut\n",
    "    #the grouping is implicit\n",
    "\n",
    "    print(\"finished preprocessing\")\n",
    "\n",
    "\n",
    "    \n",
    "    print('running one more time on whole dataset for beta map')\n",
    "    dRegressor.fit(\n",
    "    y=train_y,X=train_X,groups=train_groups)\n",
    "    \n",
    "    train_score = dRegressor.score(train_X,train_y)\n",
    "    train_y_pred = dRegressor.predict(train_X)\n",
    "\n",
    "    weight_img = dRegressor.coef_img_['beta']\n",
    "\n",
    "    print('finished learning')\n",
    "\n",
    "    with open(results_filepath, 'wb') as handle:\n",
    "        pickle.dump({\n",
    "            'regressor':dRegressor,\n",
    "            'weight_img':weight_img,\n",
    "            'obs':train_y,\n",
    "            'train_train_score':train_score,\n",
    "            'train_y_pred': train_y_pred,\n",
    "            'metadata':preprocessed_data['metadata']\n",
    "        },handle)\n",
    "\n",
    "\n",
    "    print('saved.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-palestine",
   "metadata": {},
   "source": [
    "### Interpreting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "disturbed-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_filepath, 'rb') as handle:\n",
    "    results = pickle.load(open(results_filepath, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "permanent-appraisal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['decoder', 'weight_img', 'obs', 'train_train_score', 'train_y_pred', 'metadata'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "interpreted-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['metadata']['train_y_pred'] = results['train_y_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "metric-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['correct'] = results['metadata']['condition_label']==results['metadata']['train_y_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-scope",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "phantom-stocks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(results['correct'])/len(results['correct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-artist",
   "metadata": {},
   "source": [
    "OK, so in the train/train, we got 100% correct. but that's not LOOCV is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "suited-portland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CorrectGo\n",
      " length:40\n",
      "CorrectStop\n",
      " length:40\n",
      "Cue\n",
      " length:40\n",
      "FailedGo\n",
      " length:40\n",
      "FailedStop\n",
      " length:40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(k +'\\n length: ' + str(len(v))) for k, v in results['decoder'].cv_scores_.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "attempted-individual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CorrectGo cv scores\n",
      " length: 40\n",
      "[0.4, 0.4, 0.6666666666666666, 0.6, 0.5, 0.8, 1.0, 0.4, 0.2, 1.0, 0.25, 1.0, 0.4, 0.2, 0.8, 0.4, 1.0, 0.4, 0.8, 0.5, 0.25, 0.6, 1.0, 0.75, 0.2, 0.4, 0.4, 1.0, 0.2, 0.75, 0.6, 0.8, 0.2, 0.6, 0.4, 0.8, 0.5, 0.25, 0.6, 0.5]\n",
      "CorrectStop cv scores\n",
      " length: 40\n",
      "[0.2, 0.2, 0.0, 0.2, 0.0, 0.2, 0.25, 0.2, 0.2, 0.2, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.0, 0.2, 0.2, 0.25, 0.25, 0.2, 0.25, 0.25, 0.2, 0.2, 0.2, 0.25, 0.2, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.2, 0.25]\n",
      "Cue cv scores\n",
      " length: 40\n",
      "[0.2, 0.2, 0.3333333333333333, 0.2, 0.25, 0.2, 0.25, 0.2, 0.2, 0.4, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.3333333333333333, 0.2, 0.2, 0.25, 0.25, 0.4, 0.25, 0.25, 0.2, 0.2, 0.2, 0.25, 0.2, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.2, 0.25]\n",
      "FailedGo cv scores\n",
      " length: 40\n",
      "[0.2, 0.2, 0.0, 0.2, 0.25, 0.2, 0.0, 0.2, 0.2, 0.4, 0.0, 0.0, 0.2, 0.2, 0.2, 0.2, 0.3333333333333333, 0.2, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.2, 0.0, 0.2, 0.0, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.0, 0.0, 0.2, 0.0]\n",
      "FailedStop cv scores\n",
      " length: 40\n",
      "[0.2, 0.2, 0.3333333333333333, 0.2, 0.25, 0.2, 0.25, 0.2, 0.2, 0.4, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.3333333333333333, 0.2, 0.2, 0.25, 0.25, 0.2, 0.25, 0.25, 0.2, 0.2, 0.2, 0.25, 0.2, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.2, 0.25]\n"
     ]
    }
   ],
   "source": [
    "for k, v in results['decoder'].cv_scores_.items():\n",
    "    print(k + \" cv scores\")\n",
    "    #we used LOGO CV, so each of these is a report of how it scored at correctly identifying items in each group, given the other 39 groups\n",
    "    #One would think this would be either a zero or 1 because there's only one item in each group...\n",
    "    print(\" length: \" + str(len(v)))\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-honduras",
   "metadata": {},
   "source": [
    "Measuring \"accuracy\", so I believe these are, out of ALL the samples in the set, the proportion correctly classified as within the set of all labels. An \"0.2\" could be achieved by classifying all members of the sample as not the label, whereas we know precisely 1 in each set is the label. So these are not great scores.\n",
    "\n",
    "I wonder whether we could get more interpretable results by scoring based on precision and recall? \n",
    "\n",
    "If we use recall, it'll tell us, for each class, how many were correctly classified as that class. That's useful because if we add up aross the set we can tell how many were correctly classified as the actual class they're in. Because no result can be classified as more than one class, we can be confident, if recall is 100%, precision is also 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "spread-romania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition_index</th>\n",
       "      <th>condition_label</th>\n",
       "      <th>raw_beta_description</th>\n",
       "      <th>beta</th>\n",
       "      <th>subject</th>\n",
       "      <th>wave</th>\n",
       "      <th>train_y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>CorrectGo</td>\n",
       "      <td>spm_spm:beta (0001) - Sn(1) CorrectGo*bf(1)</td>\n",
       "      <td>beta_0001.nii</td>\n",
       "      <td>DEV005</td>\n",
       "      <td>wave1</td>\n",
       "      <td>CorrectGo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CorrectStop</td>\n",
       "      <td>spm_spm:beta (0002) - Sn(1) CorrectStop*bf(1)</td>\n",
       "      <td>beta_0002.nii</td>\n",
       "      <td>DEV005</td>\n",
       "      <td>wave1</td>\n",
       "      <td>CorrectStop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>FailedStop</td>\n",
       "      <td>spm_spm:beta (0003) - Sn(1) FailedStop*bf(1)</td>\n",
       "      <td>beta_0003.nii</td>\n",
       "      <td>DEV005</td>\n",
       "      <td>wave1</td>\n",
       "      <td>FailedStop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Cue</td>\n",
       "      <td>spm_spm:beta (0004) - Sn(1) Cue*bf(1)</td>\n",
       "      <td>beta_0004.nii</td>\n",
       "      <td>DEV005</td>\n",
       "      <td>wave1</td>\n",
       "      <td>Cue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>FailedGo</td>\n",
       "      <td>spm_spm:beta (0005) - Sn(1) FailedGo*bf(1)</td>\n",
       "      <td>beta_0005.nii</td>\n",
       "      <td>DEV005</td>\n",
       "      <td>wave1</td>\n",
       "      <td>FailedGo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>CorrectGo</td>\n",
       "      <td>spm_spm:beta (0001) - Sn(1) CorrectGo*bf(1)</td>\n",
       "      <td>beta_0001.nii</td>\n",
       "      <td>DEV006</td>\n",
       "      <td>wave1</td>\n",
       "      <td>CorrectGo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>CorrectStop</td>\n",
       "      <td>spm_spm:beta (0002) - Sn(1) CorrectStop*bf(1)</td>\n",
       "      <td>beta_0002.nii</td>\n",
       "      <td>DEV006</td>\n",
       "      <td>wave1</td>\n",
       "      <td>CorrectStop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>FailedStop</td>\n",
       "      <td>spm_spm:beta (0003) - Sn(1) FailedStop*bf(1)</td>\n",
       "      <td>beta_0003.nii</td>\n",
       "      <td>DEV006</td>\n",
       "      <td>wave1</td>\n",
       "      <td>FailedStop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>Cue</td>\n",
       "      <td>spm_spm:beta (0004) - Sn(1) Cue*bf(1)</td>\n",
       "      <td>beta_0004.nii</td>\n",
       "      <td>DEV006</td>\n",
       "      <td>wave1</td>\n",
       "      <td>Cue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>FailedGo</td>\n",
       "      <td>spm_spm:beta (0005) - Sn(1) FailedGo*bf(1)</td>\n",
       "      <td>beta_0005.nii</td>\n",
       "      <td>DEV006</td>\n",
       "      <td>wave1</td>\n",
       "      <td>FailedGo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>CorrectGo</td>\n",
       "      <td>spm_spm:beta (0001) - Sn(1) CorrectGo*bf(1)</td>\n",
       "      <td>beta_0001.nii</td>\n",
       "      <td>DEV009</td>\n",
       "      <td>wave1</td>\n",
       "      <td>CorrectGo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>FailedStop</td>\n",
       "      <td>spm_spm:beta (0002) - Sn(1) FailedStop*bf(1)</td>\n",
       "      <td>beta_0002.nii</td>\n",
       "      <td>DEV009</td>\n",
       "      <td>wave1</td>\n",
       "      <td>FailedStop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>Cue</td>\n",
       "      <td>spm_spm:beta (0003) - Sn(1) Cue*bf(1)</td>\n",
       "      <td>beta_0003.nii</td>\n",
       "      <td>DEV009</td>\n",
       "      <td>wave1</td>\n",
       "      <td>Cue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>CorrectGo</td>\n",
       "      <td>spm_spm:beta (0001) - Sn(1) CorrectGo*bf(1)</td>\n",
       "      <td>beta_0001.nii</td>\n",
       "      <td>DEV010</td>\n",
       "      <td>wave1</td>\n",
       "      <td>CorrectGo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>CorrectStop</td>\n",
       "      <td>spm_spm:beta (0002) - Sn(1) CorrectStop*bf(1)</td>\n",
       "      <td>beta_0002.nii</td>\n",
       "      <td>DEV010</td>\n",
       "      <td>wave1</td>\n",
       "      <td>CorrectStop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    condition_index condition_label  \\\n",
       "0                 0       CorrectGo   \n",
       "1                 1     CorrectStop   \n",
       "2                 2      FailedStop   \n",
       "3                 3             Cue   \n",
       "4                 4        FailedGo   \n",
       "5                 0       CorrectGo   \n",
       "6                 1     CorrectStop   \n",
       "7                 2      FailedStop   \n",
       "8                 3             Cue   \n",
       "9                 4        FailedGo   \n",
       "10                0       CorrectGo   \n",
       "11                1      FailedStop   \n",
       "12                2             Cue   \n",
       "13                0       CorrectGo   \n",
       "14                1     CorrectStop   \n",
       "\n",
       "                             raw_beta_description           beta subject  \\\n",
       "0     spm_spm:beta (0001) - Sn(1) CorrectGo*bf(1)  beta_0001.nii  DEV005   \n",
       "1   spm_spm:beta (0002) - Sn(1) CorrectStop*bf(1)  beta_0002.nii  DEV005   \n",
       "2    spm_spm:beta (0003) - Sn(1) FailedStop*bf(1)  beta_0003.nii  DEV005   \n",
       "3           spm_spm:beta (0004) - Sn(1) Cue*bf(1)  beta_0004.nii  DEV005   \n",
       "4      spm_spm:beta (0005) - Sn(1) FailedGo*bf(1)  beta_0005.nii  DEV005   \n",
       "5     spm_spm:beta (0001) - Sn(1) CorrectGo*bf(1)  beta_0001.nii  DEV006   \n",
       "6   spm_spm:beta (0002) - Sn(1) CorrectStop*bf(1)  beta_0002.nii  DEV006   \n",
       "7    spm_spm:beta (0003) - Sn(1) FailedStop*bf(1)  beta_0003.nii  DEV006   \n",
       "8           spm_spm:beta (0004) - Sn(1) Cue*bf(1)  beta_0004.nii  DEV006   \n",
       "9      spm_spm:beta (0005) - Sn(1) FailedGo*bf(1)  beta_0005.nii  DEV006   \n",
       "10    spm_spm:beta (0001) - Sn(1) CorrectGo*bf(1)  beta_0001.nii  DEV009   \n",
       "11   spm_spm:beta (0002) - Sn(1) FailedStop*bf(1)  beta_0002.nii  DEV009   \n",
       "12          spm_spm:beta (0003) - Sn(1) Cue*bf(1)  beta_0003.nii  DEV009   \n",
       "13    spm_spm:beta (0001) - Sn(1) CorrectGo*bf(1)  beta_0001.nii  DEV010   \n",
       "14  spm_spm:beta (0002) - Sn(1) CorrectStop*bf(1)  beta_0002.nii  DEV010   \n",
       "\n",
       "     wave train_y_pred  \n",
       "0   wave1    CorrectGo  \n",
       "1   wave1  CorrectStop  \n",
       "2   wave1   FailedStop  \n",
       "3   wave1          Cue  \n",
       "4   wave1     FailedGo  \n",
       "5   wave1    CorrectGo  \n",
       "6   wave1  CorrectStop  \n",
       "7   wave1   FailedStop  \n",
       "8   wave1          Cue  \n",
       "9   wave1     FailedGo  \n",
       "10  wave1    CorrectGo  \n",
       "11  wave1   FailedStop  \n",
       "12  wave1          Cue  \n",
       "13  wave1    CorrectGo  \n",
       "14  wave1  CorrectStop  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['metadata'].iloc[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-literacy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "breathing-connection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([  5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,  16,  17,\n",
       "          18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,\n",
       "          31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,\n",
       "          44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
       "          57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,\n",
       "          83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,\n",
       "          96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108,\n",
       "         109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121,\n",
       "         122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134,\n",
       "         135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n",
       "         148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160,\n",
       "         161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173,\n",
       "         174, 175, 176, 177, 178, 179, 180, 181, 182]),\n",
       "  array([0, 1, 2, 3, 4])),\n",
       " (array([  0,   1,   2,   3,   4,  10,  11,  12,  13,  14,  15,  16,  17,\n",
       "          18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,\n",
       "          31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,\n",
       "          44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
       "          57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,\n",
       "          83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,\n",
       "          96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108,\n",
       "         109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121,\n",
       "         122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134,\n",
       "         135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n",
       "         148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160,\n",
       "         161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173,\n",
       "         174, 175, 176, 177, 178, 179, 180, 181, 182]),\n",
       "  array([5, 6, 7, 8, 9]))]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['decoder'].cv_[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-following",
   "metadata": {},
   "source": [
    "### Repeating test measuring precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-segment",
   "metadata": {},
   "source": [
    "#### new function based on code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "gentle-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decoding import DecoderRegressor  \n",
    "\n",
    "def apply_single_fit_and_save(\n",
    "    results_filepath,\n",
    "    brain_data_filepath = '../data/Brain_Data_2sns_60subs.pkl',\n",
    "    train_test_markers_filepath = \"../data/train_test_markers_20210601T183243.csv\",\n",
    "    subjs_to_use = None, #set this to get a subset, otherwise use all of them.\n",
    "    response_transform_func = None,\n",
    "    decoder = None,\n",
    "    mask=None,\n",
    "    cpus_to_use=1\n",
    "    ):\n",
    "    \n",
    "    preprocessed_data = load_and_preprocess(brain_data_filepath,train_test_markers_filepath,subjs_to_use,response_transform_func)\n",
    "\n",
    "    train_y = preprocessed_data['y']\n",
    "    train_X = preprocessed_data['X']\n",
    "    train_groups = preprocessed_data['groups']\n",
    "\n",
    "    if decoder is None:\n",
    "        decoder = decoderConstructor(\n",
    "                standardize= True,cv=LeaveOneGroupOut(),\n",
    "                mask=mask,\n",
    "                n_jobs=cpus_to_use,verbose=1)\n",
    "\n",
    "    print(\"starting LeaveOneOut\")\n",
    "    #in this design, we're actually dealing with groups\n",
    "    #we select group IDs and then grab the subjects\n",
    "    #so we don't need to use LeaveOneGroupOut\n",
    "    #the grouping is implicit\n",
    "\n",
    "    print('Fitting decoder')\n",
    "    decoder.fit(\n",
    "    y=train_y,X=train_X,groups=train_groups)\n",
    "\n",
    "    print(\"predicting\")\n",
    "    train_score = decoder.score(train_X,train_y)\n",
    "    train_y_pred = decoder.predict(train_X)\n",
    "\n",
    "    if type(decoder)==DecoderRegressor:\n",
    "        weight_img = decoder.coef_img_['beta']\n",
    "    else:\n",
    "        weight_img = decoder.coef_img_\n",
    "\n",
    "    print('finished learning')\n",
    "\n",
    "    print(\"saving\")\n",
    "\n",
    "    result_package = {\n",
    "            'decoder':decoder,\n",
    "            'weight_img':weight_img,\n",
    "            'obs':train_y,\n",
    "            'train_train_score':train_score,\n",
    "            'train_y_pred': train_y_pred,\n",
    "            'metadata':preprocessed_data['metadata']\n",
    "        }\n",
    "    with open(results_filepath, 'wb') as handle:\n",
    "        pickle.dump(result_package,handle)\n",
    "        \n",
    "    return(result_package)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "piano-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import get_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-delicious",
   "metadata": {},
   "source": [
    "from  https://nilearn.github.io/modules/generated/nilearn.decoding.Decoder.html\n",
    "\n",
    "here's how to define a callable scorer:\n",
    "\n",
    "scoring: str, callable or None, optional. Default: ‘roc_auc’\n",
    "\n",
    "The scoring strategy to use. See the scikit-learn documentation at https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules If callable, takes as arguments the fitted estimator, the test data (X_test) and the test target (y_test) if y is not None. e.g. scorer(estimator, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cleared-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_multiclass_score(estimator,X_test,y_test):\n",
    "    print(\"running recall_multiclass_score\")\n",
    "    y_pred = estimator.predict(X_test)\n",
    "#     print(\"y_pred:\")\n",
    "#     print(y_pred)\n",
    "#     print(\"y_test:\")\n",
    "#     print(y_test)\n",
    "    rs = recall_score(y_test,y_pred,average='micro')\n",
    "#     print(\"recall result:\")\n",
    "#     print(rs)\n",
    "    return(rs)\n",
    "    #return(1)\n",
    "\n",
    "\n",
    "#recall_multiclass_score(decoder,[0,1,1],[0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "lyric-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set vars for the function\n",
    "\n",
    "brain_data_filepath = brain_data_filepath\n",
    "train_test_markers_filepath = data_path + \"train_test_markers_20210601T183243.csv\"\n",
    "subjs_to_use = 10\n",
    "response_transform_func=condition_resp_trans_func\n",
    "decoder_recall = Decoder(\n",
    "    scoring=recall_multiclass_score,\n",
    "    standardize= True,\n",
    "    cv=LeaveOneGroupOut(),\n",
    "    verbose=1,n_jobs=10)\n",
    "mask=None\n",
    "\n",
    "results_filepath=data_path + \"SST/train_test_results_recall_\" + dataset_name + \"_40subs.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "civic-diabetes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked for intersection and no intersection between the brain data and the subjects was found.\n",
      "there were 40 subjects overlapping between the subjects marked for train data and the training dump file itself.\n",
      "CorrectGo      40\n",
      "FailedStop     40\n",
      "Cue            40\n",
      "CorrectStop    37\n",
      "FailedGo       26\n",
      "Name: condition_label, dtype: int64\n",
      "CorrectGo      40\n",
      "FailedStop     40\n",
      "Cue            40\n",
      "CorrectStop    37\n",
      "FailedGo       26\n",
      "Name: condition_label, dtype: int64\n",
      "test_train_set: 9549\n",
      "pkl_file: 168\n",
      "train_test_markers_filepath: 141\n",
      "brain_data_filepath: 139\n",
      "response_transform_func: 136\n",
      "sys: 72\n",
      "Brain_Data_allsubs: 48\n",
      "subjs_to_use: 28\n",
      "False    183\n",
      "Name: condition_label, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/projects/sanlab/shared/DEV/DEV_scripts/fMRI/ml/apply_loocv_and_save.py:191: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Brain_Data_allsubs.Y[Brain_Data_allsubs.Y=='NULL']=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n",
      "183\n",
      "using 10 subjects\n",
      "starting LeaveOneOut\n",
      "Fitting decoder\n",
      "[NiftiMasker.fit] Loading data from Nifti1Image(\n",
      "shape=(97, 115, 97, 46),\n",
      "affine=array([[   2.,    0.,    0.,  -96.],\n",
      "       [   0.,    2.,    0., -132.],\n",
      "       [   0.,    0.,    2.,  -78.],\n",
      "       [   0.,    0.,    0.,    1.]])\n",
      ")\n",
      "[NiftiMasker.fit] Computing the mask\n",
      "[NiftiMasker.fit] Resampling mask\n",
      "[NiftiMasker.transform_single_imgs] Loading data from Nifti1Image(\n",
      "shape=(97, 115, 97, 46),\n",
      "affine=array([[   2.,    0.,    0.,  -96.],\n",
      "       [   0.,    2.,    0., -132.],\n",
      "       [   0.,    0.,    2.,  -78.],\n",
      "       [   0.,    0.,    0.,    1.]])\n",
      ")\n",
      "[NiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[NiftiMasker.transform_single_imgs] Cleaning extracted signals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:   21.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting\n",
      "running recall_multiclass_score\n",
      "[NiftiMasker.transform_single_imgs] Loading data from Nifti1Image(\n",
      "shape=(97, 115, 97, 46),\n",
      "affine=array([[   2.,    0.,    0.,  -96.],\n",
      "       [   0.,    2.,    0., -132.],\n",
      "       [   0.,    0.,    2.,  -78.],\n",
      "       [   0.,    0.,    0.,    1.]])\n",
      ")\n",
      "[NiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[NiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "[NiftiMasker.transform_single_imgs] Loading data from Nifti1Image(\n",
      "shape=(97, 115, 97, 46),\n",
      "affine=array([[   2.,    0.,    0.,  -96.],\n",
      "       [   0.,    2.,    0., -132.],\n",
      "       [   0.,    0.,    2.,  -78.],\n",
      "       [   0.,    0.,    0.,    1.]])\n",
      ")\n",
      "[NiftiMasker.transform_single_imgs] Extracting region signals\n",
      "[NiftiMasker.transform_single_imgs] Cleaning extracted signals\n",
      "finished learning\n",
      "saving\n"
     ]
    }
   ],
   "source": [
    "fit_results = apply_single_fit_and_save(\n",
    "    results_filepath =results_filepath,\n",
    "    brain_data_filepath = brain_data_filepath,\n",
    "    train_test_markers_filepath = train_test_markers_filepath,\n",
    "    subjs_to_use = subjs_to_use,\n",
    "    response_transform_func = condition_resp_trans_func,\n",
    "    decoder = decoder_recall\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "athletic-orlando",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CorrectGo': [0.4, 0.4, 1.0, 0.4, 0.5, 0.8, 1.0, 0.2, 0.2, 0.8],\n",
       " 'CorrectStop': [0.2, 0.2, 0.0, 0.2, 0.0, 0.2, 0.25, 0.2, 0.2, 0.2],\n",
       " 'Cue': [0.2, 0.2, 0.3333333333333333, 0.2, 0.25, 0.2, 0.25, 0.2, 0.2, 0.2],\n",
       " 'FailedGo': [0.2,\n",
       "  0.2,\n",
       "  0.3333333333333333,\n",
       "  0.2,\n",
       "  0.25,\n",
       "  0.2,\n",
       "  0.25,\n",
       "  0.2,\n",
       "  0.2,\n",
       "  0.6],\n",
       " 'FailedStop': [0.2,\n",
       "  0.2,\n",
       "  0.3333333333333333,\n",
       "  0.2,\n",
       "  0.25,\n",
       "  0.2,\n",
       "  0.25,\n",
       "  0.2,\n",
       "  0.2,\n",
       "  0.2]}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_results['decoder'].cv_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "minor-street",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CorrectGo', 'CorrectStop', 'Cue', 'FailedGo', 'FailedStop'],\n",
       "      dtype='<U11')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_results['decoder'].classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "revised-trunk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(cv=LeaveOneGroupOut(), estimator=LinearSVC(max_iter=10000.0),\n",
       "        memory=Memory(location=None), n_jobs=10,\n",
       "        scoring=<function recall_multiclass_score at 0x2aaaf7c9f430>,\n",
       "        verbose=1)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_results['decoder']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-outreach",
   "metadata": {},
   "source": [
    "I can't get a CV score that can be unambiguously interpreted. It's hard to intepret what htese are, and the final score does not implement LOGO.\n",
    "\n",
    "Therefore I think I need to write my own CV function. We could adapt the one I already wrote, or start from scratch. Can be straightforward; wrap the decoder, do LOOCV, etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-neuralsignature]",
   "language": "python",
   "name": "conda-env-.conda-neuralsignature-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
