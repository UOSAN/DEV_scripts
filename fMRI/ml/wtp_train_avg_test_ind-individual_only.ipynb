{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "substantial-crawford",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/datasets/__init__.py:87: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltools as nlt\n",
    "import nilearn as nil\n",
    "import nibabel as nib\n",
    "import warnings\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import dev_wtp_io_utils\n",
    "import gc #garbage collection\n",
    "from nilearn import plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "agreed-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-moore",
   "metadata": {},
   "source": [
    "### Load brain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "labeled-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_set = pd.read_csv(\"../data/train_test_markers_20210601T183243.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "close-robertson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked for intersection and no intersection between the brain data and the subjects was found.\n",
      "there were 60 subjects overlapping between the subjects marked for train data and the training dump file itself.\n"
     ]
    }
   ],
   "source": [
    "with open('../data/Brain_Data_2sns_60subs.pkl', 'rb') as pkl_file:\n",
    "    Brain_Data_allsubs = pickle.load(pkl_file)\n",
    "    \n",
    "dev_wtp_io_utils.check_BD_against_test_train_set(Brain_Data_allsubs,test_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-mouse",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "extended-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0    1164\n",
      "6.0    1018\n",
      "7.0     904\n",
      "8.0     604\n",
      "Name: response, dtype: int64\n",
      "5.0    1164\n",
      "6.0    1018\n",
      "7.0     904\n",
      "8.0     604\n",
      "Name: response, dtype: int64\n",
      "False    3690\n",
      "True      150\n",
      "Name: response, dtype: int64\n",
      "3690\n",
      "3840\n"
     ]
    }
   ],
   "source": [
    "Brain_Data_allsubs.Y = Brain_Data_allsubs.X.response.copy()\n",
    "print(Brain_Data_allsubs.Y.value_counts())\n",
    "Brain_Data_allsubs.Y[Brain_Data_allsubs.Y=='NULL']=None\n",
    "print(Brain_Data_allsubs.Y.value_counts())\n",
    "print(Brain_Data_allsubs.Y.isnull().value_counts())\n",
    "Brain_Data_allsubs_nn = Brain_Data_allsubs[Brain_Data_allsubs.Y.isnull()==False]\n",
    "print(len(Brain_Data_allsubs_nn))\n",
    "print(len(Brain_Data_allsubs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "noble-observer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       DEV001\n",
       "1       DEV001\n",
       "2       DEV001\n",
       "3       DEV001\n",
       "4       DEV001\n",
       "         ...  \n",
       "3685    DEV089\n",
       "3686    DEV089\n",
       "3687    DEV089\n",
       "3688    DEV089\n",
       "3689    DEV089\n",
       "Name: subject, Length: 3690, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subs_nn_nifti = Brain_Data_allsubs_nn.to_nifti()\n",
    "all_subs_nn_nifti_Y = Brain_Data_allsubs_nn.Y\n",
    "all_subs_nn_nifti_groups = Brain_Data_allsubs_nn.X.subject\n",
    "all_subs_nn_nifti_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-basin",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Regressing in nilearn:\n",
    " - https://nilearn.github.io/decoding/estimator_choice.html\n",
    " - http://www.ncbi.nlm.nih.gov/pubmed/20691790\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-flight",
   "metadata": {},
   "source": [
    "OK, so that's how you do it. It's pretty straightforward.\n",
    "\n",
    "So...we won't look at nested cross-validation juuust yet, because the next step is to work out how to train on one set and predict on another. that will definitely require a custom pipeline. Let's get started..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "removable-transfer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del Brain_Data_allsubs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "institutional-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decoding import DecoderRegressor\n",
    "dRegressor = DecoderRegressor(estimator = 'ridge_regressor', standardize= True,scoring=\"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-nitrogen",
   "metadata": {},
   "source": [
    "As a control, we'll try this again, this time just training and testing on individual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "listed-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler.asizeof import asizeof\n",
    "import sys\n",
    "def asizeof_fmt(obj, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    num = asizeof(obj)\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "# for name, size in sorted(((name, asizeof(value)) for name, value in locals().items()),\n",
    "#                          key= lambda x: -x[1])[:10]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "olive-venezuela",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3 GiB'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asizeof_fmt(Brain_Data_allsubs_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "antique-favor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.4 GiB'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asizeof_fmt(all_subs_nn_nifti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "innovative-preparation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nibabel.nifti1.Nifti1Image at 0x2aaab4d709a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subs_nn_nifti.slicer[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "advisory-theta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 109, 91, 3690)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subs_nn_nifti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "disabled-proceeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 109, 91, 3690)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subs_nn_nifti.slicer[0:500].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "opened-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_subs_nifti = all_subs_nn_nifti.slicer[...,0:500]\n",
    "first_subs_nifti_Y = all_subs_nn_nifti_Y[0:500]\n",
    "first_subs_nifti = nil.image.clean_img(first_subs_nifti,detrend=False,standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "broad-tooth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ideal-uniform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       7.0\n",
       "1       6.0\n",
       "2       6.0\n",
       "3       5.0\n",
       "4       7.0\n",
       "       ... \n",
       "3685    5.0\n",
       "3686    5.0\n",
       "3687    5.0\n",
       "3688    5.0\n",
       "3689    7.0\n",
       "Name: response, Length: 3690, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subs_nn_nifti_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "reported-profit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 109, 91, 500)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_subs_nifti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-volleyball",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "spanish-finder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to test on a training group of 7 items, holding out the following subjects:['DEV005' 'DEV014']\n",
      "selecting training data\n",
      "2.5 GiB\n",
      "(91, 109, 91, 374)\n",
      "selecting test data\n",
      "(91, 109, 91, 126)\n",
      "regressing\n",
      "2.5 GiB\n",
      "predicting\n",
      "test score was:\n",
      "0.12762489867306293\n",
      "In order to test on a training group of 7 items, holding out the following subjects:['DEV013' 'DEV009']\n",
      "selecting training data\n",
      "2.5 GiB\n",
      "(91, 109, 91, 375)\n",
      "selecting test data\n",
      "(91, 109, 91, 125)\n",
      "regressing\n",
      "2.5 GiB\n",
      "predicting\n",
      "test score was:\n",
      "-0.002948134494224819\n",
      "In order to test on a training group of 7 items, holding out the following subjects:['DEV010' 'DEV001']\n",
      "selecting training data\n",
      "2.5 GiB\n",
      "(91, 109, 91, 379)\n",
      "selecting test data\n",
      "(91, 109, 91, 121)\n",
      "regressing\n",
      "2.5 GiB\n",
      "predicting\n",
      "test score was:\n",
      "0.3263347191308281\n",
      "In order to test on a training group of 7 items, holding out the following subjects:['DEV006' 'DEV015']\n",
      "selecting training data\n",
      "2.9 GiB\n",
      "(91, 109, 91, 434)\n",
      "selecting test data\n",
      "(91, 109, 91, 66)\n",
      "regressing\n",
      "2.9 GiB\n",
      "predicting\n",
      "test score was:\n",
      "0.2906596655211279\n",
      "In order to test on a training group of 8 items, holding out the following subjects:['DEV012']\n",
      "selecting training data\n",
      "2.9 GiB\n",
      "(91, 109, 91, 438)\n",
      "selecting test data\n",
      "(91, 109, 91, 62)\n",
      "regressing\n",
      "2.9 GiB\n",
      "predicting\n",
      "test score was:\n",
      "-0.7229050348146862\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold,GroupKFold\n",
    "#def cv_train_test_different_sets(averaged_X,averaged_Y, averaged_groups, individual_X,individual_groups, Y, cv,group_list)\n",
    "\"\"\"\n",
    "averaged_X: values grouped\n",
    "averaged_groups: group allocations for the averaged dataset\n",
    "individual_X: values grouped into averages for testing\n",
    "cv: a Grouped cross-validator\n",
    "group_list: name of the groups\n",
    "\"\"\"\n",
    "individual_X = first_subs_nifti#all_subs_nn_nifti\n",
    "individual_y = first_subs_nifti_Y #all_subs_nn_nifti_Y\n",
    "individual_groups = all_subs_nn_nifti_groups[0:500]\n",
    "cv=KFold(n_splits=5)\n",
    "\n",
    "\n",
    "\n",
    "groups_array = np.array(list(set(individual_groups)))\n",
    "\n",
    "#the CV that the inner Regressor uses\n",
    "cv_inner = GroupKFold(3)\n",
    "\n",
    "#we actually use KFold on the group names themselves, then filter across that\n",
    "#that's equivalent to doing a GroupedKFold on the data.\n",
    "test_scores = []\n",
    "for train_i,test_i in cv.split(groups_array):\n",
    "    train_group_items, test_group_items = groups_array[train_i], groups_array[test_i]\n",
    "    print('In order to test on a training group of ' +\n",
    "          str(len(train_group_items)) + ' items, holding out the following subjects:' +\n",
    "          str(test_group_items))\n",
    "    \n",
    "    #select training data from the averages\n",
    "    print('selecting training data')\n",
    "    train_selector = [i for i, x in enumerate(individual_groups) if x in train_group_items]\n",
    "    train_y = individual_y[train_selector]\n",
    "    train_X = nib.funcs.concat_images([individual_X.slicer[...,s] for s in train_selector])\n",
    "    train_groups = individual_groups[train_selector]\n",
    "    print(asizeof_fmt(train_X))\n",
    "    print(train_X.shape)\n",
    "    \n",
    "    #select testing data from the individual values\n",
    "    print('selecting test data')\n",
    "    test_selector = [i for i, x in enumerate(individual_groups) if x in test_group_items]\n",
    "    test_y = individual_y[test_selector]\n",
    "    test_X = nib.funcs.concat_images([individual_X.slicer[...,s] for s in test_selector])\n",
    "    \n",
    "    test_groups = individual_groups[test_selector]\n",
    "    print(test_X.shape)\n",
    "    \n",
    "    print(\"regressing\")\n",
    "    regressor = DecoderRegressor(standardize= True,cv=cv_inner, scoring=\"r2\")\n",
    "    print(asizeof_fmt(train_X))\n",
    "    regressor.fit(y=train_y,X=train_X,groups=train_groups)\n",
    "    \n",
    "    print(\"predicting\")\n",
    "    #now predict on our test split\n",
    "    test_score = regressor.score(test_X,test_y)\n",
    "    test_scores = test_scores+[test_score]\n",
    "    print('test score was:')\n",
    "    print(test_score)\n",
    "    \n",
    "    del test_X\n",
    "    del train_X\n",
    "    gc.collect() #clean up. this is big data we're working with\n",
    "    #https://stackoverflow.com/questions/1316767/how-can-i-explicitly-free-memory-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "surprised-taiwan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "italic-resident",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to test on a training group of 7 items, holding out the following subjects:['DEV005' 'DEV014']\n",
      "selecting training data\n",
      "2.5 GiB\n",
      "(91, 109, 91, 374)\n",
      "selecting test data\n",
      "(91, 109, 91, 126)\n",
      "regressing\n",
      "2.5 GiB\n",
      "predicting\n",
      "test score was:\n",
      "0.12762490054536846\n",
      "In order to test on a training group of 7 items, holding out the following subjects:['DEV013' 'DEV009']\n",
      "selecting training data\n",
      "2.5 GiB\n",
      "(91, 109, 91, 375)\n",
      "selecting test data\n",
      "(91, 109, 91, 125)\n",
      "regressing\n",
      "2.5 GiB\n",
      "predicting\n",
      "test score was:\n",
      "-0.002948134659435997\n",
      "In order to test on a training group of 7 items, holding out the following subjects:['DEV010' 'DEV001']\n",
      "selecting training data\n",
      "2.5 GiB\n",
      "(91, 109, 91, 379)\n",
      "selecting test data\n",
      "(91, 109, 91, 121)\n",
      "regressing\n",
      "2.5 GiB\n",
      "predicting\n",
      "test score was:\n",
      "0.3263347157390941\n",
      "In order to test on a training group of 7 items, holding out the following subjects:['DEV006' 'DEV015']\n",
      "selecting training data\n",
      "2.9 GiB\n",
      "(91, 109, 91, 434)\n",
      "selecting test data\n",
      "(91, 109, 91, 66)\n",
      "regressing\n",
      "2.9 GiB\n",
      "predicting\n",
      "test score was:\n",
      "0.2906596592680638\n",
      "In order to test on a training group of 8 items, holding out the following subjects:['DEV012']\n",
      "selecting training data\n",
      "2.9 GiB\n",
      "(91, 109, 91, 438)\n",
      "selecting test data\n",
      "(91, 109, 91, 62)\n",
      "regressing\n",
      "2.9 GiB\n",
      "predicting\n",
      "test score was:\n",
      "-0.7229050446209611\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold,GroupKFold\n",
    "#def cv_train_test_different_sets(averaged_X,averaged_Y, averaged_groups, individual_X,individual_groups, Y, cv,group_list)\n",
    "\"\"\"\n",
    "averaged_X: values grouped\n",
    "averaged_groups: group allocations for the averaged dataset\n",
    "individual_X: values grouped into averages for testing\n",
    "cv: a Grouped cross-validator\n",
    "group_list: name of the groups\n",
    "\"\"\"\n",
    "individual_X = first_subs_nifti#all_subs_nn_nifti\n",
    "individual_y = first_subs_nifti_Y #all_subs_nn_nifti_Y\n",
    "individual_groups = all_subs_nn_nifti_groups[0:500]\n",
    "cv=KFold(n_splits=5)\n",
    "\n",
    "\n",
    "\n",
    "groups_array = np.array(list(set(individual_groups)))\n",
    "\n",
    "#the CV that the inner Regressor uses\n",
    "cv_inner = GroupKFold(3)\n",
    "\n",
    "#we actually use KFold on the group names themselves, then filter across that\n",
    "#that's equivalent to doing a GroupedKFold on the data.\n",
    "test_scores = []\n",
    "for train_i,test_i in cv.split(groups_array):\n",
    "    train_group_items, test_group_items = groups_array[train_i], groups_array[test_i]\n",
    "    print('In order to test on a training group of ' +\n",
    "          str(len(train_group_items)) + ' items, holding out the following subjects:' +\n",
    "          str(test_group_items))\n",
    "    \n",
    "    #select training data from the averages\n",
    "    print('selecting training data')\n",
    "    train_selector = [i for i, x in enumerate(individual_groups) if x in train_group_items]\n",
    "    train_y = individual_y[train_selector]\n",
    "    train_X = nib.funcs.concat_images([individual_X.slicer[...,s] for s in train_selector])\n",
    "    train_groups = individual_groups[train_selector]\n",
    "    print(asizeof_fmt(train_X))\n",
    "    print(train_X.shape)\n",
    "    \n",
    "    #select testing data from the individual values\n",
    "    print('selecting test data')\n",
    "    test_selector = [i for i, x in enumerate(individual_groups) if x in test_group_items]\n",
    "    test_y = individual_y[test_selector]\n",
    "    test_X = nib.funcs.concat_images([individual_X.slicer[...,s] for s in test_selector])\n",
    "    \n",
    "    test_groups = individual_groups[test_selector]\n",
    "    print(test_X.shape)\n",
    "    \n",
    "    print(\"regressing\")\n",
    "    regressor = DecoderRegressor(standardize= True,cv=cv_inner, scoring=\"r2\")\n",
    "    print(asizeof_fmt(train_X))\n",
    "    regressor.fit(y=train_y,X=train_X,groups=train_groups)\n",
    "    \n",
    "    print(\"predicting\")\n",
    "    #now predict on our test split\n",
    "    test_score = regressor.score(test_X,test_y)\n",
    "    test_scores = test_scores+[test_score]\n",
    "    print('test score was:')\n",
    "    print(test_score)\n",
    "    \n",
    "    del test_X\n",
    "    del train_X\n",
    "    gc.collect() #clean up. this is big data we're working with\n",
    "    #https://stackoverflow.com/questions/1316767/how-can-i-explicitly-free-memory-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "alone-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,GroupKFold\n",
    "def cv_train_test_different_sets(\n",
    "    individual_X,individual_groups, individual_y, \n",
    "    averaged_X = None,averaged_y = None, averaged_groups = None,\n",
    "    cv = None):\n",
    "    \"\"\"\n",
    "    averaged_X: values grouped\n",
    "    averaged_groups: group allocations for the averaged dataset\n",
    "    individual_X: values grouped into averages for testing\n",
    "    cv: a Grouped cross-validator\n",
    "    group_list: name of the groups\n",
    "    \"\"\"\n",
    "    if cv is None:\n",
    "        cv=KFold(n_splits=5)\n",
    "\n",
    "    groups_array = np.array(list(set(individual_groups)))\n",
    "    assert(set(averaged_groups)==set(individual_groups))\n",
    "\n",
    "    #the CV that the inner Regressor uses\n",
    "    cv_inner = GroupKFold(3)\n",
    "\n",
    "    #we actually use KFold on the group names themselves, then filter across that\n",
    "    #that's equivalent to doing a GroupedKFold on the data.\n",
    "    test_scores = []\n",
    "    for train_i,test_i in cv.split(groups_array):\n",
    "        train_group_items, test_group_items = groups_array[train_i], groups_array[test_i]\n",
    "        print('In order to test on a training group of ' +\n",
    "              str(len(train_group_items)) + ' items, holding out the following subjects:' +\n",
    "              str(test_group_items))\n",
    "        \n",
    "        \n",
    "        #select training data from the averages\n",
    "        print('selecting training data')\n",
    "        train_selector = [i for i, x in enumerate(averaged_groups) if x in train_group_items]\n",
    "        train_y = averaged_y[train_selector]\n",
    "        train_X = nib.funcs.concat_images([averaged_X.slicer[...,s] for s in train_selector])\n",
    "        train_groups = averaged_groups[train_selector]\n",
    "        print(train_X.shape)\n",
    "        print(asizeof_fmt(train_X))\n",
    "\n",
    "        #select testing data from the individual values\n",
    "        print('selecting test data')\n",
    "        test_selector = [i for i, x in enumerate(individual_groups) if x in test_group_items]\n",
    "        test_y = individual_y[test_selector]\n",
    "        test_X = nib.funcs.concat_images([individual_X.slicer[...,s] for s in test_selector])\n",
    "        test_groups = individual_groups[test_selector]\n",
    "        print(asizeof_fmt(test_X))\n",
    "        print(test_X.shape)\n",
    "\n",
    "\n",
    "        print(\"regressing\")\n",
    "        regressor = DecoderRegressor(standardize= True,cv=cv_inner, scoring=\"r2\")\n",
    "        print(asizeof_fmt(train_X))\n",
    "        regressor.fit(y=train_y,X=train_X,groups=train_groups)\n",
    "\n",
    "        print(\"predicting\")\n",
    "        #now predict on our test split\n",
    "        test_score = regressor.score(test_X,test_y)\n",
    "        test_scores = test_scores+[test_score]\n",
    "        print('test score was:')\n",
    "        print(test_score)\n",
    "\n",
    "        del test_X\n",
    "        del train_X\n",
    "        gc.collect() #clean up. this is big data we're working with\n",
    "        #https://stackoverflow.com/questions/1316767/how-can-i-explicitly-free-memory-in-python\n",
    "\n",
    "    return(test_scores)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fourth-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train_test_same_sets(\n",
    "    individual_X,individual_groups, individual_y, \n",
    "    #averaged_X = None,averaged_y = None, averaged_groups = None,\n",
    "    cv = None):\n",
    "    \"\"\"\n",
    "    averaged_X: values grouped\n",
    "    averaged_groups: group allocations for the averaged dataset\n",
    "    individual_X: values grouped into averages for testing\n",
    "    cv: a Grouped cross-validator\n",
    "    \"\"\"\n",
    "    #individual_X = first_subs_nifti#all_subs_nn_nifti\n",
    "    #individual_y = first_subs_nifti_Y #all_subs_nn_nifti_Y\n",
    "    #individual_groups = all_subs_nn_nifti_groups[0:500]\n",
    "    if cv is None:\n",
    "        cv=KFold(n_splits=5)\n",
    "\n",
    "\n",
    "\n",
    "    groups_array = np.array(list(set(individual_groups)))\n",
    "\n",
    "    #the CV that the inner Regressor uses\n",
    "    cv_inner = GroupKFold(3)\n",
    "\n",
    "    #we actually use KFold on the group names themselves, then filter across that\n",
    "    #that's equivalent to doing a GroupedKFold on the data.\n",
    "    test_scores = []\n",
    "    for train_i,test_i in cv.split(groups_array):\n",
    "        train_group_items, test_group_items = groups_array[train_i], groups_array[test_i]\n",
    "        print('In order to test on a training group of ' +\n",
    "              str(len(train_group_items)) + ' items, holding out the following subjects:' +\n",
    "              str(test_group_items))\n",
    "\n",
    "        #select training data from the averages\n",
    "        print('selecting training data')\n",
    "        train_selector = [i for i, x in enumerate(individual_groups) if x in train_group_items]\n",
    "        train_y = individual_y[train_selector]\n",
    "        train_X = nib.funcs.concat_images([individual_X.slicer[...,s] for s in train_selector])\n",
    "        train_groups = individual_groups[train_selector]\n",
    "        print(asizeof_fmt(train_X))\n",
    "        print(train_X.shape)\n",
    "\n",
    "        #select testing data from the individual values\n",
    "        print('selecting test data')\n",
    "        test_selector = [i for i, x in enumerate(individual_groups) if x in test_group_items]\n",
    "        test_y = individual_y[test_selector]\n",
    "        test_X = nib.funcs.concat_images([individual_X.slicer[...,s] for s in test_selector])\n",
    "\n",
    "        test_groups = individual_groups[test_selector]\n",
    "        print(test_X.shape)\n",
    "\n",
    "        print(\"regressing\")\n",
    "        regressor = DecoderRegressor(standardize= True,cv=cv_inner, scoring=\"r2\")\n",
    "        print(asizeof_fmt(train_X))\n",
    "        regressor.fit(y=train_y,X=train_X,groups=train_groups)\n",
    "\n",
    "        print(\"predicting\")\n",
    "        #now predict on our test split\n",
    "        test_score = regressor.score(test_X,test_y)\n",
    "        test_scores = test_scores+[test_score]\n",
    "        print('test score was:')\n",
    "        print(test_score)\n",
    "\n",
    "        del test_X\n",
    "        del train_X\n",
    "        gc.collect() #clean up. this is big data we're working with\n",
    "    return(test_scores)\n",
    "        #https://stackoverflow.com/questions/1316767/how-can-i-explicitly-free-memory-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "atlantic-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_subs_nifti = all_subs_nn_nifti.slicer[...,0:500]\n",
    "first_subs_nifti_Y = all_subs_nn_nifti_Y[0:500]\n",
    "first_subs_nifti = nil.image.clean_img(first_subs_nifti,detrend=False,standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "described-brass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to test on a training group of 7 items, holding out the following subjects:['DEV005' 'DEV014']\n",
      "selecting training data\n",
      "selecting test data\n",
      "(91, 109, 91, 126)\n",
      "regressing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-65125189d31b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m test_scores = cv_train_test_different_sets(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mindividual_X\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_subs_nifti\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mindividual_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_subs_nifti_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     individual_groups=all_subs_nn_nifti_groups[0:500])\n",
      "\u001b[0;32m<ipython-input-41-cc35f37548cd>\u001b[0m in \u001b[0;36mcv_train_test_different_sets\u001b[0;34m(individual_X, individual_groups, individual_y, averaged_X, averaged_y, averaged_groups, cv)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"regressing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_inner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predicting\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py\u001b[0m in \u001b[0;36m_apply_mask\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;31m# Nifti masking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_embedded_nifti_masker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_subject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_img_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_img_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/input_data/base_masker.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, confounds, **fit_params)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_img\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                 return self.fit(X, **fit_params\n\u001b[0m\u001b[1;32m    228\u001b[0m                                 ).transform(X, confounds=confounds)\n\u001b[1;32m    229\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/input_data/base_masker.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, imgs, confounds)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfounds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh_variance_confounds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_single_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# Compute high variance confounds if requested\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/input_data/nifti_masker.py\u001b[0m in \u001b[0;36mtransform_single_imgs\u001b[0;34m(self, imgs, confounds, copy)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             ignore=['mask_img', 'mask_args', 'mask_strategy'])\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         data = self._cache(filter_and_mask,\n\u001b[0m\u001b[1;32m    444\u001b[0m                            ignore=['verbose', 'memory', 'memory_level',\n\u001b[1;32m    445\u001b[0m                                    'copy'],\n",
      "\u001b[0;32m~/.conda/envs/neuralsignature/lib/python3.8/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/input_data/nifti_masker.py\u001b[0m in \u001b[0;36mfilter_and_mask\u001b[0;34m(imgs, mask_img_, parameters, memory_level, memory, verbose, confounds, copy, dtype)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_affine'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_img_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     data, affine = filter_and_extract(imgs, _ExtractionFunctor(mask_img_),\n\u001b[0m\u001b[1;32m     69\u001b[0m                                       \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                                       \u001b[0mmemory_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/input_data/base_masker.py\u001b[0m in \u001b[0;36mfilter_and_extract\u001b[0;34m(imgs, extraction_function, parameters, memory_level, memory, verbose, confounds, copy, dtype)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[%s] Cleaning extracted signals\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sessions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     region_signals = cache(\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_memory_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mmemory_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neuralsignature/lib/python3.8/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/signal.py\u001b[0m in \u001b[0;36mclean\u001b[0;34m(signals, sessions, detrend, standardize, confounds, standardize_confounds, low_pass, high_pass, t_r, ensure_finite)\u001b[0m\n\u001b[1;32m    518\u001b[0m                          .format(ensure_finite))\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m     \u001b[0msignals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0msignals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_scores = cv_train_test_different_sets(\n",
    "    individual_X=first_subs_nifti,\n",
    "    individual_y=first_subs_nifti_Y,\n",
    "    individual_groups=all_subs_nn_nifti_groups[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-selection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = cv_train_test_same_sets(\n",
    "    individual_X=first_subs_nifti,\n",
    "    individual_y=first_subs_nifti_Y,\n",
    "    individual_groups=all_subs_nn_nifti_groups[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Nested cross-validation\n",
    "\n",
    "See for instance: http://nilearn.github.io/auto_examples/02_decoding/plot_haxby_grid_search.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-today",
   "metadata": {},
   "source": [
    "\n",
    "Issues here:\n",
    "\n",
    " - Need to get the cross-validation right. We have got it working but there is an inner validation that selects a decoder for each group. Does that make sense? I think so, but just need to consider it a bit carefully.\n",
    " - What are the individual methods that inner validation is running? Should keep track of that\n",
    " - We really need to compare this to a baseline. So we need to run an individual-level analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-puppy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-neuralsignature]",
   "language": "python",
   "name": "conda-env-.conda-neuralsignature-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
