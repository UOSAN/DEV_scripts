{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "substantial-crawford",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/datasets/__init__.py:87: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltools as nlt\n",
    "import nilearn as nil\n",
    "import nibabel as nib\n",
    "import warnings\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import dev_wtp_io_utils\n",
    "import gc #garbage collection\n",
    "from nilearn import plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "agreed-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-moore",
   "metadata": {},
   "source": [
    "### Load brain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "labeled-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_set = pd.read_csv(\"../data/train_test_markers_20210601T183243.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "close-robertson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked for intersection and no intersection between the brain data and the subjects was found.\n",
      "there were 60 subjects overlapping between the subjects marked for train data and the training dump file itself.\n"
     ]
    }
   ],
   "source": [
    "with open('../data/Brain_Data_2sns_60subs.pkl', 'rb') as pkl_file:\n",
    "    Brain_Data_allsubs = pickle.load(pkl_file)\n",
    "    \n",
    "dev_wtp_io_utils.check_BD_against_test_train_set(Brain_Data_allsubs,test_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "progressive-novel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked for intersection and no intersection between the brain data and the subjects was found.\n",
      "there were 60 subjects overlapping between the subjects marked for train data and the training dump file itself.\n"
     ]
    }
   ],
   "source": [
    "with open('../data/Brain_Data_2sns_60subs_grouped.pkl', 'rb') as pkl_file:\n",
    "    Brain_Data_allsubs_grouped = pickle.load(pkl_file)\n",
    "    \n",
    "dev_wtp_io_utils.check_BD_against_test_train_set(Brain_Data_allsubs_grouped,test_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-mouse",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "extended-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0    1164\n",
      "6.0    1018\n",
      "7.0     904\n",
      "8.0     604\n",
      "Name: response, dtype: int64\n",
      "5.0    1164\n",
      "6.0    1018\n",
      "7.0     904\n",
      "8.0     604\n",
      "Name: response, dtype: int64\n",
      "False    3690\n",
      "True      150\n",
      "Name: response, dtype: int64\n",
      "3690\n",
      "3840\n"
     ]
    }
   ],
   "source": [
    "Brain_Data_allsubs.Y = Brain_Data_allsubs.X.response.copy()\n",
    "print(Brain_Data_allsubs.Y.value_counts())\n",
    "Brain_Data_allsubs.Y[Brain_Data_allsubs.Y=='NULL']=None\n",
    "print(Brain_Data_allsubs.Y.value_counts())\n",
    "print(Brain_Data_allsubs.Y.isnull().value_counts())\n",
    "Brain_Data_allsubs_nn = Brain_Data_allsubs[Brain_Data_allsubs.Y.isnull()==False]\n",
    "print(len(Brain_Data_allsubs_nn))\n",
    "print(len(Brain_Data_allsubs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "noble-observer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       DEV001\n",
       "1       DEV001\n",
       "2       DEV001\n",
       "3       DEV001\n",
       "4       DEV001\n",
       "         ...  \n",
       "3685    DEV089\n",
       "3686    DEV089\n",
       "3687    DEV089\n",
       "3688    DEV089\n",
       "3689    DEV089\n",
       "Name: subject, Length: 3690, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subs_nn_nifti = Brain_Data_allsubs_nn.to_nifti()\n",
    "all_subs_nn_nifti_Y = Brain_Data_allsubs_nn.Y\n",
    "all_subs_nn_nifti_groups = Brain_Data_allsubs_nn.X.subject\n",
    "all_subs_nn_nifti_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "listed-strap",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0    236\n",
      "7.0    235\n",
      "5.0    227\n",
      "8.0    202\n",
      "Name: response, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      DEV001\n",
       "1      DEV001\n",
       "2      DEV001\n",
       "3      DEV001\n",
       "4      DEV001\n",
       "        ...  \n",
       "895    DEV089\n",
       "896    DEV089\n",
       "897    DEV089\n",
       "898    DEV089\n",
       "899    DEV089\n",
       "Name: subject, Length: 900, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Brain_Data_allsubs_grouped.Y = Brain_Data_allsubs_grouped.X.response.copy()\n",
    "print(Brain_Data_allsubs_grouped.Y.value_counts())\n",
    "all_subs_grouped_nifti = Brain_Data_allsubs_grouped.to_nifti()\n",
    "all_subs_grouped_nifti_Y = Brain_Data_allsubs_grouped.Y\n",
    "all_subs_grouped_nifti_groups = Brain_Data_allsubs_grouped.X.subject\n",
    "all_subs_grouped_nifti_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-basin",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Regressing in nilearn:\n",
    " - https://nilearn.github.io/decoding/estimator_choice.html\n",
    " - http://www.ncbi.nlm.nih.gov/pubmed/20691790\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-flight",
   "metadata": {},
   "source": [
    "OK, so that's how you do it. It's pretty straightforward.\n",
    "\n",
    "So...we won't look at nested cross-validation juuust yet, because the next step is to work out how to train on one set and predict on another. that will definitely require a custom pipeline. Let's get started..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "removable-transfer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del Brain_Data_allsubs\n",
    "del Brain_Data_allsubs_grouped\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "institutional-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decoding import DecoderRegressor\n",
    "dRegressor = DecoderRegressor(estimator = 'ridge_regressor', standardize= True,scoring=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "least-quarter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___: 232614\n",
      "all_subs_nn_nifti_groups: 232614\n",
      "_7: 232614\n",
      "__: 56844\n",
      "all_subs_grouped_nifti_groups: 56844\n",
      "_8: 56844\n",
      "all_subs_nn_nifti_Y: 29664\n",
      "test_train_set: 9549\n",
      "all_subs_grouped_nifti_Y: 7344\n",
      "DecoderRegressor: 1472\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "    print(name + ': ' + str(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-nitrogen",
   "metadata": {},
   "source": [
    "As a control, we'll try this again, this time just training and testing on individual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "listed-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler.asizeof import asizeof\n",
    "\n",
    "def asizeof_fmt(obj, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    num = asizeof(obj)\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "# for name, size in sorted(((name, asizeof(value)) for name, value in locals().items()),\n",
    "#                          key= lambda x: -x[1])[:10]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "olive-venezuela",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3 GiB'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asizeof_fmt(Brain_Data_allsubs_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "antique-favor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.4 GiB'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asizeof_fmt(all_subs_nn_nifti)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-gamma",
   "metadata": {},
   "source": [
    "get a small sample of subjects to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "subsequent-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_subject_items = np.unique(all_subs_nn_nifti_groups)[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "antique-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_subject_vector = [i for i, x in enumerate(all_subs_nn_nifti_groups) if x in sample_subject_items]\n",
    "sample_grouped_subject_vector = [i for i, x in enumerate(all_subs_grouped_nifti_groups) if x in sample_subject_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "economic-thought",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "897"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_subject_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-click",
   "metadata": {},
   "source": [
    "and now extract them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "opened-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_subs_nifti = nib.funcs.concat_images([all_subs_nn_nifti.slicer[...,s] for s in sample_subject_vector])\n",
    "first_subs_nifti_Y = all_subs_nn_nifti_Y[sample_subject_vector]\n",
    "first_subs_nifti = nil.image.clean_img(first_subs_nifti,detrend=False,standardize=True)\n",
    "first_subs_nifti_groups = all_subs_nn_nifti_groups[sample_subject_vector]\n",
    "\n",
    "del all_subs_nn_nifti\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "imposed-thomson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "korean-schema",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 109, 91, 897)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_subs_nifti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "false-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_subs_grouped_nifti = nib.funcs.concat_images([all_subs_grouped_nifti.slicer[...,s] for s in sample_grouped_subject_vector])\n",
    "first_subs_grouped_nifti_Y = all_subs_grouped_nifti_Y[sample_grouped_subject_vector]\n",
    "first_subs_grouped_nifti = nil.image.clean_img(first_subs_grouped_nifti,detrend=False,standardize=True)\n",
    "first_subs_grouped_nifti_groups = all_subs_grouped_nifti_groups[sample_grouped_subject_vector]\n",
    "\n",
    "del all_subs_grouped_nifti\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dangerous-pixel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 109, 91, 223)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_subs_grouped_nifti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sustainable-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "alone-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cv_train_test_different_sets(\n",
    "    individual_X,individual_groups, individual_y, \n",
    "    averaged_X = None,averaged_y = None, averaged_groups = None,\n",
    "    cv = None,regressorGenerator=None):\n",
    "    \"\"\"\n",
    "    averaged_X: values grouped\n",
    "    averaged_groups: group allocations for the averaged dataset\n",
    "    individual_X: values grouped into averages for testing\n",
    "    cv: a Grouped cross-validator\n",
    "    group_list: name of the groups\n",
    "    \"\"\"\n",
    "    if cv is None:\n",
    "        cv=KFold(n_splits=5)\n",
    "\n",
    "    groups_array = np.array(list(set(individual_groups)))\n",
    "    assert(set(averaged_groups)==set(individual_groups))\n",
    "\n",
    "    #the CV that the inner Regressor uses\n",
    "    cv_inner = GroupKFold(2)\n",
    "\n",
    "    #we actually use KFold on the group names themselves, then filter across that\n",
    "    #that's equivalent to doing a GroupedKFold on the data.\n",
    "    test_scores = []\n",
    "    for train_i,test_i in cv.split(groups_array):\n",
    "        train_group_items, test_group_items = groups_array[train_i], groups_array[test_i]\n",
    "        print('In order to test on a training group of ' +\n",
    "              str(len(train_group_items)) + ' items, holding out the following subjects:' +\n",
    "              str(test_group_items),end='. ')\n",
    "        \n",
    "        \n",
    "        #select training data from the averages\n",
    "        print('selecting training data',end='. ')\n",
    "        train_selector = [i for i, x in enumerate(averaged_groups) if x in train_group_items]\n",
    "        train_y = averaged_y[train_selector]\n",
    "        train_X = nib.funcs.concat_images([averaged_X.slicer[...,s] for s in train_selector])\n",
    "        train_groups = averaged_groups[train_selector]\n",
    "        print(train_X.shape,end='. ')\n",
    "        print(asizeof_fmt(train_X),end='. ')\n",
    "\n",
    "        #select testing data from the individual values\n",
    "        print('selecting test data',end='. ')\n",
    "        test_selector = [i for i, x in enumerate(individual_groups) if x in test_group_items]\n",
    "        test_y = individual_y[test_selector]\n",
    "        test_X = nib.funcs.concat_images([individual_X.slicer[...,s] for s in test_selector])\n",
    "        test_groups = individual_groups[test_selector]\n",
    "        print(asizeof_fmt(test_X),end='. ')\n",
    "        print(test_X.shape,end='. ')\n",
    "\n",
    "\n",
    "        print(\"regressing\",end='. ')\n",
    "        regressor = DecoderRegressor(standardize= True,cv=cv_inner, scoring=\"r2\")\n",
    "        print(asizeof_fmt(train_X),end='. ')\n",
    "        regressor.fit(y=train_y,X=train_X,groups=train_groups)\n",
    "\n",
    "        print(\"predicting\",end='. ')\n",
    "        #now predict on our test split\n",
    "        test_score = regressor.score(test_X,test_y)\n",
    "        test_scores = test_scores+[test_score]\n",
    "        print('test score was:',end='. ')\n",
    "        print(test_score)\n",
    "\n",
    "        del test_X\n",
    "        del train_X\n",
    "        gc.collect() #clean up. this is big data we're working with\n",
    "        #https://stackoverflow.com/questions/1316767/how-can-i-explicitly-free-memory-in-python\n",
    "\n",
    "    return(test_scores)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fourth-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train_test_same_sets(\n",
    "    individual_X,individual_groups, individual_y, \n",
    "    #averaged_X = None,averaged_y = None, averaged_groups = None,\n",
    "    cv = None):\n",
    "    \"\"\"\n",
    "    averaged_X: values grouped\n",
    "    averaged_groups: group allocations for the averaged dataset\n",
    "    individual_X: values grouped into averages for testing\n",
    "    cv: a Grouped cross-validator\n",
    "    \"\"\"\n",
    "    #individual_X = first_subs_nifti#all_subs_nn_nifti\n",
    "    #individual_y = first_subs_nifti_Y #all_subs_nn_nifti_Y\n",
    "    #individual_groups = all_subs_nn_nifti_groups[0:500]\n",
    "    if cv is None:\n",
    "        cv=KFold(n_splits=5)\n",
    "\n",
    "\n",
    "\n",
    "    groups_array = np.array(list(set(individual_groups)))\n",
    "\n",
    "    #the CV that the inner Regressor uses\n",
    "    cv_inner = GroupKFold(2)\n",
    "\n",
    "    #we actually use KFold on the group names themselves, then filter across that\n",
    "    #that's equivalent to doing a GroupedKFold on the data.\n",
    "    test_scores = []\n",
    "    for train_i,test_i in cv.split(groups_array):\n",
    "        train_group_items, test_group_items = groups_array[train_i], groups_array[test_i]\n",
    "        print('In order to test on a training group of ' +\n",
    "              str(len(train_group_items)) + ' items, holding out the following subjects:' +\n",
    "              str(test_group_items),end='. ')\n",
    "\n",
    "        #select training data from the averages\n",
    "        print('selecting training data',end='. ')\n",
    "        train_selector = [i for i, x in enumerate(individual_groups) if x in train_group_items]\n",
    "        train_y = individual_y[train_selector]\n",
    "        train_X = nib.funcs.concat_images([individual_X.slicer[...,s] for s in train_selector])\n",
    "        train_groups = individual_groups[train_selector]\n",
    "        print(asizeof_fmt(train_X),end='. ')\n",
    "        print(train_X.shape,end='. ')\n",
    "\n",
    "        #select testing data from the individual values\n",
    "        print('selecting test data',end='. ')\n",
    "        test_selector = [i for i, x in enumerate(individual_groups) if x in test_group_items]\n",
    "        test_y = individual_y[test_selector]\n",
    "        test_X = nib.funcs.concat_images([individual_X.slicer[...,s] for s in test_selector])\n",
    "\n",
    "        test_groups = individual_groups[test_selector]\n",
    "        print(test_X.shape,end='. ')\n",
    "\n",
    "        print(\"regressing\",end='. ')\n",
    "        #regressor = DecoderRegressor(standardize= True,cv=cv_inner, scoring=\"r2\")\n",
    "        regressor = DecoderRegressor(standardize= True,cv=cv_inner, scoring=\"r2\")\n",
    "        print(asizeof_fmt(train_X),end='. ')\n",
    "        regressor.fit(y=train_y,X=train_X,groups=train_groups)\n",
    "\n",
    "        print(\"predicting\",end='. ')\n",
    "        #now predict on our test split\n",
    "        test_score = regressor.score(test_X,test_y)\n",
    "        test_scores = test_scores+[test_score]\n",
    "        print('test score was:',end='. ')\n",
    "        print(test_score)\n",
    "\n",
    "        del test_X\n",
    "        del train_X\n",
    "        gc.collect() #clean up. this is big data we're working with\n",
    "    return(test_scores)\n",
    "        #https://stackoverflow.com/questions/1316767/how-can-i-explicitly-free-memory-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "respective-poultry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to test on a training group of 12 items, holding out the following subjects:['DEV010' 'DEV019' 'DEV021']. selecting training data. 4.9 GiB. (91, 109, 91, 733). selecting test data. (91, 109, 91, 164). regressing. 4.9 GiB. predicting. test score was:. 0.21853039978740596\n",
      "In order to test on a training group of 12 items, holding out the following subjects:['DEV006' 'DEV012' 'DEV005']. selecting training data. 4.8 GiB. (91, 109, 91, 711). selecting test data. (91, 109, 91, 186). regressing. 4.8 GiB. predicting. test score was:. 0.21890934756666858\n",
      "In order to test on a training group of 12 items, holding out the following subjects:['DEV013' 'DEV017' 'DEV001']. selecting training data. 4.8 GiB. (91, 109, 91, 720). selecting test data. (91, 109, 91, 177). regressing. 4.8 GiB. predicting. test score was:. 0.09647748341542672\n",
      "In order to test on a training group of 12 items, holding out the following subjects:['DEV022' 'DEV009' 'DEV015']. selecting training data. 4.8 GiB. (91, 109, 91, 716). selecting test data. (91, 109, 91, 181). regressing. 4.8 GiB. predicting. test score was:. 0.16768299712319068\n",
      "In order to test on a training group of 12 items, holding out the following subjects:['DEV014' 'DEV016' 'DEV018']. selecting training data. 4.8 GiB. (91, 109, 91, 708). selecting test data. (91, 109, 91, 189). regressing. 4.8 GiB. predicting. test score was:. 0.26106811642073835\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-8a7f0bdd74f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_scores' is not defined"
     ]
    }
   ],
   "source": [
    "test_scores_same = cv_train_test_same_sets(\n",
    "    individual_X=first_subs_nifti,\n",
    "    individual_y=first_subs_nifti_Y,\n",
    "    individual_groups=first_subs_nifti_groups\n",
    ")\n",
    "\n",
    "print(test_scores_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "accepted-temple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21853039978740596, 0.21890934756666858, 0.09647748341542672, 0.16768299712319068, 0.26106811642073835]\n"
     ]
    }
   ],
   "source": [
    "print(test_scores_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "valid-mongolia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12399989268473066"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(test_scores_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "described-brass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to test on a training group of 12 items, holding out the following subjects:['DEV010' 'DEV019' 'DEV021']. selecting training data. (91, 109, 91, 179). 1.2 GiB. selecting test data. 1.1 GiB. (91, 109, 91, 164). regressing. 1.2 GiB. predicting. test score was:. 0.1873604351084126\n",
      "In order to test on a training group of 12 items, holding out the following subjects:['DEV006' 'DEV012' 'DEV005']. selecting training data. (91, 109, 91, 178). 1.2 GiB. selecting test data. 1.3 GiB. (91, 109, 91, 186). regressing. 1.2 GiB. predicting. test score was:. 0.09609407198510678\n",
      "In order to test on a training group of 12 items, holding out the following subjects:['DEV013' 'DEV017' 'DEV001']. selecting training data. (91, 109, 91, 181). 1.2 GiB. selecting test data. 1.2 GiB. (91, 109, 91, 177). regressing. 1.2 GiB. predicting. test score was:. -0.07183434134739297\n",
      "In order to test on a training group of 12 items, holding out the following subjects:['DEV022' 'DEV009' 'DEV015']. selecting training data. (91, 109, 91, 178). 1.2 GiB. selecting test data. 1.2 GiB. (91, 109, 91, 181). regressing. 1.2 GiB. predicting. test score was:. 0.07951115382986251\n",
      "In order to test on a training group of 12 items, holding out the following subjects:['DEV014' 'DEV016' 'DEV018']. selecting training data. (91, 109, 91, 176). 1.2 GiB. selecting test data. 1.3 GiB. (91, 109, 91, 189). regressing. 1.2 GiB. predicting. test score was:. 0.12834946390442026\n"
     ]
    }
   ],
   "source": [
    "test_scores = cv_train_test_different_sets(\n",
    "    averaged_X=first_subs_grouped_nifti,\n",
    "    averaged_y=first_subs_grouped_nifti_Y,\n",
    "    averaged_groups=first_subs_grouped_nifti_groups,\n",
    "    individual_X=first_subs_nifti,\n",
    "    individual_y=first_subs_nifti_Y,\n",
    "    individual_groups=first_subs_nifti_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "removed-transportation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1873604351084126,\n",
       " 0.09609407198510678,\n",
       " -0.07183434134739297,\n",
       " 0.07951115382986251,\n",
       " 0.12834946390442026]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "paperback-collect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08389615669608183"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-peoples",
   "metadata": {},
   "source": [
    "#### Nested cross-validation\n",
    "\n",
    "See for instance: http://nilearn.github.io/auto_examples/02_decoding/plot_haxby_grid_search.html\n",
    "        \n",
    "See also https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/\n",
    "\n",
    "and this thread: https://neurostars.org/t/nested-cross-validation/19459\n",
    "\n",
    "The Haxby code:\n",
    "\n",
    "(1) Loops through each outer loop\n",
    "(2) And for each outer loop, loops again through each parameter\n",
    "(3) And then estimates three times on the _training_ data for that\n",
    "(4) Then gets a prediction score for that hyperparameter configuration\n",
    "\n",
    "This is fundamentally the same as the nested CV in Machine Learning Mastery, with one exception. The Haxby code calculates a prediction on the test set once per hyperparameter. The MLM code calculates a prediction on the test set once per outer split. There is more stuff going on in the GridSearchCV but you don't see that.\n",
    "\n",
    "The Decoder object actually as a param_grid command so this could be useful for doing the parameter search properly, in the inner loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-today",
   "metadata": {},
   "source": [
    "\n",
    "Issues here:\n",
    "\n",
    " - Need to get the cross-validation right. We have got it working but there is an inner validation that selects a decoder for each group. Does that make sense? I think so, but just need to consider it a bit carefully.\n",
    " - What are the individual methods that inner validation is running? Should keep track of that\n",
    " - We really need to compare this to a baseline. So we need to run an individual-level analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-puppy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-neuralsignature]",
   "language": "python",
   "name": "conda-env-.conda-neuralsignature-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
