{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "substantial-crawford",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/datasets/__init__.py:87: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltools as nlt\n",
    "import nilearn as nil\n",
    "import nibabel as nib\n",
    "import warnings\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import dev_wtp_io_utils\n",
    "import gc #garbage collection\n",
    "from nilearn import plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "agreed-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-moore",
   "metadata": {},
   "source": [
    "### Load brain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "labeled-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_set = pd.read_csv(\"../data/train_test_markers_20210601T183243.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "close-robertson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked for intersection and no intersection between the brain data and the subjects was found.\n",
      "there were 60 subjects overlapping between the subjects marked for train data and the training dump file itself.\n"
     ]
    }
   ],
   "source": [
    "with open('../data/Brain_Data_2sns_60subs.pkl', 'rb') as pkl_file:\n",
    "    Brain_Data_allsubs = pickle.load(pkl_file)\n",
    "    \n",
    "dev_wtp_io_utils.check_BD_against_test_train_set(Brain_Data_allsubs,test_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "careful-palace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked for intersection and no intersection between the brain data and the subjects was found.\n",
      "there were 60 subjects overlapping between the subjects marked for train data and the training dump file itself.\n"
     ]
    }
   ],
   "source": [
    "with open('../data/Brain_Data_2sns_60subs_grouped.pkl', 'rb') as pkl_file:\n",
    "    Brain_Data_allsubs_grouped = pickle.load(pkl_file)\n",
    "    \n",
    "dev_wtp_io_utils.check_BD_against_test_train_set(Brain_Data_allsubs_grouped,test_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-mouse",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "extended-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0    1164\n",
      "6.0    1018\n",
      "7.0     904\n",
      "8.0     604\n",
      "Name: response, dtype: int64\n",
      "5.0    1164\n",
      "6.0    1018\n",
      "7.0     904\n",
      "8.0     604\n",
      "Name: response, dtype: int64\n",
      "False    3690\n",
      "True      150\n",
      "Name: response, dtype: int64\n",
      "3690\n",
      "3840\n"
     ]
    }
   ],
   "source": [
    "Brain_Data_allsubs.Y = Brain_Data_allsubs.X.response.copy()\n",
    "print(Brain_Data_allsubs.Y.value_counts())\n",
    "Brain_Data_allsubs.Y[Brain_Data_allsubs.Y=='NULL']=None\n",
    "print(Brain_Data_allsubs.Y.value_counts())\n",
    "print(Brain_Data_allsubs.Y.isnull().value_counts())\n",
    "Brain_Data_allsubs_nn = Brain_Data_allsubs[Brain_Data_allsubs.Y.isnull()==False]\n",
    "print(len(Brain_Data_allsubs_nn))\n",
    "print(len(Brain_Data_allsubs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subs_nn_nifti = Brain_Data_allsubs_nn.to_nifti()\n",
    "all_subs_nn_nifti_Y = Brain_Data_allsubs_nn.Y\n",
    "all_subs_nn_nifti_groups = Brain_Data_allsubs_nn.X.subject\n",
    "all_subs_nn_nifti_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brain_Data_allsubs_grouped.Y = Brain_Data_allsubs_grouped.X.response.copy()\n",
    "print(Brain_Data_allsubs_grouped.Y.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "featured-particular",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      DEV001\n",
       "1      DEV001\n",
       "2      DEV001\n",
       "3      DEV001\n",
       "4      DEV001\n",
       "        ...  \n",
       "895    DEV089\n",
       "896    DEV089\n",
       "897    DEV089\n",
       "898    DEV089\n",
       "899    DEV089\n",
       "Name: subject, Length: 900, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subs_grouped_nifti = Brain_Data_allsubs_grouped.to_nifti()\n",
    "all_subs_grouped_nifti_Y = Brain_Data_allsubs_grouped.Y\n",
    "all_subs_grouped_nifti_groups = Brain_Data_allsubs_grouped.X.subject\n",
    "all_subs_grouped_nifti_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-basin",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Regressing in nilearn:\n",
    " - https://nilearn.github.io/decoding/estimator_choice.html\n",
    " - http://www.ncbi.nlm.nih.gov/pubmed/20691790\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-flight",
   "metadata": {},
   "source": [
    "OK, so that's how you do it. It's pretty straightforward.\n",
    "\n",
    "So...we won't look at nested cross-validation juuust yet, because the next step is to work out how to train on one set and predict on another. that will definitely require a custom pipeline. Let's get started..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "removable-transfer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del Brain_Data_allsubs\n",
    "del Brain_Data_allsubs_grouped\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "institutional-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decoding import DecoderRegressor\n",
    "dRegressor = DecoderRegressor(estimator = 'ridge_regressor', standardize= True,scoring=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "stylish-swift",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to test on a training group of 48 items, holding out the following subjects:['DEV069' 'DEV018' 'DEV006' 'DEV024' 'DEV043' 'DEV019' 'DEV049' 'DEV058'\n",
      " 'DEV016' 'DEV074' 'DEV087' 'DEV025']\n",
      "selecting training data\n",
      "selecting test data\n",
      "(91, 109, 91, 717)\n",
      "regressing\n",
      "predicting\n",
      "test score was:\n",
      "-0.018235151306595476\n",
      "In order to test on a training group of 48 items, holding out the following subjects:['DEV027' 'DEV052' 'DEV068' 'DEV039' 'DEV077' 'DEV055' 'DEV017' 'DEV061'\n",
      " 'DEV013' 'DEV015' 'DEV041' 'DEV060']\n",
      "selecting training data\n",
      "selecting test data\n",
      "(91, 109, 91, 727)\n",
      "regressing\n",
      "predicting\n",
      "test score was:\n",
      "0.07224740279241271\n",
      "In order to test on a training group of 48 items, holding out the following subjects:['DEV085' 'DEV086' 'DEV036' 'DEV076' 'DEV010' 'DEV028' 'DEV029' 'DEV014'\n",
      " 'DEV046' 'DEV012' 'DEV021' 'DEV040']\n",
      "selecting training data\n",
      "selecting test data\n",
      "(91, 109, 91, 745)\n",
      "regressing\n",
      "predicting\n",
      "test score was:\n",
      "0.0624101864901383\n",
      "In order to test on a training group of 48 items, holding out the following subjects:['DEV001' 'DEV071' 'DEV084' 'DEV042' 'DEV009' 'DEV057' 'DEV079' 'DEV089'\n",
      " 'DEV056' 'DEV083' 'DEV035' 'DEV073']\n",
      "selecting training data\n",
      "selecting test data\n",
      "(91, 109, 91, 752)\n",
      "regressing\n",
      "predicting\n",
      "test score was:\n",
      "0.08293314206747404\n",
      "In order to test on a training group of 48 items, holding out the following subjects:['DEV022' 'DEV030' 'DEV034' 'DEV026' 'DEV059' 'DEV062' 'DEV023' 'DEV053'\n",
      " 'DEV005' 'DEV051' 'DEV050' 'DEV048']\n",
      "selecting training data\n",
      "selecting test data\n",
      "(91, 109, 91, 749)\n",
      "regressing\n",
      "predicting\n",
      "test score was:\n",
      "0.14220621477849427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold,GroupKFold\n",
    "#def cv_train_test_different_sets(averaged_X,averaged_Y, averaged_groups, individual_X,individual_groups, Y, cv,group_list)\n",
    "\"\"\"\n",
    "averaged_X: values grouped\n",
    "averaged_groups: group allocations for the averaged dataset\n",
    "individual_X: values grouped into averages for testing\n",
    "cv: a Grouped cross-validator\n",
    "group_list: name of the groups\n",
    "\"\"\"\n",
    "averaged_X = all_subs_grouped_nifti\n",
    "averaged_y = all_subs_grouped_nifti_Y\n",
    "averaged_groups = all_subs_grouped_nifti_groups\n",
    "individual_X = all_subs_nn_nifti\n",
    "individual_y = all_subs_nn_nifti_Y\n",
    "individual_groups = all_subs_nn_nifti_groups\n",
    "cv=KFold(n_splits=5)\n",
    "\n",
    "#what we're really doing here is splitting on subjects, so let's make that explicit\n",
    "#make sure the two groups are equal\n",
    "assert(set(averaged_groups)==set(individual_groups))\n",
    "\n",
    "groups_array = np.array(list(set(averaged_groups)))\n",
    "\n",
    "#the CV that the inner Regressor uses\n",
    "cv_inner = GroupKFold(3)\n",
    "\n",
    "#we actually use KFold on the group names themselves, then filter across that\n",
    "#that's equivalent to doing a GroupedKFold on the data.\n",
    "test_scores = []\n",
    "for train_i,test_i in cv.split(groups_array):\n",
    "    train_group_items, test_group_items = groups_array[train_i], groups_array[test_i]\n",
    "    print('In order to test on a training group of ' +\n",
    "          str(len(train_group_items)) + ' items, holding out the following subjects:' +\n",
    "          str(test_group_items))\n",
    "    \n",
    "    #select training data from the averages\n",
    "    print('selecting training data')\n",
    "    train_selector = [i for i, x in enumerate(averaged_groups) if x in train_group_items]\n",
    "    train_y = averaged_y[train_selector]\n",
    "    train_X = nib.funcs.concat_images([averaged_X.slicer[...,s] for s in train_selector])\n",
    "    train_groups = averaged_groups[train_selector]\n",
    "    \n",
    "    #select testing data from the individual values\n",
    "    print('selecting test data')\n",
    "    test_selector = [i for i, x in enumerate(individual_groups) if x in test_group_items]\n",
    "    test_y = individual_y[test_selector]\n",
    "    test_X = nib.funcs.concat_images([individual_X.slicer[...,s] for s in test_selector])\n",
    "    test_groups = individual_groups[test_selector]\n",
    "    print(test_X.shape)\n",
    "    \n",
    "    print(\"regressing\")\n",
    "    regressor = DecoderRegressor(standardize= True,cv=cv_inner, scoring=\"r2\")\n",
    "    regressor.fit(y=train_y,X=train_X,groups=train_groups)\n",
    "    \n",
    "    print(\"predicting\")\n",
    "    #now predict on our test split\n",
    "    test_score = regressor.score(test_X,test_y)\n",
    "    test_scores = test_scores+[test_score]\n",
    "    print('test score was:')\n",
    "    print(test_score)\n",
    "    \n",
    "    del test_X\n",
    "    del train_X\n",
    "    gc.collect() #clean up. this is big data we're working with\n",
    "    #https://stackoverflow.com/questions/1316767/how-can-i-explicitly-free-memory-in-python\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-nitrogen",
   "metadata": {},
   "source": [
    "As a control, we'll try this again, this time just training and testing on individual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "listed-glory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           _i1:  587.0 B\n",
      "                           _oh:  232.0 B\n",
      "                           Out:  232.0 B\n",
      "                    sizeof_fmt:  136.0 B\n",
      "                       __doc__:  113.0 B\n",
      "                           _ih:   96.0 B\n",
      "                            In:   96.0 B\n",
      "                   __builtin__:   72.0 B\n",
      "                  __builtins__:   72.0 B\n",
      "                           sys:   72.0 B\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-finder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to test on a training group of 48 items, holding out the following subjects:['DEV077' 'DEV034' 'DEV027' 'DEV013' 'DEV010' 'DEV042' 'DEV056' 'DEV084'\n",
      " 'DEV036' 'DEV079' 'DEV068' 'DEV086']\n",
      "selecting training data\n",
      "selecting test data\n",
      "(91, 109, 91, 748)\n",
      "regressing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold,GroupKFold\n",
    "#def cv_train_test_different_sets(averaged_X,averaged_Y, averaged_groups, individual_X,individual_groups, Y, cv,group_list)\n",
    "\"\"\"\n",
    "averaged_X: values grouped\n",
    "averaged_groups: group allocations for the averaged dataset\n",
    "individual_X: values grouped into averages for testing\n",
    "cv: a Grouped cross-validator\n",
    "group_list: name of the groups\n",
    "\"\"\"\n",
    "individual_X = all_subs_nn_nifti\n",
    "individual_y = all_subs_nn_nifti_Y\n",
    "individual_groups = all_subs_nn_nifti_groups\n",
    "cv=KFold(n_splits=5)\n",
    "\n",
    "del all_subs_grouped_nifti\n",
    "del all_subs_grouped_nifti_Y\n",
    "del all_subs_grouped_nifti_groups\n",
    "\n",
    "\n",
    "groups_array = np.array(list(set(individual_groups)))\n",
    "\n",
    "#the CV that the inner Regressor uses\n",
    "cv_inner = GroupKFold(3)\n",
    "\n",
    "#we actually use KFold on the group names themselves, then filter across that\n",
    "#that's equivalent to doing a GroupedKFold on the data.\n",
    "test_scores = []\n",
    "for train_i,test_i in cv.split(groups_array):\n",
    "    train_group_items, test_group_items = groups_array[train_i], groups_array[test_i]\n",
    "    print('In order to test on a training group of ' +\n",
    "          str(len(train_group_items)) + ' items, holding out the following subjects:' +\n",
    "          str(test_group_items))\n",
    "    \n",
    "    #select training data from the averages\n",
    "    print('selecting training data')\n",
    "    train_selector = [i for i, x in enumerate(individual_groups) if x in train_group_items]\n",
    "    train_y = individual_y[train_selector]\n",
    "    train_X = nib.funcs.concat_images([individual_X.slicer[...,s] for s in train_selector])\n",
    "    train_groups = individual_groups[train_selector]\n",
    "    \n",
    "    #select testing data from the individual values\n",
    "    print('selecting test data')\n",
    "    test_selector = [i for i, x in enumerate(individual_groups) if x in test_group_items]\n",
    "    test_y = individual_y[test_selector]\n",
    "    test_X = nib.funcs.concat_images([individual_X.slicer[...,s] for s in test_selector])\n",
    "    test_groups = individual_groups[test_selector]\n",
    "    print(test_X.shape)\n",
    "    \n",
    "    print(\"regressing\")\n",
    "    regressor = DecoderRegressor(standardize= True,cv=cv_inner, scoring=\"r2\")\n",
    "    regressor.fit(y=train_y,X=train_X,groups=train_groups)\n",
    "    \n",
    "    print(\"predicting\")\n",
    "    #now predict on our test split\n",
    "    test_score = regressor.score(test_X,test_y)\n",
    "    test_scores = test_scores+[test_score]\n",
    "    print('test score was:')\n",
    "    print(test_score)\n",
    "    \n",
    "    del test_X\n",
    "    del train_X\n",
    "    gc.collect() #clean up. this is big data we're working with\n",
    "    #https://stackoverflow.com/questions/1316767/how-can-i-explicitly-free-memory-in-python\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Nested cross-validation\n",
    "\n",
    "See for instance: http://nilearn.github.io/auto_examples/02_decoding/plot_haxby_grid_search.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-today",
   "metadata": {},
   "source": [
    "\n",
    "Issues here:\n",
    "\n",
    " - Need to get the cross-validation right. We have got it working but there is an inner validation that selects a decoder for each group. Does that make sense? I think so, but just need to consider it a bit carefully.\n",
    " - What are the individual methods that inner validation is running? Should keep track of that\n",
    " - We really need to compare this to a baseline. So we need to run an individual-level analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-puppy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-neuralsignature]",
   "language": "python",
   "name": "conda-env-.conda-neuralsignature-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
