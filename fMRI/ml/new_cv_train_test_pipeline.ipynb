{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mighty-hungary",
   "metadata": {},
   "source": [
    "This file tests a new pipeline where cv_train_test_sets selects an overall best hyper regressor.\n",
    "\n",
    "Not sure whether it should also use it to do a final training and prediction or whether we just pass back the regressor object and have the wrapper class do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "future-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apply_loocv_and_save import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "increased-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train_test_sets(\n",
    "    trainset_X, trainset_y, trainset_groups,\n",
    "    regressors = None,\n",
    "    testset_X = None,testset_y = None, testset_groups = None,\n",
    "    param_grid=None,\n",
    "    cpus_to_use=-2,\n",
    "    cv = None):\n",
    "    \"\"\"\n",
    "    uses a division of 'trainset' and 'testset' to allow different values to be trained and tested \n",
    "    in KFold Cross Validation. All the values are used for training and testing, but we use different ones.\n",
    "    This enables us to e.g., pass in aggregated images for training and separate images for testing.\n",
    "\n",
    "    trainset_X: x values applicalbe for TRAINING\n",
    "    trainset_y\n",
    "    trainset_groups: group allocations for the trainset dataset\n",
    "    testset_X: values grouped into averages for testing\n",
    "    testset_y\n",
    "    cv: a Grouped cross-validator\n",
    "    group_list: name of the groups\n",
    "    \"\"\"\n",
    "    if cv is None:\n",
    "        cv=KFold(n_splits=5)\n",
    "\n",
    "    if param_grid is not None and regressors is not None:\n",
    "        raise Exception('values for param_grid and regressors both passed, but param_grid is ignored if regressors is passed. choose one.')\n",
    "\n",
    "    #if the groups we're using are actually the same.\n",
    "    if (testset_X is None) and (testset_y is None):\n",
    "        testset_X = trainset_X\n",
    "        testset_y = trainset_y\n",
    "        testset_groups = trainset_groups\n",
    "        print('Groups are the same.')\n",
    "\n",
    "    results_by_trainset_item = pd.DataFrame({\n",
    "        'y': trainset_y,\n",
    "        'group':trainset_groups,\n",
    "        'y_pred':np.repeat(None,len(trainset_y))#,\n",
    "        #'y_match':np.repeat(None,len(trainset_y))#just for debugging. delete.\n",
    "    })\n",
    "\n",
    "\n",
    "    groups_array = np.array(list(set(testset_groups)))\n",
    "    assert(set(trainset_groups)==set(testset_groups))\n",
    "\n",
    "    #the CV that the inner Regressor uses\n",
    "    cv_inner = GroupKFold(3)\n",
    "    if regressors is None:\n",
    "        regressors = [DecoderRegressor(standardize= True,param_grid=param_grid,cv=cv_inner,scoring=\"r2\",\n",
    "                                      n_jobs=cpus_to_use)]\n",
    "        print('using default regressor',end='. ')\n",
    "\n",
    "    #we actually use KFold on the group names themselves, then filter across that\n",
    "    #that's equivalent to doing a GroupedKFold on the data.\n",
    "    test_scores = []\n",
    "    results = []\n",
    "\n",
    "    if type(cv)==type(LeaveOneOut()):\n",
    "        outer_n=len(groups_array)\n",
    "    else:\n",
    "        outer_n = cv.get_n_splits()\n",
    "    for i, x in enumerate(cv.split(groups_array)):\n",
    "        train_i = x[0]\n",
    "        test_i = x[1]\n",
    "        print(\"fold \" + str(i+1) + \" of \" + str(outer_n))\n",
    "\n",
    "        fold_i_results = {}\n",
    "        train_group_items, test_group_items = groups_array[train_i], groups_array[test_i]\n",
    "        print('In order to test on a training group of ' +\n",
    "              str(len(train_group_items)) + ' items, holding out the following subjects:' +\n",
    "              str(test_group_items),end='. ')\n",
    "#         print(\n",
    "#             'held out ' + str(len(test_group_items)) + ' items and trained on ' + str(len(train_group_items)) + ' items',\n",
    "#             end='. ')\n",
    "\n",
    "        print('prepping fold data...',end='. ')\n",
    "        #select training data from the averages\n",
    "        #print('selecting training data',end='. ')\n",
    "        train_selector = [i for i, x in enumerate(trainset_groups) if x in train_group_items]\n",
    "        train_y = trainset_y[train_selector]\n",
    "        train_X = nib.funcs.concat_images([trainset_X.slicer[...,s] for s in train_selector])\n",
    "        train_groups = trainset_groups[train_selector]\n",
    "        #print(train_X.shape,end='. ')\n",
    "        #print(asizeof_fmt(train_X),end='. ')\n",
    "\n",
    "        #select testing data from the individual values\n",
    "        #print('selecting test data',end='. ')\n",
    "        test_selector = [i for i, x in enumerate(testset_groups) if x in test_group_items]\n",
    "        test_y = testset_y[test_selector]\n",
    "        test_X = nib.funcs.concat_images([testset_X.slicer[...,s] for s in test_selector])\n",
    "        test_groups = testset_groups[test_selector]\n",
    "        #print(asizeof_fmt(test_X),end='. ')\n",
    "        #print(test_X.shape,end='. ')\n",
    "\n",
    "\n",
    "        print(\"regressing...\",end='. ')\n",
    "        print(asizeof_fmt(train_X),end='. ')\n",
    "\n",
    "        val_scores = []\n",
    "        #iterate through regressor objects.\n",
    "        #this is my way of doing cross-validation across different regressors...\n",
    "        hyper_scores = []\n",
    "        train_results = {}\n",
    "        inner_cv_results = {}\n",
    "        for r_i, reg in enumerate(regressors):\n",
    "            cur_r_results = {}\n",
    "            print('trying regressor ' + str(r_i+1) + ' of ' + str(len(regressors)),end='. ')\n",
    "            #if there is nested CV within this function the best hyper-paramters are already being chosen\n",
    "            #we need only to finish the job by identifying the best overall regressor, as the final hyper-parameter\n",
    "            reg.fit(y=train_y,X=train_X,groups=train_groups)\n",
    "            print(\"predicting\",end='. ')\n",
    "            #hyper_score = reg.score(train_X,train_y)\n",
    "            hyper_score = np.max([np.mean(param_values) for param_name, param_values in reg.cv_scores_.items()])\n",
    "            #think there is a bug here. we should not have to be guessing/ignoring param names.\n",
    "            #need to report this.\n",
    "\n",
    "            hyper_scores = hyper_scores + [hyper_score]\n",
    "\n",
    "            cur_r_results['hyper_score'] = hyper_score\n",
    "            cur_r_results['cv_scores_'] = reg.cv_scores_\n",
    "            cur_r_results['cv_params_'] = reg.cv_params_\n",
    "            inner_cv_results[str(reg)] = cur_r_results\n",
    "\n",
    "        fold_i_results['train_results']= inner_cv_results\n",
    "\n",
    "        #identify which was the best\n",
    "        #print(hyper_scores)\n",
    "        #print(np.where([h==np.max(hyper_scores) for h in hyper_scores])[0][0])\n",
    "        best_hyper_regressor = regressors[np.where([h==np.max(hyper_scores) for h in hyper_scores])[0][0]]\n",
    "\n",
    "        #print(best_hyper_regressor)\n",
    "\n",
    "        #now run JUST that one on this fold.\n",
    "\n",
    "\n",
    "        #now predict on our test split\n",
    "        test_score = best_hyper_regressor.score(test_X,test_y)\n",
    "        test_y_pred = best_hyper_regressor.predict(test_X)\n",
    "        fold_test_rawdata = pd.DataFrame({\n",
    "            'y_obs':test_y,\n",
    "            'y_pred':test_y_pred,\n",
    "            'y_groups':test_groups\n",
    "\n",
    "        })\n",
    "        #results_by_trainset_item.loc[train_selector,'y_pred']\n",
    "        results_by_trainset_item.loc[test_selector,'y_pred'] = test_y_pred\n",
    "        #results_by_trainset_item.loc[test_selector,'y_match'] = test_y\n",
    "        fold_i_results['fold_test_rawdata'] = fold_test_rawdata\n",
    "        #so we can do scoring externally to this function.\n",
    "\n",
    "        test_scores = test_scores+[test_score]\n",
    "        print('test score was:',end='. ')\n",
    "        print(test_score)\n",
    "\n",
    "        results = results + [fold_i_results]\n",
    "\n",
    "        del test_X\n",
    "        del train_X\n",
    "        gc.collect() #clean up. this is big data we're working with\n",
    "        #https://stackoverflow.com/questions/1316767/how-can-i-explicitly-free-memory-in-python\n",
    "        \n",
    "    #now run the classifier once more on the train AND test data to get an overall beta image\n",
    "    #but if we ran more than one, then...which one do we run? We'd need to examine their performance across all the folds\n",
    "    #and select one overall one. Shall we do that? yeah let's take a look....\n",
    "    #so what we need to do is that we need to end up with a matrix of which regressor does best with which fold....\n",
    "    #best_hyper_regressor.fit()\n",
    "\n",
    "    return(test_scores,results,results_by_trainset_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vietnamese-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_loocv_and_save(\n",
    "    results_filepath,\n",
    "    brain_data_filepath = '../data/Brain_Data_2sns_60subs.pkl',\n",
    "    train_test_markers_filepath = \"../data/train_test_markers_20210601T183243.csv\",\n",
    "    subjs_to_use = None, #set this to get a subset, otherwise use all of them.\n",
    "    response_transform_func = None\n",
    "    ):\n",
    "    #pd.set_option('display.max_rows', 99)\n",
    "    test_train_set = pd.read_csv(train_test_markers_filepath)\n",
    "\n",
    "    with open(brain_data_filepath, 'rb') as pkl_file:\n",
    "        Brain_Data_allsubs = pickle.load(pkl_file)\n",
    "    \n",
    "    dev_wtp_io_utils.check_BD_against_test_train_set(Brain_Data_allsubs,test_train_set)\n",
    "\n",
    "    #################################################\n",
    "    #######PRE-PROCESS\n",
    "\n",
    "    \n",
    "    if response_transform_func is None:\n",
    "        Brain_Data_allsubs.Y = Brain_Data_allsubs.X['response'].copy()\n",
    "    else:\n",
    "        Brain_Data_allsubs.Y = response_transform_func(Brain_Data_allsubs.X)\n",
    "    \n",
    "        \n",
    "    print(Brain_Data_allsubs.Y.value_counts())\n",
    "    Brain_Data_allsubs.Y[Brain_Data_allsubs.Y=='NULL']=None\n",
    "    print(Brain_Data_allsubs.Y.value_counts())\n",
    "\n",
    "    import sys\n",
    "    for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "                             key= lambda x: -x[1])[:10]:\n",
    "        print(name + ': ' + str(size))\n",
    "    print(Brain_Data_allsubs.Y.isnull().value_counts())\n",
    "    Brain_Data_allsubs_nn = Brain_Data_allsubs[Brain_Data_allsubs.Y.isnull()==False]\n",
    "    print(len(Brain_Data_allsubs_nn))\n",
    "    print(len(Brain_Data_allsubs))\n",
    "\n",
    "\n",
    "    all_subs_nn_nifti = Brain_Data_allsubs_nn.to_nifti()\n",
    "    all_subs_nn_nifti_Y = Brain_Data_allsubs_nn.Y\n",
    "    all_subs_nn_nifti_groups = Brain_Data_allsubs_nn.X.subject\n",
    "    all_subs_nn_nifti_groups\n",
    "\n",
    "\n",
    "    all_subs_nn_nifti_metadata = Brain_Data_allsubs_nn.X\n",
    "\n",
    "\n",
    "    #################################################\n",
    "    #######GET SUB-SET\n",
    "\n",
    "\n",
    "    del Brain_Data_allsubs\n",
    "    #del Brain_Data_allsubs_grouped\n",
    "    gc.collect()\n",
    "\n",
    "    from nilearn.decoding import DecoderRegressor\n",
    "    regressors = [DecoderRegressor(standardize= True,param_grid=param_grid,cv=cv_inner,scoring=\"r2\",\n",
    "                                      n_jobs=cpus_to_use)]\n",
    "\n",
    "    print(asizeof_fmt(Brain_Data_allsubs_nn))\n",
    "\n",
    "    print(asizeof_fmt(all_subs_nn_nifti))\n",
    "    \n",
    "    if subjs_to_use is None:\n",
    "        subjs_to_use=len(np.unique(all_subs_nn_nifti_groups))\n",
    "\n",
    "    sample_subject_items = np.unique(all_subs_nn_nifti_groups)[0:subjs_to_use] #get all of them\n",
    "    sample_subject_vector = [i for i, x in enumerate(all_subs_nn_nifti_groups) if x in sample_subject_items]\n",
    "\n",
    "    first_subs_nifti = nib.funcs.concat_images([all_subs_nn_nifti.slicer[...,s] for s in sample_subject_vector])\n",
    "    first_subs_nifti_Y = all_subs_nn_nifti_Y[sample_subject_vector]\n",
    "    first_subs_nifti = nil.image.clean_img(first_subs_nifti,detrend=False,standardize=True)\n",
    "    first_subs_nifti_groups = all_subs_nn_nifti_groups[sample_subject_vector]\n",
    "\n",
    "    del all_subs_nn_nifti\n",
    "    gc.collect()\n",
    "\n",
    "    first_subs_nifti_metadata = all_subs_nn_nifti_metadata.loc[sample_subject_vector,:]\n",
    "\n",
    "    print(\"starting LeaveOneOut\")\n",
    "    #in this design, we're actually dealing with groups\n",
    "    #we select group IDs and then grab the subjects\n",
    "    #so we don't need to use LeaveOneGroupOut\n",
    "    #the grouping is implicit\n",
    "    cv_outer = LeaveOneOut()\n",
    "\n",
    "    print(\"finished preprocessing\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    test_scores_same,tt_results,results_by_trainset_item = cv_train_test_sets(\n",
    "        trainset_X=first_subs_nifti,\n",
    "        trainset_y=first_subs_nifti_Y,\n",
    "        trainset_groups=first_subs_nifti_groups,\n",
    "        regressors = regressors,\n",
    "        cv=cv_outer,\n",
    "        cpus_to_use=cpus_available\n",
    "\n",
    "    )\n",
    "\n",
    "    print(test_scores_same[0])\n",
    "    print(np.mean(test_scores_same[0]))\n",
    "\n",
    "    print('finished learning')\n",
    "\n",
    "    with open(results_filepath, 'wb') as handle:\n",
    "        pickle.dump([test_scores_same,tt_results,results_by_trainset_item],handle)\n",
    "\n",
    "\n",
    "    print('saved.')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "express-truth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked for intersection and no intersection between the brain data and the subjects was found.\n",
      "there were 5 subjects overlapping between the subjects marked for train data and the training dump file itself.\n",
      "5.0    111\n",
      "6.0     81\n",
      "7.0     64\n",
      "8.0     51\n",
      "Name: response, dtype: int64\n",
      "5.0    111\n",
      "6.0     81\n",
      "7.0     64\n",
      "8.0     51\n",
      "Name: response, dtype: int64\n",
      "test_train_set: 9549\n",
      "pkl_file: 168\n",
      "results_filepath: 98\n",
      "train_test_markers_filepath: 95\n",
      "brain_data_filepath: 80\n",
      "sys: 72\n",
      "Brain_Data_allsubs: 48\n",
      "subjs_to_use: 16\n",
      "response_transform_func: 16\n",
      "False    307\n",
      "True      13\n",
      "Name: response, dtype: int64\n",
      "307\n",
      "320\n",
      "284.3 MiB\n",
      "1.0 GiB\n",
      "starting LeaveOneOut\n",
      "finished preprocessing\n",
      "Groups are the same.\n",
      "fold 1 of 5\n",
      "In order to test on a training group of 4 items, holding out the following subjects:['DEV001']. prepping fold data.... regressing.... 1.6 GiB. trying regressor 1 of 2. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:514: UserWarning: groups parameter is specified but cv parameter is not set to custom CV splitter. Using default object LeaveOneGroupOut().\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting. trying regressor 2 of 2. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:514: UserWarning: groups parameter is specified but cv parameter is not set to custom CV splitter. Using default object LeaveOneGroupOut().\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting. test score was:. 0.2316291643071221\n",
      "fold 2 of 5\n",
      "In order to test on a training group of 4 items, holding out the following subjects:['DEV006']. prepping fold data.... regressing.... 1.7 GiB. trying regressor 1 of 2. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:143: UserWarning: Use a custom estimator at your own risk of the process not working as intended.\n",
      "  warnings.warn('Use a custom estimator at your own risk '\n",
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:514: UserWarning: groups parameter is specified but cv parameter is not set to custom CV splitter. Using default object LeaveOneGroupOut().\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting. trying regressor 2 of 2. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:143: UserWarning: Use a custom estimator at your own risk of the process not working as intended.\n",
      "  warnings.warn('Use a custom estimator at your own risk '\n",
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:514: UserWarning: groups parameter is specified but cv parameter is not set to custom CV splitter. Using default object LeaveOneGroupOut().\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting. test score was:. 0.20887206935528646\n",
      "fold 3 of 5\n",
      "In order to test on a training group of 4 items, holding out the following subjects:['DEV009']. prepping fold data.... regressing.... 1.6 GiB. trying regressor 1 of 2. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:143: UserWarning: Use a custom estimator at your own risk of the process not working as intended.\n",
      "  warnings.warn('Use a custom estimator at your own risk '\n",
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:514: UserWarning: groups parameter is specified but cv parameter is not set to custom CV splitter. Using default object LeaveOneGroupOut().\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting. trying regressor 2 of 2. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:143: UserWarning: Use a custom estimator at your own risk of the process not working as intended.\n",
      "  warnings.warn('Use a custom estimator at your own risk '\n",
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:514: UserWarning: groups parameter is specified but cv parameter is not set to custom CV splitter. Using default object LeaveOneGroupOut().\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting. test score was:. 0.06890307588877076\n",
      "fold 4 of 5\n",
      "In order to test on a training group of 4 items, holding out the following subjects:['DEV010']. prepping fold data.... regressing.... 1.7 GiB. trying regressor 1 of 2. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:143: UserWarning: Use a custom estimator at your own risk of the process not working as intended.\n",
      "  warnings.warn('Use a custom estimator at your own risk '\n",
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:514: UserWarning: groups parameter is specified but cv parameter is not set to custom CV splitter. Using default object LeaveOneGroupOut().\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting. trying regressor 2 of 2. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:143: UserWarning: Use a custom estimator at your own risk of the process not working as intended.\n",
      "  warnings.warn('Use a custom estimator at your own risk '\n",
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:514: UserWarning: groups parameter is specified but cv parameter is not set to custom CV splitter. Using default object LeaveOneGroupOut().\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting. test score was:. 0.3311807547709793\n",
      "fold 5 of 5\n",
      "In order to test on a training group of 4 items, holding out the following subjects:['DEV005']. prepping fold data.... regressing.... 1.6 GiB. trying regressor 1 of 2. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:143: UserWarning: Use a custom estimator at your own risk of the process not working as intended.\n",
      "  warnings.warn('Use a custom estimator at your own risk '\n",
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:514: UserWarning: groups parameter is specified but cv parameter is not set to custom CV splitter. Using default object LeaveOneGroupOut().\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting. trying regressor 2 of 2. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:143: UserWarning: Use a custom estimator at your own risk of the process not working as intended.\n",
      "  warnings.warn('Use a custom estimator at your own risk '\n",
      "/home/bsmith16/.conda/envs/neuralsignature/lib/python3.8/site-packages/nilearn/decoding/decoder.py:514: UserWarning: groups parameter is specified but cv parameter is not set to custom CV splitter. Using default object LeaveOneGroupOut().\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting. test score was:. 0.20553738643177666\n",
      "0.2316291643071221\n",
      "0.2316291643071221\n",
      "finished learning\n",
      "saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "    \n",
    "apply_loocv_and_save(\n",
    "    results_filepath=\"../data/cv_train_test_ns_5subjs_outer_n_loocv.pkl\",\n",
    "    brain_data_filepath = '../data/Brain_Data_ns_5subs.pkl',\n",
    "    train_test_markers_filepath = \"../data/train_test_markers_20210601T183243.csv\"#,\n",
    "#    response_transform_func =transform_normalize\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acting-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "loocv_results = pickle.load(open(\"../data/cv_train_test_ns_5subjs_outer_n_loocv.pkl\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "colonial-journal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_results', 'fold_test_rawdata'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loocv_results[1][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "governmental-arena",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DecoderRegressor(estimator=RidgeCV(alphas=array([ 0.1,  1. , 10. ])),\\n                 memory=Memory(location=None))': {'hyper_score': 0.1334553659782704,\n",
       "  'cv_scores_': {'beta': [0.24546541019031898,\n",
       "    0.12846594873346473,\n",
       "    -0.06741689987425015,\n",
       "    0.22730700486354805]},\n",
       "  'cv_params_': {'beta': {}}},\n",
       " \"DecoderRegressor(estimator=SVR(kernel='linear', max_iter=10000.0),\\n                 memory=Memory(location=None))\": {'hyper_score': 0.15676823512438506,\n",
       "  'cv_scores_': {'beta': [0.2596661569483836,\n",
       "    0.12787342116889444,\n",
       "    -0.01240405238771003,\n",
       "    0.25193741476797227]},\n",
       "  'cv_params_': {'beta': {'C': [100.0, 100.0, 100.0, 100.0]}}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loocv_results[1][0]['train_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "clean-coupon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decoder</th>\n",
       "      <th>fold</th>\n",
       "      <th>hyper_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecoderRegressor(estimator=RidgeCV(alphas=arra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.133455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecoderRegressor(estimator=SVR(kernel='linear'...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.156768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecoderRegressor(estimator=RidgeCV(alphas=arra...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.183787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecoderRegressor(estimator=SVR(kernel='linear'...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.176635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecoderRegressor(estimator=RidgeCV(alphas=arra...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.208691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DecoderRegressor(estimator=SVR(kernel='linear'...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.197646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DecoderRegressor(estimator=RidgeCV(alphas=arra...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.158236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DecoderRegressor(estimator=SVR(kernel='linear'...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.161157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DecoderRegressor(estimator=RidgeCV(alphas=arra...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.106324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DecoderRegressor(estimator=SVR(kernel='linear'...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.103360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             decoder  fold  hyper_score\n",
       "0  DecoderRegressor(estimator=RidgeCV(alphas=arra...     0     0.133455\n",
       "1  DecoderRegressor(estimator=SVR(kernel='linear'...     0     0.156768\n",
       "2  DecoderRegressor(estimator=RidgeCV(alphas=arra...     1     0.183787\n",
       "3  DecoderRegressor(estimator=SVR(kernel='linear'...     1     0.176635\n",
       "4  DecoderRegressor(estimator=RidgeCV(alphas=arra...     2     0.208691\n",
       "5  DecoderRegressor(estimator=SVR(kernel='linear'...     2     0.197646\n",
       "6  DecoderRegressor(estimator=RidgeCV(alphas=arra...     3     0.158236\n",
       "7  DecoderRegressor(estimator=SVR(kernel='linear'...     3     0.161157\n",
       "8  DecoderRegressor(estimator=RidgeCV(alphas=arra...     4     0.106324\n",
       "9  DecoderRegressor(estimator=SVR(kernel='linear'...     4     0.103360"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_scores_across_folds_list = []\n",
    "for fold_i in range(len(loocv_results[1])):\n",
    "    fold_hyper_scores = [pd.DataFrame(\n",
    "        {\"decoder\":[k],\n",
    "         \"fold\":fold_i,\n",
    "         \"hyper_score\":[v['hyper_score']]}) for k,v in loocv_results[1][fold_i]['train_results'].items()]\n",
    "    hyper_scores_across_folds_list = hyper_scores_across_folds_list + fold_hyper_scores\n",
    "hyper_scores_across_folds = pd.concat(hyper_scores_across_folds_list).reset_index(drop=True)\n",
    "hyper_scores_across_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-routine",
   "metadata": {},
   "source": [
    "Now we need the folds to 'vote' on the best regressor.\n",
    "I don't know if we should be doing this...are we somehow overfitting? Not sure...anyway..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "based-dublin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decoder\n",
       "DecoderRegressor(estimator=RidgeCV(alphas=array([ 0.1,  1. , 10. ])),\\n                 memory=Memory(location=None))    0.158099\n",
       "DecoderRegressor(estimator=SVR(kernel='linear', max_iter=10000.0),\\n                 memory=Memory(location=None))       0.159113\n",
       "Name: hyper_score, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_scores_across_folds.groupby('decoder').hyper_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "together-registrar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                                                       decoder  \\\n",
       " cv_params_   DecoderRegressor(estimator=RidgeCV(alphas=arra...   \n",
       " cv_scores_   DecoderRegressor(estimator=RidgeCV(alphas=arra...   \n",
       " hyper_score  DecoderRegressor(estimator=RidgeCV(alphas=arra...   \n",
       " \n",
       "                                                    hyper_score  \n",
       " cv_params_                                        {'beta': {}}  \n",
       " cv_scores_   {'beta': [0.24546541019031898, 0.1284659487334...  \n",
       " hyper_score                                           0.133455  ,\n",
       "                                                        decoder  \\\n",
       " cv_params_   DecoderRegressor(estimator=SVR(kernel='linear'...   \n",
       " cv_scores_   DecoderRegressor(estimator=SVR(kernel='linear'...   \n",
       " hyper_score  DecoderRegressor(estimator=SVR(kernel='linear'...   \n",
       " \n",
       "                                                    hyper_score  \n",
       " cv_params_       {'beta': {'C': [100.0, 100.0, 100.0, 100.0]}}  \n",
       " cv_scores_   {'beta': [0.2596661569483836, 0.12787342116889...  \n",
       " hyper_score                                           0.156768  ]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_hyper_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-complement",
   "metadata": {},
   "source": [
    "Right I think we have the data here. How do we select the best regressor across all the train_results?\n",
    "\n",
    "Internally for each fold we are comparing hyper_score, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyper_regressor = regressors[np.where([h==np.max(hyper_scores) for h in hyper_scores])[0][0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-neuralsignature]",
   "language": "python",
   "name": "conda-env-.conda-neuralsignature-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
